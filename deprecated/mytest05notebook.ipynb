{
 "cells": [
  {
   "cell_type": "code",
   "id": "8bee008a55469b64",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-22T14:06:19.895265Z",
     "start_time": "2024-10-22T14:06:02.636234Z"
    }
   },
   "source": [
    "import glob\n",
    "import os\n",
    "import scripts\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# import odvae\n",
    "\n",
    "os.environ['HF_HOME'] = '/NEW_EDS/JJ_Group/shaoyh/dorin/cache'\n",
    "if not os.path.isdir(os.environ['HF_HOME']):\n",
    "    os.makedirs(os.environ['HF_HOME'])\n",
    "from diffusers import StableDiffusionPipeline, AutoencoderKL\n",
    "from diffusers import UNet2DConditionModel\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T06:53:21.225502Z",
     "start_time": "2024-10-20T06:53:20.648671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "diffusion_model_id = \"./checkpoints/stable-diffusion-v1-5\"\n",
    "vae = AutoencoderKL.from_pretrained(diffusion_model_id, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(diffusion_model_id, subfolder=\"unet\")\n",
    "\n",
    "# Print the VAE model structure\n",
    "print(vae)"
   ],
   "id": "cc88357da97ae1ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoencoderKL(\n",
      "  (encoder): Encoder(\n",
      "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (down_blocks): ModuleList(\n",
      "      (0): DownEncoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (downsamplers): ModuleList(\n",
      "          (0): Downsample2D(\n",
      "            (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): DownEncoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): LoRACompatibleConv(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (downsamplers): ModuleList(\n",
      "          (0): Downsample2D(\n",
      "            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): DownEncoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): LoRACompatibleConv(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (downsamplers): ModuleList(\n",
      "          (0): Downsample2D(\n",
      "            (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): DownEncoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (mid_block): UNetMidBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0): Attention(\n",
      "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (to_q): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "          (to_k): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "          (to_v): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "          (to_out): ModuleList(\n",
      "            (0): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (conv_act): SiLU()\n",
      "    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (up_blocks): ModuleList(\n",
      "      (0-1): 2 x UpDecoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0-2): 3 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (upsamplers): ModuleList(\n",
      "          (0): Upsample2D(\n",
      "            (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): UpDecoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): LoRACompatibleConv(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1-2): 2 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (upsamplers): ModuleList(\n",
      "          (0): Upsample2D(\n",
      "            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): UpDecoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1-2): 2 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (mid_block): UNetMidBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0): Attention(\n",
      "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (to_q): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "          (to_k): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "          (to_v): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "          (to_out): ModuleList(\n",
      "            (0): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "    (conv_act): SiLU()\n",
      "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T03:14:42.589145Z",
     "start_time": "2024-10-19T03:14:42.569494Z"
    }
   },
   "cell_type": "code",
   "source": "print(unet)",
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet2DConditionModel(\n",
      "  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (time_proj): Timesteps()\n",
      "  (time_embedding): TimestepEmbedding(\n",
      "    (linear_1): LoRACompatibleLinear(in_features=320, out_features=1280, bias=True)\n",
      "    (act): SiLU()\n",
      "    (linear_2): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
      "  )\n",
      "  (down_blocks): ModuleList(\n",
      "    (0): CrossAttnDownBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0-1): 2 x Transformer2DModel(\n",
      "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0): BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
      "                (to_k): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
      "                (to_v): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
      "                (to_k): LoRACompatibleLinear(in_features=768, out_features=320, bias=False)\n",
      "                (to_v): LoRACompatibleLinear(in_features=768, out_features=320, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "          (conv1): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
      "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): CrossAttnDownBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0-1): 2 x Transformer2DModel(\n",
      "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0): BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
      "                (to_v): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): LoRACompatibleLinear(in_features=768, out_features=640, bias=False)\n",
      "                (to_v): LoRACompatibleLinear(in_features=768, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "          (conv1): LoRACompatibleConv(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
      "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (conv1): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
      "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): CrossAttnDownBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0-1): 2 x Transformer2DModel(\n",
      "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0): BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_v): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): LoRACompatibleLinear(in_features=768, out_features=1280, bias=False)\n",
      "                (to_v): LoRACompatibleLinear(in_features=768, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (conv1): LoRACompatibleConv(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
      "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
      "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): DownBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
      "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up_blocks): ModuleList(\n",
      "    (0): UpBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0-2): 3 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
      "          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
      "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (upsamplers): ModuleList(\n",
      "        (0): Upsample2D(\n",
      "          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): CrossAttnUpBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0-2): 3 x Transformer2DModel(\n",
      "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0): BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_v): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
      "                (to_k): LoRACompatibleLinear(in_features=768, out_features=1280, bias=False)\n",
      "                (to_v): LoRACompatibleLinear(in_features=768, out_features=1280, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
      "          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
      "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
      "          (conv1): LoRACompatibleConv(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
      "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (upsamplers): ModuleList(\n",
      "        (0): Upsample2D(\n",
      "          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): CrossAttnUpBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0-2): 3 x Transformer2DModel(\n",
      "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
      "          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0): BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
      "                (to_v): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
      "                (to_k): LoRACompatibleLinear(in_features=768, out_features=640, bias=False)\n",
      "                (to_v): LoRACompatibleLinear(in_features=768, out_features=640, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
      "          (conv1): LoRACompatibleConv(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
      "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "          (conv1): LoRACompatibleConv(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
      "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
      "          (conv1): LoRACompatibleConv(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
      "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (upsamplers): ModuleList(\n",
      "        (0): Upsample2D(\n",
      "          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): CrossAttnUpBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0-2): 3 x Transformer2DModel(\n",
      "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
      "          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (transformer_blocks): ModuleList(\n",
      "            (0): BasicTransformerBlock(\n",
      "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn1): Attention(\n",
      "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
      "                (to_k): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
      "                (to_v): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn2): Attention(\n",
      "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
      "                (to_k): LoRACompatibleLinear(in_features=768, out_features=320, bias=False)\n",
      "                (to_v): LoRACompatibleLinear(in_features=768, out_features=320, bias=False)\n",
      "                (to_out): ModuleList(\n",
      "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
      "              (ff): FeedForward(\n",
      "                (net): ModuleList(\n",
      "                  (0): GEGLU(\n",
      "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
      "                  )\n",
      "                  (1): Dropout(p=0.0, inplace=False)\n",
      "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
      "          (conv1): LoRACompatibleConv(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
      "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1-2): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
      "          (conv1): LoRACompatibleConv(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
      "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mid_block): UNetMidBlock2DCrossAttn(\n",
      "    (attentions): ModuleList(\n",
      "      (0): Transformer2DModel(\n",
      "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
      "        (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (transformer_blocks): ModuleList(\n",
      "          (0): BasicTransformerBlock(\n",
      "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn1): Attention(\n",
      "              (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_k): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_v): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_out): ModuleList(\n",
      "                (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn2): Attention(\n",
      "              (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
      "              (to_k): LoRACompatibleLinear(in_features=768, out_features=1280, bias=False)\n",
      "              (to_v): LoRACompatibleLinear(in_features=768, out_features=1280, bias=False)\n",
      "              (to_out): ModuleList(\n",
      "                (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (ff): FeedForward(\n",
      "              (net): ModuleList(\n",
      "                (0): GEGLU(\n",
      "                  (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
      "                )\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "                (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (resnets): ModuleList(\n",
      "      (0-1): 2 x ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "        (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
      "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
      "  (conv_act): SiLU()\n",
      "  (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T03:14:43.390114Z",
     "start_time": "2024-10-19T03:14:43.349311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# VAE整体结构\n",
    "for name, module in vae.named_modules():\n",
    "    print(f\"{name}: {module}\")"
   ],
   "id": "36c746950ac40ccf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": AutoencoderKL(\n",
      "  (encoder): Encoder(\n",
      "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (down_blocks): ModuleList(\n",
      "      (0): DownEncoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (downsamplers): ModuleList(\n",
      "          (0): Downsample2D(\n",
      "            (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): DownEncoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): LoRACompatibleConv(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (downsamplers): ModuleList(\n",
      "          (0): Downsample2D(\n",
      "            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): DownEncoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): LoRACompatibleConv(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (downsamplers): ModuleList(\n",
      "          (0): Downsample2D(\n",
      "            (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): DownEncoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (mid_block): UNetMidBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0): Attention(\n",
      "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (to_q): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "          (to_k): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "          (to_v): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "          (to_out): ModuleList(\n",
      "            (0): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (conv_act): SiLU()\n",
      "    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (up_blocks): ModuleList(\n",
      "      (0-1): 2 x UpDecoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0-2): 3 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (upsamplers): ModuleList(\n",
      "          (0): Upsample2D(\n",
      "            (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): UpDecoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): LoRACompatibleConv(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1-2): 2 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "        (upsamplers): ModuleList(\n",
      "          (0): Upsample2D(\n",
      "            (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): UpDecoderBlock2D(\n",
      "        (resnets): ModuleList(\n",
      "          (0): ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "            (conv_shortcut): LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1-2): 2 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (mid_block): UNetMidBlock2D(\n",
      "      (attentions): ModuleList(\n",
      "        (0): Attention(\n",
      "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (to_q): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "          (to_k): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "          (to_v): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "          (to_out): ModuleList(\n",
      "            (0): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "    (conv_act): SiLU()\n",
      "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "encoder: Encoder(\n",
      "  (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (down_blocks): ModuleList(\n",
      "    (0): DownEncoderBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): DownEncoderBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): DownEncoderBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): DownEncoderBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mid_block): UNetMidBlock2D(\n",
      "    (attentions): ModuleList(\n",
      "      (0): Attention(\n",
      "        (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (to_q): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "        (to_k): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "        (to_v): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "        (to_out): ModuleList(\n",
      "          (0): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (resnets): ModuleList(\n",
      "      (0-1): 2 x ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (conv_act): SiLU()\n",
      "  (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "encoder.conv_in: Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.down_blocks: ModuleList(\n",
      "  (0): DownEncoderBlock2D(\n",
      "    (resnets): ModuleList(\n",
      "      (0-1): 2 x ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "        (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "    (downsamplers): ModuleList(\n",
      "      (0): Downsample2D(\n",
      "        (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): DownEncoderBlock2D(\n",
      "    (resnets): ModuleList(\n",
      "      (0): ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "        (conv1): LoRACompatibleConv(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "        (conv_shortcut): LoRACompatibleConv(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "        (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "    (downsamplers): ModuleList(\n",
      "      (0): Downsample2D(\n",
      "        (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): DownEncoderBlock2D(\n",
      "    (resnets): ModuleList(\n",
      "      (0): ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "        (conv1): LoRACompatibleConv(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "        (conv_shortcut): LoRACompatibleConv(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "    (downsamplers): ModuleList(\n",
      "      (0): Downsample2D(\n",
      "        (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): DownEncoderBlock2D(\n",
      "    (resnets): ModuleList(\n",
      "      (0-1): 2 x ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "encoder.down_blocks.0: DownEncoderBlock2D(\n",
      "  (resnets): ModuleList(\n",
      "    (0-1): 2 x ResnetBlock2D(\n",
      "      (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "      (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (downsamplers): ModuleList(\n",
      "    (0): Downsample2D(\n",
      "      (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "encoder.down_blocks.0.resnets: ModuleList(\n",
      "  (0-1): 2 x ResnetBlock2D(\n",
      "    (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "    (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (nonlinearity): SiLU()\n",
      "  )\n",
      ")\n",
      "encoder.down_blocks.0.resnets.0: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "encoder.down_blocks.0.resnets.0.norm1: GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "encoder.down_blocks.0.resnets.0.conv1: LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.down_blocks.0.resnets.0.norm2: GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "encoder.down_blocks.0.resnets.0.dropout: Dropout(p=0.0, inplace=False)\n",
      "encoder.down_blocks.0.resnets.0.conv2: LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.down_blocks.0.resnets.0.nonlinearity: SiLU()\n",
      "encoder.down_blocks.0.resnets.1: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "encoder.down_blocks.0.resnets.1.norm1: GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "encoder.down_blocks.0.resnets.1.conv1: LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.down_blocks.0.resnets.1.norm2: GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "encoder.down_blocks.0.resnets.1.dropout: Dropout(p=0.0, inplace=False)\n",
      "encoder.down_blocks.0.resnets.1.conv2: LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.down_blocks.0.downsamplers: ModuleList(\n",
      "  (0): Downsample2D(\n",
      "    (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "  )\n",
      ")\n",
      "encoder.down_blocks.0.downsamplers.0: Downsample2D(\n",
      "  (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      ")\n",
      "encoder.down_blocks.0.downsamplers.0.conv: LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "encoder.down_blocks.1: DownEncoderBlock2D(\n",
      "  (resnets): ModuleList(\n",
      "    (0): ResnetBlock2D(\n",
      "      (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "      (conv1): LoRACompatibleConv(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "      (conv_shortcut): LoRACompatibleConv(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): ResnetBlock2D(\n",
      "      (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "      (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (downsamplers): ModuleList(\n",
      "    (0): Downsample2D(\n",
      "      (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "encoder.down_blocks.1.resnets: ModuleList(\n",
      "  (0): ResnetBlock2D(\n",
      "    (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "    (conv1): LoRACompatibleConv(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (nonlinearity): SiLU()\n",
      "    (conv_shortcut): LoRACompatibleConv(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (1): ResnetBlock2D(\n",
      "    (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "    (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (nonlinearity): SiLU()\n",
      "  )\n",
      ")\n",
      "encoder.down_blocks.1.resnets.0: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      "  (conv_shortcut): LoRACompatibleConv(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "encoder.down_blocks.1.resnets.0.norm1: GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "encoder.down_blocks.1.resnets.0.conv1: LoRACompatibleConv(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.down_blocks.1.resnets.0.norm2: GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "encoder.down_blocks.1.resnets.0.dropout: Dropout(p=0.0, inplace=False)\n",
      "encoder.down_blocks.1.resnets.0.conv2: LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.down_blocks.1.resnets.0.conv_shortcut: LoRACompatibleConv(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "encoder.down_blocks.1.resnets.1: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "encoder.down_blocks.1.resnets.1.norm1: GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "encoder.down_blocks.1.resnets.1.conv1: LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.down_blocks.1.resnets.1.norm2: GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "encoder.down_blocks.1.resnets.1.dropout: Dropout(p=0.0, inplace=False)\n",
      "encoder.down_blocks.1.resnets.1.conv2: LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.down_blocks.1.downsamplers: ModuleList(\n",
      "  (0): Downsample2D(\n",
      "    (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "  )\n",
      ")\n",
      "encoder.down_blocks.1.downsamplers.0: Downsample2D(\n",
      "  (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      ")\n",
      "encoder.down_blocks.1.downsamplers.0.conv: LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "encoder.down_blocks.2: DownEncoderBlock2D(\n",
      "  (resnets): ModuleList(\n",
      "    (0): ResnetBlock2D(\n",
      "      (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "      (conv1): LoRACompatibleConv(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "      (conv_shortcut): LoRACompatibleConv(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): ResnetBlock2D(\n",
      "      (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (downsamplers): ModuleList(\n",
      "    (0): Downsample2D(\n",
      "      (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "encoder.down_blocks.2.resnets: ModuleList(\n",
      "  (0): ResnetBlock2D(\n",
      "    (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "    (conv1): LoRACompatibleConv(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (nonlinearity): SiLU()\n",
      "    (conv_shortcut): LoRACompatibleConv(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (1): ResnetBlock2D(\n",
      "    (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (nonlinearity): SiLU()\n",
      "  )\n",
      ")\n",
      "encoder.down_blocks.2.resnets.0: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      "  (conv_shortcut): LoRACompatibleConv(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "encoder.down_blocks.2.resnets.0.norm1: GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "encoder.down_blocks.2.resnets.0.conv1: LoRACompatibleConv(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.down_blocks.2.resnets.0.norm2: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "encoder.down_blocks.2.resnets.0.dropout: Dropout(p=0.0, inplace=False)\n",
      "encoder.down_blocks.2.resnets.0.conv2: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.down_blocks.2.resnets.0.conv_shortcut: LoRACompatibleConv(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "encoder.down_blocks.2.resnets.1: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "encoder.down_blocks.2.resnets.1.norm1: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "encoder.down_blocks.2.resnets.1.conv1: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.down_blocks.2.resnets.1.norm2: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "encoder.down_blocks.2.resnets.1.dropout: Dropout(p=0.0, inplace=False)\n",
      "encoder.down_blocks.2.resnets.1.conv2: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.down_blocks.2.downsamplers: ModuleList(\n",
      "  (0): Downsample2D(\n",
      "    (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
      "  )\n",
      ")\n",
      "encoder.down_blocks.2.downsamplers.0: Downsample2D(\n",
      "  (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
      ")\n",
      "encoder.down_blocks.2.downsamplers.0.conv: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
      "encoder.down_blocks.3: DownEncoderBlock2D(\n",
      "  (resnets): ModuleList(\n",
      "    (0-1): 2 x ResnetBlock2D(\n",
      "      (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "encoder.down_blocks.3.resnets: ModuleList(\n",
      "  (0-1): 2 x ResnetBlock2D(\n",
      "    (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (nonlinearity): SiLU()\n",
      "  )\n",
      ")\n",
      "encoder.down_blocks.3.resnets.0: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "encoder.down_blocks.3.resnets.0.norm1: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "encoder.down_blocks.3.resnets.0.conv1: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.down_blocks.3.resnets.0.norm2: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "encoder.down_blocks.3.resnets.0.dropout: Dropout(p=0.0, inplace=False)\n",
      "encoder.down_blocks.3.resnets.0.conv2: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.down_blocks.3.resnets.1: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "encoder.down_blocks.3.resnets.1.norm1: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "encoder.down_blocks.3.resnets.1.conv1: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.down_blocks.3.resnets.1.norm2: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "encoder.down_blocks.3.resnets.1.dropout: Dropout(p=0.0, inplace=False)\n",
      "encoder.down_blocks.3.resnets.1.conv2: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.mid_block: UNetMidBlock2D(\n",
      "  (attentions): ModuleList(\n",
      "    (0): Attention(\n",
      "      (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (to_q): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "      (to_k): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "      (to_v): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "      (to_out): ModuleList(\n",
      "        (0): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (resnets): ModuleList(\n",
      "    (0-1): 2 x ResnetBlock2D(\n",
      "      (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "encoder.mid_block.attentions: ModuleList(\n",
      "  (0): Attention(\n",
      "    (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (to_q): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "    (to_k): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "    (to_v): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "encoder.mid_block.attentions.0: Attention(\n",
      "  (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (to_q): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "  (to_k): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "  (to_v): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "  (to_out): ModuleList(\n",
      "    (0): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "encoder.mid_block.attentions.0.group_norm: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "encoder.mid_block.attentions.0.to_q: LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "encoder.mid_block.attentions.0.to_k: LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "encoder.mid_block.attentions.0.to_v: LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "encoder.mid_block.attentions.0.to_out: ModuleList(\n",
      "  (0): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "  (1): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "encoder.mid_block.attentions.0.to_out.0: LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "encoder.mid_block.attentions.0.to_out.1: Dropout(p=0.0, inplace=False)\n",
      "encoder.mid_block.resnets: ModuleList(\n",
      "  (0-1): 2 x ResnetBlock2D(\n",
      "    (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (nonlinearity): SiLU()\n",
      "  )\n",
      ")\n",
      "encoder.mid_block.resnets.0: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "encoder.mid_block.resnets.0.norm1: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "encoder.mid_block.resnets.0.conv1: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.mid_block.resnets.0.norm2: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "encoder.mid_block.resnets.0.dropout: Dropout(p=0.0, inplace=False)\n",
      "encoder.mid_block.resnets.0.conv2: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.mid_block.resnets.1: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "encoder.mid_block.resnets.1.norm1: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "encoder.mid_block.resnets.1.conv1: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.mid_block.resnets.1.norm2: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "encoder.mid_block.resnets.1.dropout: Dropout(p=0.0, inplace=False)\n",
      "encoder.mid_block.resnets.1.conv2: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder.conv_norm_out: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "encoder.conv_act: SiLU()\n",
      "encoder.conv_out: Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder: Decoder(\n",
      "  (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (up_blocks): ModuleList(\n",
      "    (0-1): 2 x UpDecoderBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0-2): 3 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (upsamplers): ModuleList(\n",
      "        (0): Upsample2D(\n",
      "          (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): UpDecoderBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1-2): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (upsamplers): ModuleList(\n",
      "        (0): Upsample2D(\n",
      "          (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): UpDecoderBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1-2): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mid_block): UNetMidBlock2D(\n",
      "    (attentions): ModuleList(\n",
      "      (0): Attention(\n",
      "        (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (to_q): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "        (to_k): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "        (to_v): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "        (to_out): ModuleList(\n",
      "          (0): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (resnets): ModuleList(\n",
      "      (0-1): 2 x ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "  (conv_act): SiLU()\n",
      "  (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "decoder.conv_in: Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks: ModuleList(\n",
      "  (0-1): 2 x UpDecoderBlock2D(\n",
      "    (resnets): ModuleList(\n",
      "      (0-2): 3 x ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "    (upsamplers): ModuleList(\n",
      "      (0): Upsample2D(\n",
      "        (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): UpDecoderBlock2D(\n",
      "    (resnets): ModuleList(\n",
      "      (0): ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (conv1): LoRACompatibleConv(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "        (conv_shortcut): LoRACompatibleConv(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1-2): 2 x ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "        (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "    (upsamplers): ModuleList(\n",
      "      (0): Upsample2D(\n",
      "        (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): UpDecoderBlock2D(\n",
      "    (resnets): ModuleList(\n",
      "      (0): ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "        (conv1): LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "        (conv_shortcut): LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1-2): 2 x ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "        (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "decoder.up_blocks.0: UpDecoderBlock2D(\n",
      "  (resnets): ModuleList(\n",
      "    (0-2): 3 x ResnetBlock2D(\n",
      "      (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (upsamplers): ModuleList(\n",
      "    (0): Upsample2D(\n",
      "      (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "decoder.up_blocks.0.resnets: ModuleList(\n",
      "  (0-2): 3 x ResnetBlock2D(\n",
      "    (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (nonlinearity): SiLU()\n",
      "  )\n",
      ")\n",
      "decoder.up_blocks.0.resnets.0: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "decoder.up_blocks.0.resnets.0.norm1: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.0.resnets.0.conv1: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.0.resnets.0.norm2: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.0.resnets.0.dropout: Dropout(p=0.0, inplace=False)\n",
      "decoder.up_blocks.0.resnets.0.conv2: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.0.resnets.1: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "decoder.up_blocks.0.resnets.1.norm1: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.0.resnets.1.conv1: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.0.resnets.1.norm2: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.0.resnets.1.dropout: Dropout(p=0.0, inplace=False)\n",
      "decoder.up_blocks.0.resnets.1.conv2: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.0.resnets.2: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "decoder.up_blocks.0.resnets.2.norm1: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.0.resnets.2.conv1: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.0.resnets.2.norm2: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.0.resnets.2.dropout: Dropout(p=0.0, inplace=False)\n",
      "decoder.up_blocks.0.resnets.2.conv2: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.0.upsamplers: ModuleList(\n",
      "  (0): Upsample2D(\n",
      "    (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "decoder.up_blocks.0.upsamplers.0: Upsample2D(\n",
      "  (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "decoder.up_blocks.0.upsamplers.0.conv: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.1: UpDecoderBlock2D(\n",
      "  (resnets): ModuleList(\n",
      "    (0-2): 3 x ResnetBlock2D(\n",
      "      (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (upsamplers): ModuleList(\n",
      "    (0): Upsample2D(\n",
      "      (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "decoder.up_blocks.1.resnets: ModuleList(\n",
      "  (0-2): 3 x ResnetBlock2D(\n",
      "    (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (nonlinearity): SiLU()\n",
      "  )\n",
      ")\n",
      "decoder.up_blocks.1.resnets.0: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "decoder.up_blocks.1.resnets.0.norm1: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.1.resnets.0.conv1: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.1.resnets.0.norm2: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.1.resnets.0.dropout: Dropout(p=0.0, inplace=False)\n",
      "decoder.up_blocks.1.resnets.0.conv2: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.1.resnets.1: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "decoder.up_blocks.1.resnets.1.norm1: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.1.resnets.1.conv1: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.1.resnets.1.norm2: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.1.resnets.1.dropout: Dropout(p=0.0, inplace=False)\n",
      "decoder.up_blocks.1.resnets.1.conv2: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.1.resnets.2: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "decoder.up_blocks.1.resnets.2.norm1: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.1.resnets.2.conv1: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.1.resnets.2.norm2: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.1.resnets.2.dropout: Dropout(p=0.0, inplace=False)\n",
      "decoder.up_blocks.1.resnets.2.conv2: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.1.upsamplers: ModuleList(\n",
      "  (0): Upsample2D(\n",
      "    (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "decoder.up_blocks.1.upsamplers.0: Upsample2D(\n",
      "  (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "decoder.up_blocks.1.upsamplers.0.conv: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.2: UpDecoderBlock2D(\n",
      "  (resnets): ModuleList(\n",
      "    (0): ResnetBlock2D(\n",
      "      (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (conv1): LoRACompatibleConv(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "      (conv_shortcut): LoRACompatibleConv(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1-2): 2 x ResnetBlock2D(\n",
      "      (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "      (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (upsamplers): ModuleList(\n",
      "    (0): Upsample2D(\n",
      "      (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "decoder.up_blocks.2.resnets: ModuleList(\n",
      "  (0): ResnetBlock2D(\n",
      "    (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (conv1): LoRACompatibleConv(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (nonlinearity): SiLU()\n",
      "    (conv_shortcut): LoRACompatibleConv(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (1-2): 2 x ResnetBlock2D(\n",
      "    (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "    (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (nonlinearity): SiLU()\n",
      "  )\n",
      ")\n",
      "decoder.up_blocks.2.resnets.0: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      "  (conv_shortcut): LoRACompatibleConv(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "decoder.up_blocks.2.resnets.0.norm1: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.2.resnets.0.conv1: LoRACompatibleConv(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.2.resnets.0.norm2: GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.2.resnets.0.dropout: Dropout(p=0.0, inplace=False)\n",
      "decoder.up_blocks.2.resnets.0.conv2: LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.2.resnets.0.conv_shortcut: LoRACompatibleConv(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "decoder.up_blocks.2.resnets.1: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "decoder.up_blocks.2.resnets.1.norm1: GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.2.resnets.1.conv1: LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.2.resnets.1.norm2: GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.2.resnets.1.dropout: Dropout(p=0.0, inplace=False)\n",
      "decoder.up_blocks.2.resnets.1.conv2: LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.2.resnets.2: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "decoder.up_blocks.2.resnets.2.norm1: GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.2.resnets.2.conv1: LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.2.resnets.2.norm2: GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.2.resnets.2.dropout: Dropout(p=0.0, inplace=False)\n",
      "decoder.up_blocks.2.resnets.2.conv2: LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.2.upsamplers: ModuleList(\n",
      "  (0): Upsample2D(\n",
      "    (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "decoder.up_blocks.2.upsamplers.0: Upsample2D(\n",
      "  (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n",
      "decoder.up_blocks.2.upsamplers.0.conv: LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.3: UpDecoderBlock2D(\n",
      "  (resnets): ModuleList(\n",
      "    (0): ResnetBlock2D(\n",
      "      (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "      (conv1): LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "      (conv_shortcut): LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1-2): 2 x ResnetBlock2D(\n",
      "      (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "      (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "decoder.up_blocks.3.resnets: ModuleList(\n",
      "  (0): ResnetBlock2D(\n",
      "    (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "    (conv1): LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (nonlinearity): SiLU()\n",
      "    (conv_shortcut): LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (1-2): 2 x ResnetBlock2D(\n",
      "    (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "    (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (nonlinearity): SiLU()\n",
      "  )\n",
      ")\n",
      "decoder.up_blocks.3.resnets.0: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      "  (conv_shortcut): LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "decoder.up_blocks.3.resnets.0.norm1: GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.3.resnets.0.conv1: LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.3.resnets.0.norm2: GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.3.resnets.0.dropout: Dropout(p=0.0, inplace=False)\n",
      "decoder.up_blocks.3.resnets.0.conv2: LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.3.resnets.0.conv_shortcut: LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "decoder.up_blocks.3.resnets.1: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "decoder.up_blocks.3.resnets.1.norm1: GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.3.resnets.1.conv1: LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.3.resnets.1.norm2: GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.3.resnets.1.dropout: Dropout(p=0.0, inplace=False)\n",
      "decoder.up_blocks.3.resnets.1.conv2: LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.3.resnets.2: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "decoder.up_blocks.3.resnets.2.norm1: GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.3.resnets.2.conv1: LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.up_blocks.3.resnets.2.norm2: GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "decoder.up_blocks.3.resnets.2.dropout: Dropout(p=0.0, inplace=False)\n",
      "decoder.up_blocks.3.resnets.2.conv2: LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.mid_block: UNetMidBlock2D(\n",
      "  (attentions): ModuleList(\n",
      "    (0): Attention(\n",
      "      (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (to_q): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "      (to_k): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "      (to_v): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "      (to_out): ModuleList(\n",
      "        (0): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "        (1): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (resnets): ModuleList(\n",
      "    (0-1): 2 x ResnetBlock2D(\n",
      "      (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (nonlinearity): SiLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "decoder.mid_block.attentions: ModuleList(\n",
      "  (0): Attention(\n",
      "    (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (to_q): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "    (to_k): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "    (to_v): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "    (to_out): ModuleList(\n",
      "      (0): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "      (1): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "decoder.mid_block.attentions.0: Attention(\n",
      "  (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (to_q): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "  (to_k): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "  (to_v): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "  (to_out): ModuleList(\n",
      "    (0): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "    (1): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "decoder.mid_block.attentions.0.group_norm: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.mid_block.attentions.0.to_q: LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "decoder.mid_block.attentions.0.to_k: LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "decoder.mid_block.attentions.0.to_v: LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "decoder.mid_block.attentions.0.to_out: ModuleList(\n",
      "  (0): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "  (1): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "decoder.mid_block.attentions.0.to_out.0: LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "decoder.mid_block.attentions.0.to_out.1: Dropout(p=0.0, inplace=False)\n",
      "decoder.mid_block.resnets: ModuleList(\n",
      "  (0-1): 2 x ResnetBlock2D(\n",
      "    (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (nonlinearity): SiLU()\n",
      "  )\n",
      ")\n",
      "decoder.mid_block.resnets.0: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "decoder.mid_block.resnets.0.norm1: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.mid_block.resnets.0.conv1: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.mid_block.resnets.0.norm2: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.mid_block.resnets.0.dropout: Dropout(p=0.0, inplace=False)\n",
      "decoder.mid_block.resnets.0.conv2: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.mid_block.resnets.1: ResnetBlock2D(\n",
      "  (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (nonlinearity): SiLU()\n",
      ")\n",
      "decoder.mid_block.resnets.1.norm1: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.mid_block.resnets.1.conv1: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.mid_block.resnets.1.norm2: GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "decoder.mid_block.resnets.1.dropout: Dropout(p=0.0, inplace=False)\n",
      "decoder.mid_block.resnets.1.conv2: LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "decoder.conv_norm_out: GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "decoder.conv_act: SiLU()\n",
      "decoder.conv_out: Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "quant_conv: Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "post_quant_conv: Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T03:14:44.153869Z",
     "start_time": "2024-10-19T03:14:44.145576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder = vae.encoder\n",
    "decoder = vae.decoder\n",
    "\n",
    "# Print the encoder structure\n",
    "print(encoder)"
   ],
   "id": "a66592eee7882318",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (down_blocks): ModuleList(\n",
      "    (0): DownEncoderBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): DownEncoderBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): DownEncoderBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (downsamplers): ModuleList(\n",
      "        (0): Downsample2D(\n",
      "          (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): DownEncoderBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0-1): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mid_block): UNetMidBlock2D(\n",
      "    (attentions): ModuleList(\n",
      "      (0): Attention(\n",
      "        (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (to_q): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "        (to_k): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "        (to_v): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "        (to_out): ModuleList(\n",
      "          (0): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (resnets): ModuleList(\n",
      "      (0-1): 2 x ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "  (conv_act): SiLU()\n",
      "  (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T03:14:44.830593Z",
     "start_time": "2024-10-19T03:14:44.821451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the decoder structure\n",
    "print(decoder)"
   ],
   "id": "2bd5237ce8150a17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (up_blocks): ModuleList(\n",
      "    (0-1): 2 x UpDecoderBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0-2): 3 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (upsamplers): ModuleList(\n",
      "        (0): Upsample2D(\n",
      "          (conv): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): UpDecoderBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1-2): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "      (upsamplers): ModuleList(\n",
      "        (0): Upsample2D(\n",
      "          (conv): LoRACompatibleConv(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): UpDecoderBlock2D(\n",
      "      (resnets): ModuleList(\n",
      "        (0): ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "          (conv_shortcut): LoRACompatibleConv(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1-2): 2 x ResnetBlock2D(\n",
      "          (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (nonlinearity): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mid_block): UNetMidBlock2D(\n",
      "    (attentions): ModuleList(\n",
      "      (0): Attention(\n",
      "        (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (to_q): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "        (to_k): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "        (to_v): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "        (to_out): ModuleList(\n",
      "          (0): LoRACompatibleLinear(in_features=512, out_features=512, bias=True)\n",
      "          (1): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (resnets): ModuleList(\n",
      "      (0-1): 2 x ResnetBlock2D(\n",
      "        (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (conv1): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (conv2): LoRACompatibleConv(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (nonlinearity): SiLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "  (conv_act): SiLU()\n",
      "  (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T03:14:46.878214Z",
     "start_time": "2024-10-19T03:14:45.353941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 载入lora weights，打印观察结构\n",
    "# lora_model_id = \"./checkpoints/georgefen-AquaLoRA-Models/models--georgefen--AquaLoRA-Models/snapshots/98688b2a1e762339593ee8fe96ed13762f06b732/ppft_trained/101010101010101010101010101010101010101010101010/pytorch_lora_weights.safetensors\"\n",
    "lora_model_id = \"./checkpoints/pytorch_lora_weights.safetensors\"\n",
    "lora = AutoencoderKL.from_pretrained(lora_model_id, device_map=\"auto\",\n",
    "                                     low_cpu_mem_usage=True,\n",
    "                                     use_safetensors=True)\n",
    "print(lora)"
   ],
   "id": "95df668f4c023cd3",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "It looks like the config file at './checkpoints/pytorch_lora_weights.safetensors' is not a valid JSON file.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "File \u001B[0;32m/NEW_EDS/JJ_Group/shaoyh/env/aqualora/lib/python3.10/site-packages/diffusers/configuration_utils.py:424\u001B[0m, in \u001B[0;36mConfigMixin.load_config\u001B[0;34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001B[0m\n\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    423\u001B[0m     \u001B[38;5;66;03m# Load config dict\u001B[39;00m\n\u001B[0;32m--> 424\u001B[0m     config_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dict_from_json_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig_file\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    426\u001B[0m     commit_hash \u001B[38;5;241m=\u001B[39m extract_commit_hash(config_file)\n",
      "File \u001B[0;32m/NEW_EDS/JJ_Group/shaoyh/env/aqualora/lib/python3.10/site-packages/diffusers/configuration_utils.py:546\u001B[0m, in \u001B[0;36mConfigMixin._dict_from_json_file\u001B[0;34m(cls, json_file)\u001B[0m\n\u001B[1;32m    545\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(json_file, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m reader:\n\u001B[0;32m--> 546\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[43mreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    547\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m json\u001B[38;5;241m.\u001B[39mloads(text)\n",
      "File \u001B[0;32m/NEW_EDS/JJ_Group/shaoyh/env/aqualora/lib/python3.10/codecs.py:322\u001B[0m, in \u001B[0;36mBufferedIncrementalDecoder.decode\u001B[0;34m(self, input, final)\u001B[0m\n\u001B[1;32m    321\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer \u001B[38;5;241m+\u001B[39m \u001B[38;5;28minput\u001B[39m\n\u001B[0;32m--> 322\u001B[0m (result, consumed) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_buffer_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfinal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    323\u001B[0m \u001B[38;5;66;03m# keep undecoded input until the next call\u001B[39;00m\n",
      "\u001B[0;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0xd0 in position 0: invalid continuation byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 载入lora weights，打印观察结构\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# lora_model_id = \"./checkpoints/georgefen-AquaLoRA-Models/models--georgefen--AquaLoRA-Models/snapshots/98688b2a1e762339593ee8fe96ed13762f06b732/ppft_trained/101010101010101010101010101010101010101010101010/pytorch_lora_weights.safetensors\"\u001B[39;00m\n\u001B[1;32m      3\u001B[0m lora_model_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./checkpoints/pytorch_lora_weights.safetensors\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 4\u001B[0m lora \u001B[38;5;241m=\u001B[39m \u001B[43mAutoencoderKL\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlora_model_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                                     \u001B[49m\u001B[43mlow_cpu_mem_usage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m                                     \u001B[49m\u001B[43muse_safetensors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(lora)\n",
      "File \u001B[0;32m/NEW_EDS/JJ_Group/shaoyh/env/aqualora/lib/python3.10/site-packages/diffusers/models/modeling_utils.py:712\u001B[0m, in \u001B[0;36mModelMixin.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    705\u001B[0m user_agent \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdiffusers\u001B[39m\u001B[38;5;124m\"\u001B[39m: __version__,\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    708\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mframework\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpytorch\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    709\u001B[0m }\n\u001B[1;32m    711\u001B[0m \u001B[38;5;66;03m# load config\u001B[39;00m\n\u001B[0;32m--> 712\u001B[0m config, unused_kwargs, commit_hash \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    713\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    714\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    715\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_unused_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    716\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    717\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    718\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    719\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    720\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    721\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    722\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    723\u001B[0m \u001B[43m    \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    724\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    725\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_memory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    726\u001B[0m \u001B[43m    \u001B[49m\u001B[43moffload_folder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_folder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    727\u001B[0m \u001B[43m    \u001B[49m\u001B[43moffload_state_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffload_state_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    728\u001B[0m \u001B[43m    \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    729\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    730\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    732\u001B[0m \u001B[38;5;66;03m# load model\u001B[39;00m\n\u001B[1;32m    733\u001B[0m model_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/NEW_EDS/JJ_Group/shaoyh/env/aqualora/lib/python3.10/site-packages/diffusers/configuration_utils.py:428\u001B[0m, in \u001B[0;36mConfigMixin.load_config\u001B[0;34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001B[0m\n\u001B[1;32m    426\u001B[0m     commit_hash \u001B[38;5;241m=\u001B[39m extract_commit_hash(config_file)\n\u001B[1;32m    427\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (json\u001B[38;5;241m.\u001B[39mJSONDecodeError, \u001B[38;5;167;01mUnicodeDecodeError\u001B[39;00m):\n\u001B[0;32m--> 428\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIt looks like the config file at \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is not a valid JSON file.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    430\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (return_unused_kwargs \u001B[38;5;129;01mor\u001B[39;00m return_commit_hash):\n\u001B[1;32m    431\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m config_dict\n",
      "\u001B[0;31mOSError\u001B[0m: It looks like the config file at './checkpoints/pytorch_lora_weights.safetensors' is not a valid JSON file."
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T03:14:47.923815Z",
     "start_time": "2024-10-19T03:14:47.514038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 这个是合并了水印的lora模型\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "filename = \"./checkpoints/pytorch_lora_weights.safetensors\"\n",
    "model = load_file(filename)\n",
    "state_dict = model\n",
    "for key, value in model.items():\n",
    "    print(key, value)\n"
   ],
   "id": "f965c1b423d4948a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet.down_blocks.0.attentions.0.proj_in.lora.down.weight tensor([[[[ 1.8030e-03]],\n",
      "\n",
      "         [[-6.4340e-04]],\n",
      "\n",
      "         [[ 3.2492e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.9049e-04]],\n",
      "\n",
      "         [[ 2.3294e-03]],\n",
      "\n",
      "         [[-4.4623e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 9.2670e-05]],\n",
      "\n",
      "         [[ 4.2637e-04]],\n",
      "\n",
      "         [[-1.1674e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.6520e-04]],\n",
      "\n",
      "         [[ 2.9646e-04]],\n",
      "\n",
      "         [[ 2.8687e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1780e-03]],\n",
      "\n",
      "         [[-9.8284e-04]],\n",
      "\n",
      "         [[ 3.1857e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.5395e-04]],\n",
      "\n",
      "         [[ 8.0614e-04]],\n",
      "\n",
      "         [[-6.4031e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.1940e-04]],\n",
      "\n",
      "         [[-7.5481e-04]],\n",
      "\n",
      "         [[-9.8031e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.8931e-04]],\n",
      "\n",
      "         [[-1.7594e-03]],\n",
      "\n",
      "         [[ 2.3879e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.9110e-03]],\n",
      "\n",
      "         [[-6.1994e-03]],\n",
      "\n",
      "         [[ 2.8201e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.5762e-03]],\n",
      "\n",
      "         [[ 1.2254e-02]],\n",
      "\n",
      "         [[ 5.0766e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9647e-03]],\n",
      "\n",
      "         [[-2.2836e-03]],\n",
      "\n",
      "         [[ 1.4742e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1967e-03]],\n",
      "\n",
      "         [[-8.0857e-04]],\n",
      "\n",
      "         [[-1.0172e-03]]]])\n",
      "unet.down_blocks.0.attentions.0.proj_in.lora.up.weight tensor([[[[-0.0024]],\n",
      "\n",
      "         [[ 0.0023]],\n",
      "\n",
      "         [[-0.0101]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0042]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[-0.0020]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0026]],\n",
      "\n",
      "         [[ 0.0030]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0003]],\n",
      "\n",
      "         [[-0.0043]],\n",
      "\n",
      "         [[ 0.0073]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0023]],\n",
      "\n",
      "         [[ 0.0069]],\n",
      "\n",
      "         [[-0.0008]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         [[-0.0035]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0036]],\n",
      "\n",
      "         [[-0.0058]],\n",
      "\n",
      "         [[ 0.0074]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         [[-0.0106]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0066]],\n",
      "\n",
      "         [[ 0.0055]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0010]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[ 0.0014]]],\n",
      "\n",
      "\n",
      "        [[[-0.0044]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0074]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[ 0.0086]]]])\n",
      "unet.down_blocks.0.attentions.0.proj_out.lora.down.weight tensor([[[[ 3.8933e-03]],\n",
      "\n",
      "         [[ 1.8271e-05]],\n",
      "\n",
      "         [[ 5.7453e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.8159e-03]],\n",
      "\n",
      "         [[ 4.4120e-03]],\n",
      "\n",
      "         [[-4.6614e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.4644e-03]],\n",
      "\n",
      "         [[-6.4128e-04]],\n",
      "\n",
      "         [[-6.8944e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.5772e-04]],\n",
      "\n",
      "         [[-9.7109e-04]],\n",
      "\n",
      "         [[ 2.0321e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.8830e-03]],\n",
      "\n",
      "         [[-1.8623e-03]],\n",
      "\n",
      "         [[-7.7525e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.0958e-03]],\n",
      "\n",
      "         [[ 3.4337e-03]],\n",
      "\n",
      "         [[ 2.5203e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.2712e-03]],\n",
      "\n",
      "         [[ 5.5259e-03]],\n",
      "\n",
      "         [[-9.8766e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.2044e-03]],\n",
      "\n",
      "         [[ 4.9796e-03]],\n",
      "\n",
      "         [[-9.7226e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0650e-04]],\n",
      "\n",
      "         [[-1.6756e-02]],\n",
      "\n",
      "         [[ 3.3883e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8447e-02]],\n",
      "\n",
      "         [[ 1.3508e-03]],\n",
      "\n",
      "         [[-1.6748e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.7503e-03]],\n",
      "\n",
      "         [[-1.1285e-03]],\n",
      "\n",
      "         [[-3.8416e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.2834e-04]],\n",
      "\n",
      "         [[ 6.6988e-04]],\n",
      "\n",
      "         [[ 2.3349e-03]]]])\n",
      "unet.down_blocks.0.attentions.0.proj_out.lora.up.weight tensor([[[[-0.0030]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         [[ 0.0052]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[-0.0084]],\n",
      "\n",
      "         [[-0.0029]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0028]],\n",
      "\n",
      "         [[ 0.0012]],\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[-0.0015]]],\n",
      "\n",
      "\n",
      "        [[[-0.0090]],\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         [[-0.0041]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0050]],\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         [[ 0.0044]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0103]],\n",
      "\n",
      "         [[ 0.0112]],\n",
      "\n",
      "         [[ 0.0024]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0010]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         [[ 0.0101]]],\n",
      "\n",
      "\n",
      "        [[[-0.0028]],\n",
      "\n",
      "         [[-0.0110]],\n",
      "\n",
      "         [[-0.0057]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0036]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[-0.0020]]],\n",
      "\n",
      "\n",
      "        [[[-0.0024]],\n",
      "\n",
      "         [[ 0.0072]],\n",
      "\n",
      "         [[ 0.0051]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         [[ 0.0001]],\n",
      "\n",
      "         [[ 0.0014]]]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight tensor([[ 0.0033, -0.0006,  0.0010,  ...,  0.0051,  0.0030,  0.0015],\n",
      "        [-0.0011, -0.0003, -0.0002,  ..., -0.0005,  0.0011,  0.0002],\n",
      "        [ 0.0032, -0.0032,  0.0018,  ..., -0.0014, -0.0031, -0.0059],\n",
      "        ...,\n",
      "        [ 0.0043,  0.0041, -0.0003,  ..., -0.0002,  0.0017, -0.0023],\n",
      "        [ 0.0012,  0.0153,  0.0005,  ..., -0.0026, -0.0012,  0.0045],\n",
      "        [ 0.0016,  0.0017, -0.0016,  ...,  0.0010,  0.0034,  0.0033]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight tensor([[ 2.9920e-03, -1.0507e-03, -1.0934e-03,  ...,  5.6809e-03,\n",
      "          6.2737e-03, -2.9567e-03],\n",
      "        [ 2.5939e-03,  9.3724e-04, -7.7603e-03,  ..., -1.9042e-03,\n",
      "          5.9859e-03, -6.2347e-03],\n",
      "        [-1.4211e-02,  3.4130e-03, -1.2161e-03,  ...,  2.0769e-03,\n",
      "          1.0663e-02, -3.1851e-03],\n",
      "        ...,\n",
      "        [ 4.6098e-03, -2.7930e-03, -1.7797e-03,  ...,  5.5384e-04,\n",
      "         -3.5034e-03, -1.9354e-03],\n",
      "        [-1.2643e-03, -1.8099e-03, -2.2602e-03,  ...,  1.7232e-03,\n",
      "          1.2067e-05,  3.1870e-03],\n",
      "        [-1.1149e-02, -5.9707e-03, -4.7565e-03,  ...,  3.7991e-03,\n",
      "          9.1035e-03,  7.2870e-03]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight tensor([[ 6.0569e-03,  7.1075e-03, -2.4147e-03,  ...,  1.9043e-03,\n",
      "         -2.4024e-03,  3.4112e-03],\n",
      "        [ 1.3983e-04, -4.6636e-04, -2.6827e-04,  ..., -6.6071e-05,\n",
      "          8.9518e-04, -7.4264e-04],\n",
      "        [-2.7093e-04, -2.8494e-03,  3.1267e-03,  ...,  2.3505e-03,\n",
      "         -1.4015e-03,  4.7431e-03],\n",
      "        ...,\n",
      "        [ 6.2685e-04, -4.0183e-04,  2.9179e-03,  ...,  1.1103e-03,\n",
      "         -5.1636e-04,  5.2492e-03],\n",
      "        [ 2.1797e-03, -6.0161e-04, -4.9343e-03,  ...,  8.8511e-03,\n",
      "          3.1834e-03,  8.3906e-03],\n",
      "        [ 2.7559e-04, -3.6255e-04,  8.4463e-04,  ...,  3.1387e-03,\n",
      "          2.9796e-03, -7.1461e-04]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight tensor([[ 0.0002, -0.0040,  0.0037,  ...,  0.0028,  0.0104, -0.0052],\n",
      "        [-0.0034,  0.0007,  0.0070,  ...,  0.0020, -0.0045, -0.0031],\n",
      "        [-0.0025,  0.0072,  0.0061,  ...,  0.0039, -0.0028, -0.0009],\n",
      "        ...,\n",
      "        [-0.0019,  0.0013,  0.0034,  ...,  0.0040, -0.0055, -0.0089],\n",
      "        [-0.0061,  0.0046,  0.0046,  ...,  0.0036, -0.0014, -0.0005],\n",
      "        [ 0.0017,  0.0131,  0.0041,  ...,  0.0070, -0.0035, -0.0100]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight tensor([[ 1.9492e-03,  2.9824e-03, -2.2386e-03,  ..., -8.5636e-04,\n",
      "          1.8055e-03,  2.8220e-03],\n",
      "        [ 2.2509e-04, -8.7519e-04,  2.2018e-04,  ..., -1.1876e-03,\n",
      "          1.5710e-04,  7.0340e-04],\n",
      "        [ 3.1816e-03,  7.6628e-03, -2.1211e-03,  ..., -2.8918e-04,\n",
      "          2.4604e-03,  1.0585e-03],\n",
      "        ...,\n",
      "        [ 5.9518e-03,  3.6644e-03,  1.2566e-03,  ..., -3.9343e-03,\n",
      "          4.1183e-04,  1.3343e-03],\n",
      "        [-2.9966e-03,  2.1016e-03,  5.6240e-03,  ...,  9.1686e-03,\n",
      "          1.3208e-02,  6.8961e-03],\n",
      "        [ 3.2476e-03, -2.9971e-03, -4.3398e-03,  ...,  1.1909e-03,\n",
      "          7.0171e-05,  3.1234e-03]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight tensor([[-4.0427e-03,  1.4557e-03,  1.8350e-04,  ...,  2.5958e-03,\n",
      "          4.0318e-03, -6.5469e-03],\n",
      "        [ 3.2521e-03, -2.5751e-03, -4.6226e-04,  ...,  5.3004e-03,\n",
      "          7.1366e-03,  9.2916e-03],\n",
      "        [-1.5103e-03,  2.8812e-03,  6.8930e-03,  ..., -2.5148e-03,\n",
      "         -2.8178e-03,  9.9661e-03],\n",
      "        ...,\n",
      "        [ 3.3109e-03,  4.6051e-03, -7.5339e-03,  ..., -6.3745e-03,\n",
      "          1.1121e-03,  1.7892e-03],\n",
      "        [ 5.2172e-04, -1.9417e-03,  6.3243e-04,  ...,  1.4785e-03,\n",
      "         -4.6879e-04, -6.4951e-04],\n",
      "        [-1.4512e-03,  1.7483e-03, -4.9664e-03,  ...,  3.0707e-03,\n",
      "          6.4800e-03,  8.3908e-05]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight tensor([[-2.1813e-03,  1.3048e-03,  3.5221e-04,  ..., -2.7954e-03,\n",
      "          2.9604e-03,  5.9085e-05],\n",
      "        [-8.9125e-04, -1.4937e-03, -7.6622e-04,  ..., -1.3673e-03,\n",
      "          1.1592e-03, -1.3436e-03],\n",
      "        [-5.2846e-03,  1.6637e-04, -5.5083e-05,  ..., -2.9653e-03,\n",
      "          7.0694e-04, -1.2216e-03],\n",
      "        ...,\n",
      "        [-5.9512e-04, -4.0174e-03,  2.1353e-03,  ...,  2.6676e-03,\n",
      "         -2.8474e-03,  1.2028e-03],\n",
      "        [ 6.0131e-03,  5.5668e-03,  7.0762e-03,  ...,  9.0335e-03,\n",
      "         -1.2795e-02, -1.0314e-02],\n",
      "        [-2.9963e-03,  4.4955e-03, -8.3254e-04,  ..., -2.4115e-03,\n",
      "          1.4572e-03, -6.2352e-03]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight tensor([[ 0.0039, -0.0112, -0.0075,  ...,  0.0066,  0.0034,  0.0091],\n",
      "        [ 0.0019, -0.0013,  0.0047,  ..., -0.0041, -0.0011, -0.0072],\n",
      "        [ 0.0045, -0.0136,  0.0012,  ..., -0.0037,  0.0028, -0.0038],\n",
      "        ...,\n",
      "        [ 0.0034, -0.0093, -0.0054,  ...,  0.0029,  0.0049, -0.0046],\n",
      "        [ 0.0017, -0.0034,  0.0015,  ..., -0.0135, -0.0041,  0.0004],\n",
      "        [ 0.0089,  0.0132, -0.0047,  ..., -0.0045,  0.0030,  0.0009]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight tensor([[ 8.5085e-05,  1.5710e-03, -1.0380e-03,  ...,  1.0200e-03,\n",
      "         -2.2225e-03, -4.6040e-04],\n",
      "        [ 1.4570e-04, -3.1768e-04,  1.0725e-04,  ...,  9.9753e-04,\n",
      "          2.9241e-06,  7.1668e-04],\n",
      "        [-4.8403e-04, -2.8630e-04, -2.5318e-04,  ..., -1.0904e-04,\n",
      "         -8.5580e-04, -1.4972e-04],\n",
      "        ...,\n",
      "        [-2.6476e-03, -6.3485e-04, -2.7688e-03,  ..., -5.9896e-04,\n",
      "         -1.1874e-03,  4.5971e-03],\n",
      "        [ 7.0297e-03, -4.5615e-03, -1.4116e-03,  ..., -4.6189e-03,\n",
      "         -6.9848e-03,  1.1628e-02],\n",
      "        [-6.6773e-04,  2.0865e-03,  2.2580e-04,  ..., -2.0335e-03,\n",
      "          6.5704e-04,  1.1855e-03]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight tensor([[ 0.0033,  0.0042, -0.0025,  ...,  0.0056,  0.0010,  0.0008],\n",
      "        [ 0.0081,  0.0039, -0.0041,  ..., -0.0068,  0.0014,  0.0044],\n",
      "        [ 0.0025, -0.0021,  0.0059,  ..., -0.0010,  0.0012,  0.0029],\n",
      "        ...,\n",
      "        [ 0.0028, -0.0034,  0.0042,  ...,  0.0024, -0.0032, -0.0033],\n",
      "        [ 0.0018, -0.0001,  0.0037,  ..., -0.0018, -0.0032,  0.0061],\n",
      "        [-0.0026, -0.0016,  0.0011,  ...,  0.0025,  0.0045, -0.0033]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight tensor([[-0.0029,  0.0002, -0.0043,  ..., -0.0020,  0.0020,  0.0038],\n",
      "        [-0.0010,  0.0004,  0.0005,  ..., -0.0003, -0.0012, -0.0002],\n",
      "        [-0.0014, -0.0019, -0.0058,  ..., -0.0026,  0.0013, -0.0041],\n",
      "        ...,\n",
      "        [ 0.0028, -0.0048,  0.0005,  ...,  0.0031,  0.0050, -0.0003],\n",
      "        [ 0.0023,  0.0041,  0.0010,  ...,  0.0030,  0.0066,  0.0041],\n",
      "        [ 0.0087, -0.0052,  0.0017,  ..., -0.0016,  0.0020, -0.0036]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight tensor([[-0.0026, -0.0014, -0.0056,  ...,  0.0103, -0.0028,  0.0090],\n",
      "        [-0.0106,  0.0062, -0.0019,  ...,  0.0191, -0.0063, -0.0040],\n",
      "        [ 0.0102, -0.0003,  0.0059,  ..., -0.0104,  0.0010,  0.0126],\n",
      "        ...,\n",
      "        [ 0.0035, -0.0025, -0.0007,  ..., -0.0008, -0.0060,  0.0090],\n",
      "        [ 0.0001,  0.0047, -0.0070,  ...,  0.0042, -0.0081,  0.0171],\n",
      "        [ 0.0042,  0.0067, -0.0056,  ...,  0.0074,  0.0097,  0.0178]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight tensor([[ 0.0020, -0.0046, -0.0019,  ...,  0.0025,  0.0001,  0.0002],\n",
      "        [ 0.0004, -0.0003, -0.0014,  ...,  0.0005,  0.0008,  0.0002],\n",
      "        [-0.0067, -0.0046,  0.0009,  ...,  0.0019, -0.0059,  0.0064],\n",
      "        ...,\n",
      "        [ 0.0023,  0.0054, -0.0011,  ...,  0.0098, -0.0008,  0.0033],\n",
      "        [-0.0066,  0.0005, -0.0040,  ..., -0.0058, -0.0126, -0.0067],\n",
      "        [ 0.0048, -0.0030, -0.0001,  ..., -0.0008,  0.0008,  0.0008]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight tensor([[-1.7793e-03,  6.2000e-03, -3.5821e-03,  ...,  6.0751e-03,\n",
      "         -4.7896e-03, -6.4086e-03],\n",
      "        [ 3.1086e-03,  3.5364e-04, -1.8037e-03,  ...,  1.3282e-03,\n",
      "         -2.7917e-03, -1.1467e-03],\n",
      "        [ 1.2521e-03, -1.2064e-03, -1.5969e-03,  ...,  2.4116e-03,\n",
      "         -1.1351e-03,  3.3437e-03],\n",
      "        ...,\n",
      "        [ 8.1110e-04,  1.4884e-03,  3.7755e-03,  ...,  4.4522e-04,\n",
      "          1.1743e-03, -9.5418e-05],\n",
      "        [-5.5103e-04,  1.9580e-03, -3.5026e-03,  ..., -8.0001e-04,\n",
      "          1.5134e-03, -1.5724e-04],\n",
      "        [-1.5746e-04,  3.8379e-04,  1.9362e-03,  ...,  3.5432e-03,\n",
      "          2.8307e-03,  2.2154e-03]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight tensor([[-3.0523e-03, -3.1633e-03,  1.2415e-03,  ..., -6.8399e-04,\n",
      "          1.3300e-03,  2.8741e-03],\n",
      "        [-1.2202e-03, -6.1147e-04,  4.6224e-04,  ..., -9.9344e-04,\n",
      "         -5.0251e-04, -6.2804e-04],\n",
      "        [ 8.7238e-04,  6.5563e-04, -3.7094e-03,  ...,  9.2120e-04,\n",
      "          1.8410e-05, -7.9768e-04],\n",
      "        ...,\n",
      "        [ 2.7654e-05, -1.5357e-03,  2.2596e-03,  ..., -2.2802e-03,\n",
      "         -1.7873e-03, -2.1484e-04],\n",
      "        [ 5.9369e-03, -1.7705e-02,  6.7352e-03,  ...,  3.0221e-03,\n",
      "          6.2639e-03,  1.1865e-02],\n",
      "        [-8.7178e-04, -1.1281e-03,  2.4181e-05,  ...,  2.4352e-03,\n",
      "         -1.2448e-03,  2.4033e-03]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight tensor([[-5.5407e-03,  6.2238e-03, -1.8213e-03,  ...,  4.9749e-03,\n",
      "          1.2074e-03, -6.0521e-03],\n",
      "        [-4.3724e-03,  1.4415e-03, -1.7427e-03,  ...,  4.2122e-03,\n",
      "          3.6350e-05,  1.0687e-03],\n",
      "        [-6.7283e-03,  1.5840e-03,  7.2794e-04,  ...,  5.4378e-03,\n",
      "         -4.4928e-03,  3.6730e-03],\n",
      "        ...,\n",
      "        [-1.8607e-03,  2.7997e-03,  1.2020e-03,  ...,  2.5962e-03,\n",
      "         -1.2432e-03, -4.6829e-03],\n",
      "        [ 1.0867e-03,  3.7787e-03, -2.5664e-03,  ..., -8.7857e-03,\n",
      "          2.2770e-03, -1.7731e-04],\n",
      "        [-2.8134e-03,  3.3380e-03, -9.3260e-03,  ..., -7.5001e-03,\n",
      "         -6.8584e-03, -3.9627e-03]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight tensor([[-0.0062, -0.0019, -0.0002,  ...,  0.0022, -0.0014,  0.0007],\n",
      "        [ 0.0012,  0.0001, -0.0005,  ...,  0.0017, -0.0006,  0.0007],\n",
      "        [-0.0004, -0.0034,  0.0001,  ..., -0.0031,  0.0017, -0.0036],\n",
      "        ...,\n",
      "        [-0.0031,  0.0051,  0.0026,  ..., -0.0038, -0.0008, -0.0011],\n",
      "        [-0.0070,  0.0112, -0.0092,  ...,  0.0228, -0.0057, -0.0063],\n",
      "        [ 0.0008, -0.0039,  0.0014,  ..., -0.0027, -0.0035,  0.0063]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight tensor([[-0.0011,  0.0042, -0.0091,  ..., -0.0006, -0.0017, -0.0022],\n",
      "        [-0.0111, -0.0201, -0.0093,  ..., -0.0071,  0.0108, -0.0003],\n",
      "        [ 0.0016, -0.0035,  0.0009,  ..., -0.0062, -0.0083, -0.0095],\n",
      "        ...,\n",
      "        [ 0.0046, -0.0050,  0.0017,  ..., -0.0123,  0.0028, -0.0011],\n",
      "        [-0.0050, -0.0074,  0.0073,  ...,  0.0048,  0.0049, -0.0060],\n",
      "        [-0.0013, -0.0121,  0.0113,  ...,  0.0061,  0.0047, -0.0011]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight tensor([[ 5.3287e-05, -3.1534e-03, -2.1048e-03,  ..., -3.6715e-03,\n",
      "          3.5569e-03, -5.2440e-03],\n",
      "        [ 1.7549e-03, -2.0432e-03,  5.9842e-05,  ..., -1.0561e-03,\n",
      "          5.2698e-05,  1.7911e-03],\n",
      "        [ 5.3724e-03,  1.3166e-02, -2.9559e-03,  ...,  1.7643e-03,\n",
      "         -9.4125e-04,  6.6362e-03],\n",
      "        ...,\n",
      "        [ 2.4310e-03, -9.8259e-03, -4.1242e-04,  ..., -1.7891e-03,\n",
      "          4.4830e-03, -3.2169e-03],\n",
      "        [-9.4551e-03, -7.6049e-03, -4.3640e-03,  ..., -2.0440e-03,\n",
      "          1.7766e-02, -1.9785e-03],\n",
      "        [ 4.9119e-03, -8.4806e-03,  5.0629e-04,  ..., -1.6054e-03,\n",
      "          7.1607e-03, -5.4011e-03]])\n",
      "unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight tensor([[-0.0029,  0.0002, -0.0069,  ..., -0.0060, -0.0037,  0.0003],\n",
      "        [-0.0012,  0.0020,  0.0052,  ...,  0.0040,  0.0113, -0.0017],\n",
      "        [-0.0016, -0.0038,  0.0080,  ..., -0.0050, -0.0003,  0.0014],\n",
      "        ...,\n",
      "        [-0.0063,  0.0051, -0.0060,  ..., -0.0083,  0.0003, -0.0009],\n",
      "        [ 0.0090,  0.0136, -0.0036,  ..., -0.0066,  0.0027, -0.0119],\n",
      "        [ 0.0081, -0.0033,  0.0005,  ...,  0.0054,  0.0137, -0.0008]])\n",
      "unet.down_blocks.0.attentions.1.proj_in.lora.down.weight tensor([[[[-7.3909e-05]],\n",
      "\n",
      "         [[-5.9704e-04]],\n",
      "\n",
      "         [[ 4.6103e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.9980e-03]],\n",
      "\n",
      "         [[ 1.1335e-03]],\n",
      "\n",
      "         [[ 6.5732e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.5585e-04]],\n",
      "\n",
      "         [[-2.9474e-04]],\n",
      "\n",
      "         [[ 9.8077e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.5409e-05]],\n",
      "\n",
      "         [[-1.0054e-03]],\n",
      "\n",
      "         [[ 1.2323e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1909e-03]],\n",
      "\n",
      "         [[-4.3722e-04]],\n",
      "\n",
      "         [[-2.9515e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.8049e-04]],\n",
      "\n",
      "         [[ 1.6235e-03]],\n",
      "\n",
      "         [[ 2.0758e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.7066e-03]],\n",
      "\n",
      "         [[-1.3138e-03]],\n",
      "\n",
      "         [[ 3.0577e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.4107e-04]],\n",
      "\n",
      "         [[-2.8455e-03]],\n",
      "\n",
      "         [[ 1.9674e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.2357e-03]],\n",
      "\n",
      "         [[ 5.5756e-04]],\n",
      "\n",
      "         [[ 3.8133e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.8204e-03]],\n",
      "\n",
      "         [[-7.8071e-03]],\n",
      "\n",
      "         [[ 1.3240e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 6.9993e-04]],\n",
      "\n",
      "         [[-6.4253e-04]],\n",
      "\n",
      "         [[-1.6424e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.3938e-03]],\n",
      "\n",
      "         [[-4.8163e-04]],\n",
      "\n",
      "         [[ 5.7565e-05]]]])\n",
      "unet.down_blocks.0.attentions.1.proj_in.lora.up.weight tensor([[[[-6.1631e-03]],\n",
      "\n",
      "         [[-4.3038e-03]],\n",
      "\n",
      "         [[-2.0067e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7261e-03]],\n",
      "\n",
      "         [[ 3.2422e-03]],\n",
      "\n",
      "         [[ 4.6332e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 5.1652e-03]],\n",
      "\n",
      "         [[ 2.6195e-04]],\n",
      "\n",
      "         [[-3.6464e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.4257e-04]],\n",
      "\n",
      "         [[ 2.9475e-03]],\n",
      "\n",
      "         [[ 2.9216e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 7.8778e-03]],\n",
      "\n",
      "         [[ 2.2323e-03]],\n",
      "\n",
      "         [[-3.3371e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.9058e-03]],\n",
      "\n",
      "         [[ 4.3023e-03]],\n",
      "\n",
      "         [[-3.0750e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.7576e-05]],\n",
      "\n",
      "         [[ 2.7960e-04]],\n",
      "\n",
      "         [[-9.7108e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.7732e-03]],\n",
      "\n",
      "         [[-4.6864e-03]],\n",
      "\n",
      "         [[-1.0647e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.6749e-03]],\n",
      "\n",
      "         [[ 2.9037e-03]],\n",
      "\n",
      "         [[ 7.0872e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.1451e-03]],\n",
      "\n",
      "         [[ 4.9782e-04]],\n",
      "\n",
      "         [[-2.7548e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.0139e-02]],\n",
      "\n",
      "         [[-3.5889e-04]],\n",
      "\n",
      "         [[-9.2820e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3019e-03]],\n",
      "\n",
      "         [[ 5.0623e-03]],\n",
      "\n",
      "         [[ 4.9857e-03]]]])\n",
      "unet.down_blocks.0.attentions.1.proj_out.lora.down.weight tensor([[[[ 0.0019]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[-0.0045]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[ 0.0076]],\n",
      "\n",
      "         [[-0.0011]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0014]],\n",
      "\n",
      "         [[ 0.0005]],\n",
      "\n",
      "         [[-0.0008]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[-0.0007]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0070]],\n",
      "\n",
      "         [[-0.0074]],\n",
      "\n",
      "         [[ 0.0024]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0088]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[-0.0027]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0010]],\n",
      "\n",
      "         [[ 0.0030]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[-0.0045]],\n",
      "\n",
      "         [[ 0.0034]]],\n",
      "\n",
      "\n",
      "        [[[-0.0005]],\n",
      "\n",
      "         [[ 0.0120]],\n",
      "\n",
      "         [[-0.0067]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0149]],\n",
      "\n",
      "         [[-0.0112]],\n",
      "\n",
      "         [[ 0.0028]]],\n",
      "\n",
      "\n",
      "        [[[-0.0004]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0066]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         [[-0.0041]]]])\n",
      "unet.down_blocks.0.attentions.1.proj_out.lora.up.weight tensor([[[[ 0.0017]],\n",
      "\n",
      "         [[-0.0066]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         [[-0.0015]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0061]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[-0.0053]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         [[-0.0038]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0040]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         [[-0.0058]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[ 0.0083]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0032]],\n",
      "\n",
      "         [[ 0.0037]],\n",
      "\n",
      "         [[ 0.0047]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         [[ 0.0096]],\n",
      "\n",
      "         [[ 0.0029]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0018]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[ 0.0083]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[ 0.0016]],\n",
      "\n",
      "         [[-0.0038]]],\n",
      "\n",
      "\n",
      "        [[[-0.0036]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[ 0.0047]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0092]],\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         [[ 0.0027]]]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight tensor([[ 1.1632e-03, -6.7078e-03,  5.0626e-03,  ...,  1.4144e-03,\n",
      "         -3.0184e-03, -5.8932e-03],\n",
      "        [ 8.2334e-04, -1.9401e-03, -4.4354e-04,  ..., -2.4706e-04,\n",
      "         -8.4354e-04, -9.8312e-04],\n",
      "        [ 9.0486e-03, -1.2528e-03, -8.5435e-04,  ...,  1.9609e-03,\n",
      "          3.4189e-04,  1.6335e-04],\n",
      "        ...,\n",
      "        [ 3.8091e-03, -3.9436e-04, -8.6192e-04,  ...,  3.2290e-03,\n",
      "         -6.6677e-03,  2.8415e-03],\n",
      "        [-3.4703e-03, -7.7682e-03,  5.8044e-04,  ...,  7.8457e-03,\n",
      "         -3.9443e-03, -1.4772e-02],\n",
      "        [-2.0367e-03,  9.6055e-04, -3.9532e-04,  ..., -1.0815e-03,\n",
      "          4.0131e-03,  8.5250e-05]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight tensor([[ 0.0025, -0.0095, -0.0048,  ...,  0.0073, -0.0067, -0.0017],\n",
      "        [ 0.0068,  0.0061,  0.0023,  ..., -0.0018,  0.0066,  0.0065],\n",
      "        [-0.0032,  0.0031,  0.0003,  ..., -0.0035,  0.0079, -0.0071],\n",
      "        ...,\n",
      "        [-0.0003, -0.0047, -0.0030,  ..., -0.0018,  0.0007,  0.0015],\n",
      "        [ 0.0041, -0.0041, -0.0039,  ..., -0.0012,  0.0029, -0.0027],\n",
      "        [ 0.0017,  0.0015,  0.0063,  ..., -0.0050, -0.0068, -0.0039]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight tensor([[-2.1777e-03,  1.1996e-03,  1.3170e-03,  ..., -7.4754e-04,\n",
      "         -8.2405e-04,  8.1958e-04],\n",
      "        [ 7.9841e-04,  2.9715e-04,  2.7987e-04,  ..., -7.6361e-04,\n",
      "          3.0381e-04, -6.0005e-05],\n",
      "        [-4.0820e-03,  1.4487e-03,  2.9036e-04,  ..., -2.9610e-03,\n",
      "         -3.4450e-04, -1.8359e-03],\n",
      "        ...,\n",
      "        [-9.6983e-04, -2.7834e-03,  2.1347e-03,  ..., -1.1995e-04,\n",
      "         -1.2118e-03,  3.0150e-03],\n",
      "        [-9.5233e-04,  8.0842e-03,  3.0495e-03,  ..., -2.0453e-03,\n",
      "          1.2941e-03, -2.1316e-05],\n",
      "        [-4.0327e-03,  2.4613e-03, -2.2503e-03,  ...,  2.9544e-03,\n",
      "         -4.2147e-04,  6.3661e-04]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight tensor([[ 0.0034,  0.0061, -0.0032,  ...,  0.0123,  0.0002, -0.0059],\n",
      "        [ 0.0093, -0.0072, -0.0004,  ..., -0.0041,  0.0004,  0.0019],\n",
      "        [-0.0039,  0.0021,  0.0030,  ..., -0.0011,  0.0055,  0.0035],\n",
      "        ...,\n",
      "        [ 0.0007, -0.0017,  0.0063,  ...,  0.0012,  0.0001,  0.0064],\n",
      "        [ 0.0018,  0.0026,  0.0054,  ...,  0.0019,  0.0017, -0.0051],\n",
      "        [-0.0003, -0.0006, -0.0011,  ...,  0.0024,  0.0033, -0.0069]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight tensor([[ 0.0011, -0.0009, -0.0029,  ...,  0.0010, -0.0011, -0.0010],\n",
      "        [-0.0015, -0.0004,  0.0008,  ...,  0.0010, -0.0006,  0.0001],\n",
      "        [ 0.0028,  0.0023,  0.0054,  ...,  0.0036, -0.0028,  0.0058],\n",
      "        ...,\n",
      "        [-0.0017, -0.0024, -0.0006,  ..., -0.0029, -0.0011, -0.0010],\n",
      "        [ 0.0100, -0.0060,  0.0096,  ..., -0.0006, -0.0084, -0.0068],\n",
      "        [ 0.0047,  0.0061, -0.0006,  ..., -0.0006,  0.0034,  0.0020]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight tensor([[ 1.3673e-02,  8.3444e-05,  6.1254e-03,  ..., -5.3502e-03,\n",
      "          5.2428e-03,  5.6199e-03],\n",
      "        [-1.8646e-03,  9.0148e-03,  5.9363e-04,  ..., -7.7789e-04,\n",
      "         -2.7132e-03,  1.8054e-03],\n",
      "        [-3.5516e-03,  1.7669e-03, -7.6207e-03,  ..., -2.2957e-03,\n",
      "          4.8451e-03,  1.6658e-03],\n",
      "        ...,\n",
      "        [-1.1446e-02, -9.0545e-04, -2.9412e-03,  ...,  3.4637e-03,\n",
      "         -4.4871e-04, -2.8654e-03],\n",
      "        [-8.4570e-04, -1.2236e-03, -6.0150e-04,  ..., -3.9168e-05,\n",
      "          1.8983e-03, -4.4649e-04],\n",
      "        [ 5.1194e-03, -2.3173e-03, -2.5792e-03,  ..., -3.2334e-03,\n",
      "          1.7231e-03,  1.9041e-03]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight tensor([[ 1.4787e-03, -4.0851e-03,  2.6704e-03,  ...,  1.3717e-04,\n",
      "          1.5538e-03,  3.7526e-03],\n",
      "        [-2.2684e-04, -7.0727e-04,  4.7579e-04,  ..., -2.0785e-04,\n",
      "         -7.7643e-04,  1.0465e-04],\n",
      "        [ 2.9108e-03,  3.3897e-04, -1.0611e-03,  ...,  2.7478e-03,\n",
      "         -7.0801e-03, -6.8427e-04],\n",
      "        ...,\n",
      "        [ 7.8179e-04, -2.0109e-03,  2.0339e-03,  ...,  1.3737e-03,\n",
      "          3.2804e-04, -4.9499e-05],\n",
      "        [ 2.7116e-03, -4.3982e-04, -1.3495e-02,  ...,  1.3389e-02,\n",
      "          2.0530e-03, -1.3092e-02],\n",
      "        [ 9.1955e-04,  7.1813e-04,  5.7850e-03,  ...,  1.2497e-03,\n",
      "         -6.8550e-03, -1.4104e-03]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight tensor([[ 0.0068,  0.0036, -0.0003,  ...,  0.0006,  0.0075, -0.0028],\n",
      "        [-0.0025, -0.0012, -0.0014,  ..., -0.0042,  0.0009,  0.0071],\n",
      "        [ 0.0010, -0.0052, -0.0064,  ..., -0.0077,  0.0031, -0.0036],\n",
      "        ...,\n",
      "        [ 0.0033, -0.0011,  0.0053,  ...,  0.0032, -0.0051,  0.0013],\n",
      "        [ 0.0031, -0.0089, -0.0029,  ..., -0.0026,  0.0014,  0.0002],\n",
      "        [-0.0005, -0.0040,  0.0012,  ..., -0.0012,  0.0037,  0.0007]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight tensor([[-0.0027,  0.0011, -0.0012,  ..., -0.0022, -0.0015,  0.0019],\n",
      "        [ 0.0006,  0.0004, -0.0006,  ...,  0.0003,  0.0002, -0.0004],\n",
      "        [-0.0024,  0.0002,  0.0018,  ..., -0.0005, -0.0032,  0.0001],\n",
      "        ...,\n",
      "        [-0.0002, -0.0020,  0.0034,  ..., -0.0039, -0.0002, -0.0015],\n",
      "        [-0.0065, -0.0087, -0.0038,  ..., -0.0015, -0.0054,  0.0023],\n",
      "        [-0.0025,  0.0002, -0.0013,  ..., -0.0004,  0.0019, -0.0004]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight tensor([[ 0.0008,  0.0030,  0.0023,  ..., -0.0066, -0.0004, -0.0013],\n",
      "        [-0.0011,  0.0014,  0.0013,  ...,  0.0005,  0.0013, -0.0041],\n",
      "        [ 0.0039, -0.0024, -0.0034,  ..., -0.0070,  0.0011,  0.0056],\n",
      "        ...,\n",
      "        [ 0.0036,  0.0002,  0.0006,  ...,  0.0043, -0.0006, -0.0011],\n",
      "        [ 0.0009,  0.0002,  0.0027,  ...,  0.0010,  0.0022,  0.0018],\n",
      "        [-0.0016, -0.0026, -0.0022,  ..., -0.0014, -0.0024,  0.0020]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight tensor([[ 2.8058e-03,  6.7713e-03,  5.7337e-03,  ..., -8.7381e-04,\n",
      "          6.6361e-03,  3.0057e-04],\n",
      "        [-5.4231e-04, -1.3710e-03, -7.7766e-04,  ...,  2.8217e-04,\n",
      "         -6.9061e-05,  1.8155e-04],\n",
      "        [ 1.7372e-03, -9.7612e-03,  5.5532e-05,  ..., -8.6058e-03,\n",
      "          5.9444e-03, -2.3880e-03],\n",
      "        ...,\n",
      "        [-3.8693e-03, -8.0685e-04, -5.3953e-03,  ...,  2.7867e-03,\n",
      "          2.2684e-03, -3.8554e-04],\n",
      "        [ 2.0394e-03,  5.7568e-03,  4.0561e-03,  ...,  2.2544e-04,\n",
      "         -5.4701e-03,  1.5388e-02],\n",
      "        [-6.6421e-03, -5.2755e-04,  5.5881e-03,  ..., -1.1360e-04,\n",
      "          3.8586e-03,  2.6650e-03]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight tensor([[ 0.0017, -0.0163,  0.0118,  ..., -0.0096, -0.0004, -0.0057],\n",
      "        [ 0.0053, -0.0043,  0.0003,  ...,  0.0099, -0.0059, -0.0037],\n",
      "        [-0.0035,  0.0028,  0.0023,  ..., -0.0006,  0.0020, -0.0004],\n",
      "        ...,\n",
      "        [-0.0090, -0.0017, -0.0052,  ...,  0.0018,  0.0030,  0.0018],\n",
      "        [-0.0064,  0.0053, -0.0039,  ..., -0.0002,  0.0054, -0.0031],\n",
      "        [ 0.0002, -0.0048, -0.0007,  ...,  0.0060, -0.0067, -0.0003]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight tensor([[-1.4311e-03, -3.6731e-03,  1.2101e-03,  ...,  1.3959e-03,\n",
      "         -2.0364e-03, -2.8607e-03],\n",
      "        [ 5.6524e-04,  6.5133e-04,  3.3803e-04,  ...,  8.0798e-04,\n",
      "         -5.0468e-04,  3.0202e-04],\n",
      "        [ 6.8134e-04, -1.1837e-03,  4.0194e-05,  ...,  2.0292e-03,\n",
      "          1.3088e-03, -2.1298e-04],\n",
      "        ...,\n",
      "        [-4.4721e-04, -1.0208e-03,  1.0226e-03,  ...,  2.4475e-03,\n",
      "         -3.8890e-03,  8.3229e-04],\n",
      "        [ 2.7217e-03,  2.5769e-03, -1.1417e-02,  ..., -7.2232e-03,\n",
      "          3.5067e-03,  5.0512e-03],\n",
      "        [ 1.4471e-03, -2.9659e-04, -1.5778e-03,  ...,  2.5802e-03,\n",
      "          1.7603e-03,  6.8278e-03]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight tensor([[ 0.0012,  0.0024,  0.0030,  ..., -0.0010, -0.0050,  0.0009],\n",
      "        [ 0.0023,  0.0042, -0.0011,  ..., -0.0039, -0.0039, -0.0011],\n",
      "        [ 0.0008, -0.0064, -0.0011,  ...,  0.0012, -0.0005, -0.0011],\n",
      "        ...,\n",
      "        [ 0.0050,  0.0013,  0.0010,  ..., -0.0020, -0.0022,  0.0010],\n",
      "        [ 0.0015,  0.0046,  0.0045,  ...,  0.0009, -0.0043,  0.0036],\n",
      "        [-0.0008, -0.0036,  0.0008,  ...,  0.0003, -0.0028,  0.0008]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight tensor([[ 2.0703e-03, -4.8536e-04, -4.1147e-03,  ..., -2.2688e-03,\n",
      "         -8.9238e-04, -1.4014e-03],\n",
      "        [ 8.1565e-04,  2.8216e-04, -2.3832e-04,  ...,  2.6341e-04,\n",
      "         -8.5961e-04,  7.5944e-04],\n",
      "        [ 7.8763e-04,  5.3412e-03,  1.3521e-03,  ...,  1.0187e-03,\n",
      "         -2.3900e-03,  2.6202e-05],\n",
      "        ...,\n",
      "        [-1.3904e-03, -3.2444e-03,  2.5390e-03,  ..., -3.7399e-03,\n",
      "          2.3460e-03, -7.1711e-04],\n",
      "        [-9.1571e-04,  2.7082e-03, -4.3310e-03,  ...,  1.8042e-03,\n",
      "         -5.5321e-03, -1.2428e-03],\n",
      "        [-8.2744e-04,  3.2921e-03, -2.1165e-03,  ..., -1.8304e-03,\n",
      "          9.6013e-04,  1.6554e-03]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight tensor([[ 0.0007,  0.0083,  0.0016,  ...,  0.0081,  0.0003, -0.0035],\n",
      "        [ 0.0009, -0.0087,  0.0039,  ..., -0.0175,  0.0008,  0.0022],\n",
      "        [ 0.0043, -0.0081,  0.0021,  ...,  0.0062,  0.0001, -0.0004],\n",
      "        ...,\n",
      "        [ 0.0031, -0.0027,  0.0110,  ...,  0.0024,  0.0008, -0.0031],\n",
      "        [ 0.0013, -0.0072,  0.0044,  ...,  0.0077, -0.0027, -0.0045],\n",
      "        [-0.0026,  0.0010,  0.0004,  ...,  0.0028,  0.0047, -0.0059]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight tensor([[-3.2747e-03, -1.0304e-03, -6.5730e-03,  ..., -8.9785e-03,\n",
      "          3.9030e-03,  1.2715e-04],\n",
      "        [-1.9620e-03,  7.0734e-04,  6.3747e-04,  ...,  8.0449e-06,\n",
      "         -7.2681e-05, -1.9042e-03],\n",
      "        [ 1.0146e-03,  2.3882e-03, -3.0050e-03,  ..., -2.1347e-03,\n",
      "          7.4829e-04,  5.1558e-03],\n",
      "        ...,\n",
      "        [ 3.7200e-03,  1.3438e-03,  2.8204e-03,  ..., -5.6552e-03,\n",
      "          2.9424e-03, -2.5305e-03],\n",
      "        [-6.2560e-04, -6.4255e-03,  1.3595e-02,  ...,  8.7011e-03,\n",
      "         -6.8711e-03, -1.2303e-02],\n",
      "        [ 1.8273e-03, -8.1331e-04, -5.6998e-04,  ...,  2.5226e-03,\n",
      "         -3.8409e-03, -4.2808e-04]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight tensor([[ 0.0069,  0.0012,  0.0017,  ..., -0.0002, -0.0003, -0.0012],\n",
      "        [ 0.0105, -0.0048, -0.0009,  ...,  0.0012, -0.0028, -0.0018],\n",
      "        [-0.0023, -0.0056,  0.0082,  ...,  0.0022, -0.0010,  0.0060],\n",
      "        ...,\n",
      "        [ 0.0087,  0.0030,  0.0006,  ...,  0.0078, -0.0020,  0.0023],\n",
      "        [-0.0037,  0.0028, -0.0015,  ...,  0.0029, -0.0026,  0.0055],\n",
      "        [-0.0049,  0.0107,  0.0052,  ...,  0.0091, -0.0059,  0.0012]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight tensor([[-1.5542e-04, -3.0874e-03,  5.1985e-04,  ...,  8.3591e-04,\n",
      "         -3.1075e-04, -6.3897e-04],\n",
      "        [-2.9290e-04,  6.1168e-04, -1.1108e-03,  ...,  4.8650e-04,\n",
      "          3.9259e-04, -1.9685e-03],\n",
      "        [-6.8628e-04,  1.0086e-03, -4.1044e-03,  ...,  1.7457e-03,\n",
      "          2.7524e-03, -6.4571e-03],\n",
      "        ...,\n",
      "        [-2.8017e-03,  1.5400e-03,  1.4577e-03,  ..., -6.5721e-04,\n",
      "          9.5477e-04,  3.9148e-03],\n",
      "        [ 6.0466e-03, -2.9279e-03, -1.7031e-03,  ..., -1.0348e-02,\n",
      "          9.5684e-03, -8.6171e-03],\n",
      "        [-9.2033e-05,  7.0843e-03, -2.5882e-03,  ..., -2.7142e-03,\n",
      "         -1.3425e-03,  1.4254e-03]])\n",
      "unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight tensor([[-1.2040e-02, -5.1758e-03,  9.0024e-03,  ...,  8.5680e-04,\n",
      "          6.5408e-03, -9.3874e-04],\n",
      "        [ 1.1619e-02, -1.8354e-03, -7.0634e-03,  ..., -9.3538e-03,\n",
      "          1.1821e-02,  1.7443e-03],\n",
      "        [-4.5047e-03,  5.8278e-03,  6.9402e-03,  ...,  5.4031e-03,\n",
      "          7.3122e-03,  5.6026e-03],\n",
      "        ...,\n",
      "        [-4.9391e-03,  5.7758e-03,  7.8517e-03,  ...,  1.8527e-03,\n",
      "         -1.0487e-04, -5.9512e-03],\n",
      "        [ 1.0292e-03, -4.3078e-03,  2.4700e-03,  ..., -2.2654e-03,\n",
      "         -5.5193e-03,  1.4962e-03],\n",
      "        [-3.5807e-03,  9.4748e-03,  4.0883e-03,  ..., -1.6071e-03,\n",
      "         -5.9458e-03,  3.0600e-05]])\n",
      "unet.down_blocks.1.attentions.0.proj_in.lora.down.weight tensor([[[[-0.0008]],\n",
      "\n",
      "         [[-0.0049]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         [[ 0.0062]]],\n",
      "\n",
      "\n",
      "        [[[-0.0003]],\n",
      "\n",
      "         [[ 0.0010]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[-0.0014]],\n",
      "\n",
      "         [[ 0.0012]]],\n",
      "\n",
      "\n",
      "        [[[-0.0016]],\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         [[ 0.0007]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         [[ 0.0042]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0006]],\n",
      "\n",
      "         [[-0.0003]],\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[-0.0011]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0061]],\n",
      "\n",
      "         [[ 0.0127]],\n",
      "\n",
      "         [[-0.0234]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0129]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         [[-0.0014]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0019]],\n",
      "\n",
      "         [[-0.0087]],\n",
      "\n",
      "         [[-0.0005]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0035]],\n",
      "\n",
      "         [[ 0.0010]],\n",
      "\n",
      "         [[ 0.0002]]]])\n",
      "unet.down_blocks.1.attentions.0.proj_in.lora.up.weight tensor([[[[ 8.4569e-03]],\n",
      "\n",
      "         [[ 1.0186e-02]],\n",
      "\n",
      "         [[ 1.3658e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.4320e-03]],\n",
      "\n",
      "         [[-5.7109e-05]],\n",
      "\n",
      "         [[ 6.6165e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.4754e-04]],\n",
      "\n",
      "         [[-1.7158e-02]],\n",
      "\n",
      "         [[-1.0872e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.9898e-04]],\n",
      "\n",
      "         [[-5.2980e-03]],\n",
      "\n",
      "         [[-3.4143e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.0321e-03]],\n",
      "\n",
      "         [[ 3.2596e-03]],\n",
      "\n",
      "         [[-5.8144e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.7128e-03]],\n",
      "\n",
      "         [[ 2.0031e-03]],\n",
      "\n",
      "         [[-3.8670e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 6.9170e-03]],\n",
      "\n",
      "         [[-1.0749e-02]],\n",
      "\n",
      "         [[ 1.6417e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.5010e-03]],\n",
      "\n",
      "         [[-2.2052e-03]],\n",
      "\n",
      "         [[-7.2340e-03]]],\n",
      "\n",
      "\n",
      "        [[[-9.1710e-03]],\n",
      "\n",
      "         [[ 1.3595e-02]],\n",
      "\n",
      "         [[ 3.2209e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.8644e-05]],\n",
      "\n",
      "         [[ 8.4089e-04]],\n",
      "\n",
      "         [[-3.3927e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.3521e-02]],\n",
      "\n",
      "         [[ 9.7877e-04]],\n",
      "\n",
      "         [[ 9.9528e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.3706e-03]],\n",
      "\n",
      "         [[-1.6146e-03]],\n",
      "\n",
      "         [[-3.4164e-03]]]])\n",
      "unet.down_blocks.1.attentions.0.proj_out.lora.down.weight tensor([[[[ 1.8636e-03]],\n",
      "\n",
      "         [[ 2.6910e-04]],\n",
      "\n",
      "         [[-8.7780e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.7966e-03]],\n",
      "\n",
      "         [[-4.7922e-03]],\n",
      "\n",
      "         [[ 2.6840e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 7.7660e-04]],\n",
      "\n",
      "         [[-6.2866e-04]],\n",
      "\n",
      "         [[-2.2170e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.8857e-04]],\n",
      "\n",
      "         [[-2.5963e-04]],\n",
      "\n",
      "         [[-3.7452e-05]]],\n",
      "\n",
      "\n",
      "        [[[-1.1536e-03]],\n",
      "\n",
      "         [[ 6.5185e-04]],\n",
      "\n",
      "         [[ 3.0561e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1181e-02]],\n",
      "\n",
      "         [[ 2.4803e-03]],\n",
      "\n",
      "         [[ 4.4358e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.2567e-03]],\n",
      "\n",
      "         [[ 5.6985e-03]],\n",
      "\n",
      "         [[-3.7986e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.5423e-03]],\n",
      "\n",
      "         [[-5.8871e-04]],\n",
      "\n",
      "         [[ 4.0482e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.7821e-03]],\n",
      "\n",
      "         [[-4.6280e-03]],\n",
      "\n",
      "         [[ 9.9760e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.6023e-03]],\n",
      "\n",
      "         [[-1.2294e-02]],\n",
      "\n",
      "         [[-2.1374e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6623e-03]],\n",
      "\n",
      "         [[-5.8887e-04]],\n",
      "\n",
      "         [[-3.3647e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8439e-03]],\n",
      "\n",
      "         [[ 4.5214e-03]],\n",
      "\n",
      "         [[-1.6898e-03]]]])\n",
      "unet.down_blocks.1.attentions.0.proj_out.lora.up.weight tensor([[[[ 0.0011]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[ 0.0096]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0039]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         [[ 0.0044]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0059]],\n",
      "\n",
      "         [[ 0.0030]],\n",
      "\n",
      "         [[-0.0040]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0039]],\n",
      "\n",
      "         [[-0.0141]],\n",
      "\n",
      "         [[ 0.0065]]],\n",
      "\n",
      "\n",
      "        [[[-0.0094]],\n",
      "\n",
      "         [[ 0.0021]],\n",
      "\n",
      "         [[ 0.0022]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0095]],\n",
      "\n",
      "         [[-0.0052]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0061]],\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         [[ 0.0059]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0048]],\n",
      "\n",
      "         [[-0.0089]],\n",
      "\n",
      "         [[ 0.0060]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0008]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         [[ 0.0055]]],\n",
      "\n",
      "\n",
      "        [[[-0.0002]],\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         [[ 0.0037]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0062]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[-0.0060]]]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight tensor([[-0.0006, -0.0018,  0.0061,  ...,  0.0031,  0.0061, -0.0010],\n",
      "        [ 0.0003,  0.0004, -0.0003,  ..., -0.0017,  0.0007,  0.0015],\n",
      "        [-0.0031,  0.0005,  0.0026,  ...,  0.0023, -0.0072, -0.0051],\n",
      "        ...,\n",
      "        [ 0.0019, -0.0036, -0.0028,  ...,  0.0013,  0.0018,  0.0025],\n",
      "        [ 0.0006, -0.0127,  0.0136,  ...,  0.0075,  0.0187,  0.0055],\n",
      "        [ 0.0006, -0.0009,  0.0037,  ..., -0.0017, -0.0001, -0.0003]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight tensor([[-2.0159e-03, -7.2402e-03,  4.9374e-03,  ..., -4.0789e-03,\n",
      "          2.3183e-03,  8.8404e-03],\n",
      "        [ 2.8012e-03, -1.1668e-02,  6.9704e-04,  ...,  6.7810e-06,\n",
      "         -1.4308e-03,  5.3503e-04],\n",
      "        [ 3.8184e-03, -5.8460e-03,  3.5123e-03,  ..., -3.1739e-03,\n",
      "          9.5962e-03, -3.7068e-03],\n",
      "        ...,\n",
      "        [ 1.8202e-03,  1.0904e-03, -2.5850e-03,  ..., -7.6704e-03,\n",
      "         -9.3698e-03,  3.0076e-03],\n",
      "        [-3.5548e-03, -8.9006e-03, -3.2220e-03,  ...,  1.3774e-02,\n",
      "          4.4834e-03, -5.3068e-04],\n",
      "        [ 3.5594e-04,  4.7847e-03, -3.8957e-03,  ..., -3.1899e-03,\n",
      "          7.1835e-05,  1.1107e-03]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight tensor([[ 5.2040e-04, -5.8096e-05, -2.2824e-03,  ...,  1.2998e-03,\n",
      "         -2.1639e-03, -1.9645e-04],\n",
      "        [ 1.2914e-03,  2.6577e-04,  7.3946e-04,  ..., -4.4220e-04,\n",
      "          8.8116e-04,  1.0490e-03],\n",
      "        [-2.3228e-03, -1.8758e-03, -7.6857e-03,  ...,  7.2539e-04,\n",
      "          6.4931e-03,  4.6353e-04],\n",
      "        ...,\n",
      "        [ 1.2492e-03, -3.3302e-03, -2.5215e-03,  ..., -4.1531e-03,\n",
      "         -2.4781e-03, -3.8408e-03],\n",
      "        [ 8.2322e-03,  1.1543e-02,  5.0829e-03,  ...,  1.4617e-03,\n",
      "          5.9060e-04, -6.5543e-03],\n",
      "        [-3.2232e-03,  2.1847e-03,  1.1104e-03,  ...,  1.5128e-03,\n",
      "         -1.5504e-03, -1.4816e-03]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight tensor([[ 3.2828e-03, -3.1922e-03, -1.2702e-02,  ...,  7.6906e-03,\n",
      "          4.8098e-03, -8.3844e-03],\n",
      "        [-1.0954e-03, -1.0261e-02,  5.9013e-03,  ..., -1.2640e-02,\n",
      "         -8.4783e-04,  1.2323e-02],\n",
      "        [ 1.2961e-03,  1.2489e-02,  8.7113e-03,  ...,  8.8744e-03,\n",
      "          4.6156e-03, -5.9173e-03],\n",
      "        ...,\n",
      "        [ 5.3031e-04,  2.3040e-03, -8.6313e-03,  ...,  1.1616e-02,\n",
      "         -5.2119e-03, -1.5650e-02],\n",
      "        [ 2.5580e-03,  6.6959e-03, -8.4386e-05,  ..., -8.0236e-03,\n",
      "         -4.7706e-03,  4.1019e-03],\n",
      "        [ 6.3765e-03,  5.3596e-03,  2.5761e-03,  ...,  8.6824e-04,\n",
      "          1.7299e-03,  5.6491e-04]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight tensor([[ 0.0030,  0.0054, -0.0082,  ..., -0.0060, -0.0012,  0.0035],\n",
      "        [ 0.0018,  0.0022, -0.0005,  ...,  0.0019,  0.0021,  0.0012],\n",
      "        [ 0.0045, -0.0002,  0.0006,  ..., -0.0088,  0.0016, -0.0025],\n",
      "        ...,\n",
      "        [ 0.0005,  0.0013, -0.0038,  ..., -0.0041,  0.0012, -0.0011],\n",
      "        [ 0.0097, -0.0014, -0.0155,  ...,  0.0022, -0.0034, -0.0048],\n",
      "        [-0.0035,  0.0020,  0.0028,  ..., -0.0046,  0.0031,  0.0010]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight tensor([[ 3.4833e-03,  1.2356e-02,  2.3502e-03,  ..., -2.8917e-03,\n",
      "         -9.9104e-03, -1.8911e-03],\n",
      "        [-2.4668e-03,  7.3605e-03,  7.1214e-04,  ...,  7.7329e-04,\n",
      "         -4.3336e-03,  1.1803e-02],\n",
      "        [-6.7555e-03, -5.5505e-04,  9.1494e-04,  ..., -2.5288e-03,\n",
      "         -1.1307e-02,  4.4193e-05],\n",
      "        ...,\n",
      "        [-1.2794e-03, -9.9864e-04,  6.2031e-04,  ...,  1.0529e-04,\n",
      "         -4.1212e-03,  4.7195e-03],\n",
      "        [-4.0556e-03, -1.3092e-03, -4.2004e-03,  ...,  1.0046e-02,\n",
      "         -7.8408e-04, -2.9416e-03],\n",
      "        [ 7.2650e-03,  2.2121e-03,  5.4016e-03,  ...,  1.2386e-02,\n",
      "         -9.7292e-03,  1.0018e-02]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight tensor([[-3.2712e-03, -5.0398e-03, -4.9179e-03,  ...,  2.1417e-04,\n",
      "         -4.2180e-03,  1.2786e-04],\n",
      "        [-3.3698e-05, -1.2556e-04,  1.5808e-03,  ..., -1.8158e-03,\n",
      "         -7.7543e-04,  1.2422e-03],\n",
      "        [ 4.5955e-03, -3.9366e-03,  2.1474e-03,  ..., -1.6764e-03,\n",
      "          1.1034e-03,  3.4970e-03],\n",
      "        ...,\n",
      "        [-1.0710e-03, -1.3395e-03, -5.3174e-03,  ...,  4.5589e-03,\n",
      "          8.9755e-04, -9.9635e-04],\n",
      "        [ 2.7573e-03, -4.7088e-04,  6.2036e-03,  ...,  7.4058e-03,\n",
      "          5.5527e-03,  2.3764e-03],\n",
      "        [-2.8190e-03, -3.9098e-03,  3.6378e-03,  ...,  2.6352e-03,\n",
      "         -9.6922e-04, -3.9723e-03]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight tensor([[-0.0032, -0.0044,  0.0037,  ...,  0.0040, -0.0049, -0.0092],\n",
      "        [-0.0047,  0.0098, -0.0059,  ...,  0.0039,  0.0003, -0.0038],\n",
      "        [ 0.0034,  0.0016, -0.0032,  ..., -0.0010, -0.0028,  0.0071],\n",
      "        ...,\n",
      "        [ 0.0045,  0.0014,  0.0013,  ..., -0.0118, -0.0006,  0.0035],\n",
      "        [ 0.0019, -0.0071,  0.0056,  ..., -0.0049,  0.0041, -0.0032],\n",
      "        [-0.0035,  0.0051, -0.0065,  ...,  0.0025, -0.0009, -0.0025]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight tensor([[-1.7427e-03, -1.0980e-03, -1.5430e-03,  ..., -7.7184e-04,\n",
      "         -2.6596e-03,  9.7431e-04],\n",
      "        [-1.6633e-03,  2.3157e-04, -4.3685e-04,  ..., -3.0782e-04,\n",
      "         -5.0726e-04,  8.7953e-05],\n",
      "        [-5.6616e-03,  5.8469e-04, -8.1133e-04,  ...,  6.4310e-04,\n",
      "          4.0636e-03,  4.1031e-03],\n",
      "        ...,\n",
      "        [ 3.4317e-03, -4.1333e-03,  2.1897e-04,  ...,  5.6719e-03,\n",
      "          6.1418e-03,  1.3953e-03],\n",
      "        [-5.8026e-04, -1.3868e-03,  5.4108e-04,  ..., -1.1490e-03,\n",
      "         -9.9769e-03,  1.0282e-02],\n",
      "        [-1.1991e-03,  1.6864e-03,  3.3966e-03,  ...,  2.4270e-03,\n",
      "          2.5104e-04,  1.4495e-03]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight tensor([[-0.0003, -0.0007,  0.0041,  ...,  0.0003,  0.0026,  0.0033],\n",
      "        [-0.0021,  0.0036, -0.0007,  ...,  0.0034, -0.0071,  0.0009],\n",
      "        [-0.0007,  0.0041,  0.0009,  ...,  0.0002,  0.0016, -0.0062],\n",
      "        ...,\n",
      "        [-0.0007,  0.0017,  0.0012,  ...,  0.0050,  0.0058,  0.0006],\n",
      "        [-0.0048,  0.0030, -0.0072,  ...,  0.0033, -0.0016,  0.0030],\n",
      "        [ 0.0046, -0.0015,  0.0007,  ..., -0.0075,  0.0044, -0.0037]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight tensor([[-0.0050, -0.0003, -0.0063,  ..., -0.0018, -0.0010, -0.0006],\n",
      "        [-0.0001, -0.0005,  0.0001,  ..., -0.0007,  0.0004,  0.0017],\n",
      "        [-0.0017, -0.0038, -0.0064,  ...,  0.0016, -0.0054, -0.0062],\n",
      "        ...,\n",
      "        [-0.0032,  0.0006, -0.0016,  ..., -0.0012, -0.0007, -0.0012],\n",
      "        [-0.0063, -0.0120,  0.0037,  ...,  0.0034,  0.0085, -0.0005],\n",
      "        [ 0.0029, -0.0034, -0.0012,  ..., -0.0004,  0.0024, -0.0036]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight tensor([[-1.0481e-02, -4.8184e-03,  5.9048e-03,  ...,  1.1235e-02,\n",
      "         -3.2339e-03,  8.7874e-04],\n",
      "        [ 3.1537e-03, -9.5585e-03, -1.5552e-02,  ...,  4.9316e-03,\n",
      "         -1.3392e-02, -7.4391e-03],\n",
      "        [ 4.6410e-03, -1.3533e-02,  2.4817e-03,  ...,  7.3495e-03,\n",
      "         -4.4229e-03,  1.0257e-03],\n",
      "        ...,\n",
      "        [ 6.9888e-05, -6.4306e-03,  4.3036e-03,  ..., -1.1537e-02,\n",
      "         -1.4069e-03,  1.6537e-03],\n",
      "        [-1.8952e-03,  9.9521e-03, -1.0921e-02,  ...,  7.4379e-03,\n",
      "          1.4365e-03,  1.6162e-03],\n",
      "        [-8.3775e-03, -5.0192e-03, -1.2660e-03,  ...,  2.7200e-03,\n",
      "          4.0661e-03,  4.5798e-03]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight tensor([[ 0.0006, -0.0035,  0.0015,  ..., -0.0002, -0.0074,  0.0016],\n",
      "        [ 0.0007,  0.0003, -0.0021,  ...,  0.0017,  0.0006,  0.0005],\n",
      "        [ 0.0040, -0.0005, -0.0056,  ..., -0.0003, -0.0014,  0.0008],\n",
      "        ...,\n",
      "        [-0.0007, -0.0020,  0.0022,  ...,  0.0021, -0.0046, -0.0034],\n",
      "        [ 0.0036, -0.0101,  0.0089,  ..., -0.0079,  0.0091,  0.0115],\n",
      "        [ 0.0012, -0.0007,  0.0019,  ..., -0.0010, -0.0016, -0.0026]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight tensor([[ 0.0012,  0.0005, -0.0005,  ..., -0.0016,  0.0031, -0.0003],\n",
      "        [ 0.0032,  0.0036, -0.0057,  ..., -0.0046, -0.0073,  0.0005],\n",
      "        [ 0.0007,  0.0034,  0.0017,  ...,  0.0029,  0.0033, -0.0008],\n",
      "        ...,\n",
      "        [ 0.0054,  0.0039, -0.0109,  ...,  0.0003, -0.0044, -0.0020],\n",
      "        [ 0.0077,  0.0042,  0.0044,  ..., -0.0020, -0.0031, -0.0022],\n",
      "        [ 0.0036, -0.0032, -0.0039,  ..., -0.0003,  0.0002,  0.0078]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight tensor([[-9.5928e-04, -5.8688e-03, -4.5456e-04,  ..., -1.0952e-03,\n",
      "          3.9212e-04, -5.5881e-03],\n",
      "        [ 5.2370e-04,  6.3959e-04, -2.9299e-05,  ..., -6.5100e-04,\n",
      "          1.4990e-04,  1.5579e-03],\n",
      "        [ 1.2241e-03, -2.9018e-03,  9.1303e-04,  ...,  1.6600e-03,\n",
      "          1.7255e-05,  5.6979e-03],\n",
      "        ...,\n",
      "        [-1.6516e-04,  4.0825e-03,  3.0161e-04,  ...,  1.9833e-03,\n",
      "          1.5947e-03, -5.3616e-03],\n",
      "        [ 7.2081e-03, -4.0865e-03,  1.8238e-02,  ..., -8.0687e-03,\n",
      "         -6.7589e-03, -5.6906e-03],\n",
      "        [ 1.7323e-03,  1.6911e-03, -1.0071e-03,  ..., -1.8038e-04,\n",
      "         -1.3548e-03, -4.1721e-03]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight tensor([[ 0.0005, -0.0087,  0.0154,  ...,  0.0062, -0.0006, -0.0013],\n",
      "        [ 0.0066, -0.0006, -0.0030,  ..., -0.0022, -0.0064,  0.0015],\n",
      "        [-0.0052, -0.0007,  0.0081,  ..., -0.0076,  0.0051, -0.0006],\n",
      "        ...,\n",
      "        [ 0.0042, -0.0065,  0.0089,  ..., -0.0030,  0.0008, -0.0069],\n",
      "        [-0.0021,  0.0012, -0.0027,  ..., -0.0024,  0.0017,  0.0169],\n",
      "        [ 0.0088, -0.0059, -0.0067,  ..., -0.0032, -0.0060, -0.0067]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight tensor([[-4.5369e-03, -4.6300e-03, -6.5569e-03,  ...,  4.9053e-03,\n",
      "          1.6741e-03, -5.6659e-03],\n",
      "        [-2.1474e-04, -1.9093e-03, -8.3193e-04,  ..., -1.1570e-03,\n",
      "         -1.0319e-03,  1.2940e-04],\n",
      "        [ 3.5836e-04, -2.9390e-03,  5.3891e-04,  ..., -3.3008e-03,\n",
      "         -2.6876e-03, -4.2963e-03],\n",
      "        ...,\n",
      "        [ 1.7630e-03, -4.3228e-03,  3.9348e-04,  ..., -6.0572e-05,\n",
      "         -1.0059e-02, -5.2261e-04],\n",
      "        [-8.2271e-03,  1.0982e-02, -6.7248e-03,  ..., -1.8226e-02,\n",
      "          5.9406e-03, -6.7866e-03],\n",
      "        [-8.4597e-06, -2.1451e-03, -1.3792e-04,  ..., -5.9541e-04,\n",
      "         -1.0517e-04, -7.3338e-03]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight tensor([[ 3.5590e-03,  7.8786e-03, -1.1748e-02,  ...,  5.0340e-03,\n",
      "          2.8332e-04, -8.0651e-03],\n",
      "        [-1.3892e-02,  8.6803e-03,  3.8162e-03,  ...,  3.5267e-03,\n",
      "         -4.7087e-03, -4.2456e-03],\n",
      "        [-4.6763e-03, -1.7205e-03, -5.9470e-03,  ..., -2.3954e-03,\n",
      "          3.6627e-03, -5.6813e-03],\n",
      "        ...,\n",
      "        [-3.0318e-03, -5.5972e-03, -2.6777e-04,  ..., -1.3040e-02,\n",
      "         -5.1576e-03,  4.9787e-04],\n",
      "        [ 1.9807e-03, -6.2940e-03, -1.2436e-02,  ...,  1.1397e-02,\n",
      "         -6.9882e-03,  3.6724e-03],\n",
      "        [-3.2503e-05,  8.7418e-03, -1.0835e-03,  ...,  3.4590e-03,\n",
      "          1.9509e-04,  1.1331e-02]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight tensor([[-6.0629e-04, -8.5032e-03,  1.5154e-03,  ...,  1.8488e-03,\n",
      "          6.7775e-04,  1.2059e-03],\n",
      "        [-1.8968e-04, -1.3301e-04,  4.5137e-05,  ...,  1.0088e-04,\n",
      "          6.9427e-04,  1.2375e-04],\n",
      "        [ 1.9438e-03,  1.7672e-03,  5.3363e-03,  ..., -3.7532e-03,\n",
      "         -1.2070e-03,  2.3742e-03],\n",
      "        ...,\n",
      "        [ 5.6594e-04, -2.3541e-03,  3.8297e-03,  ..., -4.9640e-04,\n",
      "          1.2266e-03, -3.9401e-03],\n",
      "        [ 1.2396e-02,  3.0342e-03, -3.0808e-03,  ..., -9.3923e-04,\n",
      "         -2.9670e-03, -1.1809e-02],\n",
      "        [-1.9082e-03, -1.7222e-03, -3.7949e-04,  ..., -2.3601e-03,\n",
      "          4.9501e-03, -2.0471e-03]])\n",
      "unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight tensor([[ 8.3174e-03,  5.0514e-03,  5.2893e-03,  ..., -8.2238e-03,\n",
      "          5.3189e-03,  3.5051e-03],\n",
      "        [ 2.0782e-03,  1.2983e-02, -4.3975e-03,  ..., -2.7791e-03,\n",
      "          3.8999e-03,  8.0714e-03],\n",
      "        [-4.2426e-03,  2.1264e-04, -5.2360e-03,  ...,  1.1442e-02,\n",
      "         -1.0160e-02,  6.2080e-03],\n",
      "        ...,\n",
      "        [-6.6893e-04, -7.2201e-04, -2.9062e-03,  ..., -7.0081e-03,\n",
      "          5.2971e-04, -1.7699e-03],\n",
      "        [-5.3336e-04, -1.0974e-03, -9.4765e-05,  ...,  4.9656e-04,\n",
      "          3.2896e-03, -3.0045e-03],\n",
      "        [ 1.8852e-03, -5.7403e-03,  7.6314e-03,  ...,  1.8431e-03,\n",
      "          9.9669e-03, -3.7649e-03]])\n",
      "unet.down_blocks.1.attentions.1.proj_in.lora.down.weight tensor([[[[-2.0895e-03]],\n",
      "\n",
      "         [[-4.9087e-03]],\n",
      "\n",
      "         [[-3.4945e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.5187e-03]],\n",
      "\n",
      "         [[ 2.8586e-04]],\n",
      "\n",
      "         [[ 3.5073e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1260e-04]],\n",
      "\n",
      "         [[-1.4314e-03]],\n",
      "\n",
      "         [[ 3.0818e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.8236e-04]],\n",
      "\n",
      "         [[-2.1364e-04]],\n",
      "\n",
      "         [[ 9.2643e-05]]],\n",
      "\n",
      "\n",
      "        [[[-3.3463e-03]],\n",
      "\n",
      "         [[ 8.5846e-03]],\n",
      "\n",
      "         [[ 4.0521e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2909e-02]],\n",
      "\n",
      "         [[-1.6112e-03]],\n",
      "\n",
      "         [[-4.1796e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.2948e-03]],\n",
      "\n",
      "         [[-8.9691e-03]],\n",
      "\n",
      "         [[-1.4282e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.5324e-03]],\n",
      "\n",
      "         [[ 4.8976e-04]],\n",
      "\n",
      "         [[ 1.8784e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 9.7715e-03]],\n",
      "\n",
      "         [[ 6.0788e-04]],\n",
      "\n",
      "         [[-2.7256e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.1028e-03]],\n",
      "\n",
      "         [[-1.2641e-02]],\n",
      "\n",
      "         [[ 9.5681e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7109e-03]],\n",
      "\n",
      "         [[-8.5703e-04]],\n",
      "\n",
      "         [[-5.0738e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.8404e-03]],\n",
      "\n",
      "         [[ 7.1591e-03]],\n",
      "\n",
      "         [[ 1.1987e-03]]]])\n",
      "unet.down_blocks.1.attentions.1.proj_in.lora.up.weight tensor([[[[-0.0049]],\n",
      "\n",
      "         [[ 0.0061]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[-0.0133]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0057]],\n",
      "\n",
      "         [[-0.0101]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0055]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         [[-0.0049]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0018]],\n",
      "\n",
      "         [[ 0.0101]],\n",
      "\n",
      "         [[ 0.0068]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         [[-0.0045]],\n",
      "\n",
      "         [[ 0.0017]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0099]],\n",
      "\n",
      "         [[-0.0143]],\n",
      "\n",
      "         [[ 0.0061]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0046]],\n",
      "\n",
      "         [[-0.0081]],\n",
      "\n",
      "         [[-0.0040]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0027]],\n",
      "\n",
      "         [[-0.0081]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[-0.0095]],\n",
      "\n",
      "         [[ 0.0017]]],\n",
      "\n",
      "\n",
      "        [[[-0.0088]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[-0.0037]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0097]],\n",
      "\n",
      "         [[-0.0069]],\n",
      "\n",
      "         [[-0.0007]]]])\n",
      "unet.down_blocks.1.attentions.1.proj_out.lora.down.weight tensor([[[[ 4.2745e-03]],\n",
      "\n",
      "         [[-4.0506e-03]],\n",
      "\n",
      "         [[-2.7880e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0367e-04]],\n",
      "\n",
      "         [[-1.9385e-03]],\n",
      "\n",
      "         [[ 1.7616e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9477e-03]],\n",
      "\n",
      "         [[-6.6559e-04]],\n",
      "\n",
      "         [[-5.2191e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7425e-03]],\n",
      "\n",
      "         [[ 1.0367e-05]],\n",
      "\n",
      "         [[ 5.9083e-04]]],\n",
      "\n",
      "\n",
      "        [[[-2.5752e-03]],\n",
      "\n",
      "         [[ 2.9020e-03]],\n",
      "\n",
      "         [[ 5.9527e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.3753e-03]],\n",
      "\n",
      "         [[ 4.1921e-03]],\n",
      "\n",
      "         [[-2.5421e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 5.8653e-04]],\n",
      "\n",
      "         [[-1.0271e-03]],\n",
      "\n",
      "         [[-1.7837e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.1295e-03]],\n",
      "\n",
      "         [[-1.0496e-04]],\n",
      "\n",
      "         [[ 1.9698e-03]]],\n",
      "\n",
      "\n",
      "        [[[-9.0674e-03]],\n",
      "\n",
      "         [[-1.1130e-02]],\n",
      "\n",
      "         [[-3.1463e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.2552e-04]],\n",
      "\n",
      "         [[ 1.1864e-02]],\n",
      "\n",
      "         [[ 2.8432e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.0200e-03]],\n",
      "\n",
      "         [[-5.4229e-03]],\n",
      "\n",
      "         [[ 1.8072e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.4783e-04]],\n",
      "\n",
      "         [[-2.1206e-03]],\n",
      "\n",
      "         [[-3.0023e-03]]]])\n",
      "unet.down_blocks.1.attentions.1.proj_out.lora.up.weight tensor([[[[-0.0101]],\n",
      "\n",
      "         [[-0.0069]],\n",
      "\n",
      "         [[-0.0063]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         [[ 0.0041]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0051]],\n",
      "\n",
      "         [[-0.0150]],\n",
      "\n",
      "         [[-0.0256]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0066]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[ 0.0039]]],\n",
      "\n",
      "\n",
      "        [[[-0.0076]],\n",
      "\n",
      "         [[-0.0030]],\n",
      "\n",
      "         [[ 0.0062]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0063]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[ 0.0034]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0002]],\n",
      "\n",
      "         [[-0.0054]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0072]],\n",
      "\n",
      "         [[-0.0027]],\n",
      "\n",
      "         [[ 0.0110]]],\n",
      "\n",
      "\n",
      "        [[[-0.0026]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[-0.0038]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[-0.0092]],\n",
      "\n",
      "         [[ 0.0014]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0012]],\n",
      "\n",
      "         [[ 0.0149]],\n",
      "\n",
      "         [[ 0.0089]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[-0.0055]],\n",
      "\n",
      "         [[ 0.0035]]]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight tensor([[ 2.4043e-03, -4.0245e-04, -3.6447e-03,  ..., -2.8708e-03,\n",
      "          4.0374e-03, -3.2792e-03],\n",
      "        [ 4.5219e-04, -1.1140e-03, -4.2020e-05,  ...,  7.3049e-04,\n",
      "          6.0130e-04, -1.8801e-03],\n",
      "        [ 4.3607e-04, -1.7383e-03,  5.2934e-04,  ...,  4.2416e-03,\n",
      "         -3.8407e-05,  6.5156e-03],\n",
      "        ...,\n",
      "        [ 1.8570e-03, -7.9245e-03,  2.5740e-03,  ...,  1.8207e-03,\n",
      "          2.0784e-03, -1.9489e-03],\n",
      "        [-5.7973e-03, -4.8744e-03, -3.1559e-03,  ...,  4.9247e-03,\n",
      "         -1.6457e-02, -7.0350e-03],\n",
      "        [ 9.8173e-04,  3.0350e-03, -7.2289e-04,  ...,  3.1296e-03,\n",
      "         -3.9288e-03,  1.8547e-03]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight tensor([[ 0.0002, -0.0010, -0.0024,  ...,  0.0009,  0.0050, -0.0027],\n",
      "        [ 0.0093, -0.0035,  0.0026,  ..., -0.0077, -0.0019, -0.0002],\n",
      "        [-0.0011,  0.0002, -0.0036,  ...,  0.0029,  0.0046, -0.0064],\n",
      "        ...,\n",
      "        [ 0.0047, -0.0056,  0.0026,  ..., -0.0015,  0.0022,  0.0018],\n",
      "        [ 0.0045,  0.0026, -0.0097,  ...,  0.0082,  0.0052,  0.0003],\n",
      "        [ 0.0075, -0.0015, -0.0075,  ..., -0.0016, -0.0006, -0.0118]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight tensor([[ 9.5354e-04, -4.8835e-04,  5.0668e-03,  ..., -1.4479e-03,\n",
      "         -2.0759e-03,  2.4038e-03],\n",
      "        [-4.0952e-04, -1.6476e-04,  2.7199e-04,  ...,  9.7277e-04,\n",
      "         -2.9723e-04, -1.7138e-03],\n",
      "        [ 4.7122e-04, -4.1038e-03, -5.1046e-04,  ...,  3.0004e-03,\n",
      "          1.6585e-04,  2.9836e-03],\n",
      "        ...,\n",
      "        [ 5.2092e-03, -8.4441e-06,  7.5846e-04,  ...,  2.8802e-03,\n",
      "         -1.6341e-04,  4.2531e-03],\n",
      "        [ 5.3858e-03, -6.2014e-03,  1.0814e-02,  ..., -2.4907e-03,\n",
      "          7.0826e-04, -4.5959e-03],\n",
      "        [-4.0496e-03,  4.4127e-03,  1.2367e-04,  ...,  2.4049e-03,\n",
      "         -4.1092e-03, -1.0079e-03]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight tensor([[ 0.0043,  0.0041,  0.0008,  ...,  0.0034, -0.0038, -0.0005],\n",
      "        [ 0.0032, -0.0098,  0.0002,  ..., -0.0007,  0.0016, -0.0056],\n",
      "        [-0.0080, -0.0012, -0.0022,  ..., -0.0019,  0.0013,  0.0099],\n",
      "        ...,\n",
      "        [-0.0152,  0.0030, -0.0080,  ...,  0.0078,  0.0050, -0.0015],\n",
      "        [-0.0039, -0.0123,  0.0062,  ...,  0.0041, -0.0019, -0.0002],\n",
      "        [-0.0042,  0.0008, -0.0037,  ...,  0.0068,  0.0066, -0.0007]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight tensor([[-9.7125e-04, -1.1126e-02,  1.6648e-03,  ..., -3.3923e-03,\n",
      "          6.9899e-05, -3.8260e-03],\n",
      "        [ 1.3663e-03,  1.2502e-03, -1.2025e-03,  ...,  1.6989e-04,\n",
      "          8.1525e-04,  1.5012e-03],\n",
      "        [-3.6854e-03, -6.8792e-04, -1.3302e-03,  ...,  3.0612e-03,\n",
      "         -3.4876e-03, -9.5825e-03],\n",
      "        ...,\n",
      "        [-1.0265e-02, -5.1114e-03,  7.9873e-03,  ...,  2.2298e-03,\n",
      "          7.6010e-04, -3.9722e-03],\n",
      "        [ 8.3213e-03,  1.5669e-02, -1.4589e-02,  ...,  1.0458e-02,\n",
      "         -5.2764e-03, -6.1856e-03],\n",
      "        [-2.6265e-03,  4.1778e-03, -3.6889e-03,  ...,  1.4838e-03,\n",
      "         -2.8255e-03,  2.8660e-03]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight tensor([[ 0.0051,  0.0045,  0.0022,  ...,  0.0031,  0.0003,  0.0049],\n",
      "        [-0.0094, -0.0006, -0.0074,  ...,  0.0012,  0.0015,  0.0090],\n",
      "        [-0.0030,  0.0056,  0.0085,  ..., -0.0015, -0.0017,  0.0060],\n",
      "        ...,\n",
      "        [ 0.0016, -0.0037,  0.0020,  ...,  0.0114, -0.0071,  0.0002],\n",
      "        [ 0.0085,  0.0008,  0.0157,  ..., -0.0086,  0.0104,  0.0031],\n",
      "        [ 0.0043,  0.0026,  0.0130,  ...,  0.0015, -0.0048, -0.0138]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight tensor([[-1.3812e-03,  2.3645e-03, -4.0672e-03,  ...,  5.9860e-04,\n",
      "         -3.2933e-03,  6.2400e-04],\n",
      "        [-1.2758e-03, -3.8140e-04,  1.5867e-03,  ...,  8.4320e-04,\n",
      "          2.5375e-04, -1.4939e-04],\n",
      "        [ 1.0964e-03,  2.7278e-03,  2.8060e-03,  ..., -5.0216e-03,\n",
      "          7.0229e-03, -4.4826e-03],\n",
      "        ...,\n",
      "        [-1.9878e-03,  4.5510e-03,  4.2502e-05,  ...,  7.5456e-03,\n",
      "          5.6308e-03, -7.5696e-03],\n",
      "        [ 1.0119e-02,  3.5600e-03, -1.0975e-02,  ...,  1.0551e-02,\n",
      "          1.2329e-02, -3.4829e-03],\n",
      "        [-5.6455e-03, -3.1988e-03,  5.5811e-03,  ...,  3.5008e-03,\n",
      "         -2.7326e-03, -1.9063e-03]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight tensor([[-1.1660e-03,  7.1420e-03,  2.1186e-03,  ..., -3.9428e-03,\n",
      "         -7.6446e-03,  6.7767e-03],\n",
      "        [ 5.0878e-03,  6.1241e-03, -4.7122e-03,  ...,  6.6480e-04,\n",
      "         -6.3313e-04, -9.2397e-03],\n",
      "        [ 8.7304e-04,  2.1263e-03,  3.7448e-03,  ..., -1.0852e-03,\n",
      "         -1.5775e-04, -8.1591e-03],\n",
      "        ...,\n",
      "        [ 4.1656e-03, -5.1941e-03, -1.7310e-03,  ...,  3.8552e-03,\n",
      "          1.1964e-05,  2.8504e-03],\n",
      "        [ 1.0534e-03, -2.2199e-03, -1.0368e-03,  ..., -9.6384e-03,\n",
      "         -8.5575e-04, -7.0140e-03],\n",
      "        [ 2.1439e-03, -6.6007e-03,  3.5758e-03,  ...,  4.8581e-03,\n",
      "          8.5601e-04,  2.6160e-03]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight tensor([[-1.9870e-03,  1.1417e-03, -2.0939e-03,  ..., -8.2111e-04,\n",
      "         -2.2001e-03, -1.1321e-03],\n",
      "        [ 2.4456e-04, -2.6967e-05,  2.4628e-04,  ..., -3.5035e-04,\n",
      "          5.2726e-04, -7.7882e-04],\n",
      "        [ 3.6285e-03,  1.0678e-03, -3.0442e-03,  ...,  1.4768e-03,\n",
      "         -3.4029e-03, -1.3483e-03],\n",
      "        ...,\n",
      "        [ 1.5477e-03,  5.2215e-04, -3.2740e-03,  ..., -3.3112e-04,\n",
      "         -3.1704e-03,  7.3593e-04],\n",
      "        [-2.0948e-03, -2.2154e-03, -3.6172e-04,  ...,  2.3831e-03,\n",
      "          7.8425e-04,  5.6969e-03],\n",
      "        [-1.9546e-05, -1.2805e-03, -6.0189e-04,  ..., -2.8689e-04,\n",
      "         -5.6538e-04,  6.5429e-04]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight tensor([[-0.0014, -0.0032,  0.0033,  ..., -0.0002,  0.0034,  0.0053],\n",
      "        [ 0.0008,  0.0002, -0.0053,  ...,  0.0078,  0.0015, -0.0006],\n",
      "        [ 0.0012,  0.0038, -0.0016,  ...,  0.0020,  0.0023, -0.0001],\n",
      "        ...,\n",
      "        [-0.0022, -0.0013,  0.0067,  ...,  0.0057,  0.0028,  0.0030],\n",
      "        [-0.0003, -0.0027, -0.0045,  ..., -0.0037, -0.0040,  0.0027],\n",
      "        [ 0.0011, -0.0022, -0.0064,  ..., -0.0024, -0.0012,  0.0088]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight tensor([[-2.1922e-03, -3.1850e-03,  5.2351e-03,  ..., -1.0655e-03,\n",
      "         -3.3312e-03, -1.2139e-05],\n",
      "        [-8.8401e-04,  1.3897e-04, -6.2466e-04,  ..., -1.4247e-03,\n",
      "          1.3811e-03, -4.9263e-06],\n",
      "        [-6.0046e-04, -2.8784e-03, -4.2147e-03,  ...,  2.5898e-03,\n",
      "          4.7585e-04,  6.0154e-03],\n",
      "        ...,\n",
      "        [-1.3021e-03,  1.0314e-03,  2.5017e-03,  ..., -2.3556e-03,\n",
      "         -1.9947e-03, -6.3585e-06],\n",
      "        [-9.2735e-03, -1.9485e-03, -1.4381e-02,  ..., -8.4666e-03,\n",
      "         -5.5649e-03,  3.1009e-03],\n",
      "        [-1.6696e-03, -1.6365e-03, -1.9250e-03,  ..., -5.6881e-03,\n",
      "          4.3479e-04, -1.9690e-03]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight tensor([[-6.1217e-04, -1.1263e-02, -3.3832e-03,  ..., -2.7299e-03,\n",
      "         -7.4509e-03,  4.6034e-03],\n",
      "        [ 1.9717e-03, -5.6940e-04,  5.6206e-05,  ...,  2.6488e-03,\n",
      "          5.7383e-03,  4.1919e-03],\n",
      "        [ 4.1761e-04,  1.8180e-03,  5.0229e-03,  ...,  3.9864e-03,\n",
      "          3.0261e-03, -2.0296e-04],\n",
      "        ...,\n",
      "        [-6.8800e-04, -1.3476e-03,  8.8384e-03,  ..., -3.2106e-03,\n",
      "         -1.8191e-03,  4.3479e-03],\n",
      "        [-1.9422e-03, -6.2188e-03,  3.7166e-03,  ...,  4.7350e-03,\n",
      "          7.9916e-03, -5.5167e-03],\n",
      "        [-2.0508e-03,  6.2088e-03,  3.7140e-04,  ...,  8.4022e-03,\n",
      "         -1.4558e-03, -4.3424e-03]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight tensor([[ 5.8422e-04, -1.3419e-03,  9.4891e-05,  ..., -4.5358e-03,\n",
      "          1.2331e-03,  5.5404e-04],\n",
      "        [-5.5377e-05, -3.1626e-05,  3.0665e-04,  ..., -8.4424e-06,\n",
      "          3.3960e-04, -1.0286e-03],\n",
      "        [ 3.1729e-04, -9.0925e-04, -1.1234e-03,  ..., -5.9461e-04,\n",
      "          1.6656e-03, -2.9049e-03],\n",
      "        ...,\n",
      "        [ 1.4066e-03,  2.8128e-05, -8.3266e-04,  ...,  1.8040e-03,\n",
      "         -5.3422e-03,  2.4810e-03],\n",
      "        [-3.3956e-03, -8.9749e-03,  1.0987e-02,  ..., -1.2265e-02,\n",
      "         -1.1935e-02, -5.3017e-03],\n",
      "        [ 3.6784e-03,  1.3836e-03,  3.6176e-05,  ..., -5.9680e-06,\n",
      "          6.1954e-04,  2.0670e-04]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight tensor([[ 0.0002,  0.0002,  0.0003,  ..., -0.0014,  0.0031, -0.0025],\n",
      "        [ 0.0029, -0.0014, -0.0009,  ..., -0.0050,  0.0038,  0.0002],\n",
      "        [ 0.0043, -0.0017, -0.0006,  ...,  0.0029,  0.0011, -0.0007],\n",
      "        ...,\n",
      "        [ 0.0014,  0.0009,  0.0015,  ...,  0.0083, -0.0029,  0.0024],\n",
      "        [-0.0076, -0.0023,  0.0018,  ...,  0.0031, -0.0023, -0.0046],\n",
      "        [-0.0002,  0.0003, -0.0022,  ..., -0.0008, -0.0060, -0.0017]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight tensor([[ 3.5141e-04, -5.7339e-03,  5.2210e-04,  ...,  3.5328e-06,\n",
      "         -3.3540e-03,  3.5036e-03],\n",
      "        [ 4.3487e-04,  1.7214e-04, -1.2804e-03,  ..., -7.2568e-04,\n",
      "         -1.2737e-05, -1.8052e-04],\n",
      "        [-9.8400e-04,  2.8141e-03,  1.1880e-03,  ...,  1.2208e-03,\n",
      "          3.3886e-03, -1.0585e-03],\n",
      "        ...,\n",
      "        [-1.2173e-03, -1.3361e-03,  1.4322e-03,  ..., -1.6653e-04,\n",
      "         -9.7266e-04,  2.4847e-03],\n",
      "        [ 3.5090e-03,  4.4640e-03, -9.8663e-03,  ...,  5.0460e-03,\n",
      "         -3.0646e-03,  1.8614e-03],\n",
      "        [-1.5533e-03,  5.3209e-04, -2.7556e-03,  ..., -1.6450e-03,\n",
      "         -1.5512e-03,  1.0596e-03]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight tensor([[ 5.1242e-03, -6.3351e-03, -2.9772e-03,  ...,  4.9727e-03,\n",
      "          3.7603e-03,  4.2931e-03],\n",
      "        [-2.3745e-03,  6.2516e-03,  1.0458e-03,  ..., -5.9598e-04,\n",
      "          2.0834e-03,  1.5903e-02],\n",
      "        [-4.2161e-03, -1.0039e-04,  6.7829e-04,  ...,  1.0660e-02,\n",
      "         -8.7505e-04, -2.4747e-03],\n",
      "        ...,\n",
      "        [ 2.0591e-03, -7.5771e-05, -2.6958e-03,  ...,  2.3496e-03,\n",
      "         -1.5836e-03,  4.4140e-03],\n",
      "        [-2.4277e-03,  5.6055e-03,  9.3829e-03,  ..., -1.6714e-03,\n",
      "         -1.1258e-03, -1.4261e-03],\n",
      "        [-6.0018e-03,  9.6971e-03, -2.7861e-03,  ..., -1.1973e-02,\n",
      "         -3.5946e-03,  2.0173e-03]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight tensor([[-0.0045, -0.0056,  0.0011,  ..., -0.0055,  0.0054, -0.0014],\n",
      "        [-0.0011,  0.0012,  0.0013,  ..., -0.0002, -0.0009,  0.0004],\n",
      "        [ 0.0004, -0.0049, -0.0034,  ..., -0.0015, -0.0008, -0.0006],\n",
      "        ...,\n",
      "        [-0.0072,  0.0035, -0.0021,  ..., -0.0039, -0.0012,  0.0055],\n",
      "        [ 0.0098,  0.0081, -0.0099,  ..., -0.0078,  0.0005, -0.0015],\n",
      "        [-0.0006,  0.0029,  0.0001,  ..., -0.0012, -0.0017, -0.0031]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight tensor([[-0.0019, -0.0153, -0.0040,  ..., -0.0013, -0.0081, -0.0028],\n",
      "        [ 0.0002,  0.0006, -0.0062,  ...,  0.0074, -0.0071,  0.0065],\n",
      "        [ 0.0021, -0.0004, -0.0015,  ..., -0.0055, -0.0052,  0.0046],\n",
      "        ...,\n",
      "        [-0.0034, -0.0044,  0.0052,  ...,  0.0063,  0.0013,  0.0096],\n",
      "        [-0.0090,  0.0007, -0.0044,  ...,  0.0019,  0.0121, -0.0034],\n",
      "        [-0.0021, -0.0092,  0.0036,  ..., -0.0066, -0.0093,  0.0024]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight tensor([[-0.0054,  0.0014,  0.0003,  ...,  0.0048,  0.0044,  0.0070],\n",
      "        [-0.0001, -0.0005, -0.0005,  ...,  0.0001, -0.0011,  0.0003],\n",
      "        [-0.0044,  0.0010,  0.0016,  ..., -0.0007, -0.0045, -0.0048],\n",
      "        ...,\n",
      "        [ 0.0022,  0.0047, -0.0021,  ..., -0.0016, -0.0072,  0.0019],\n",
      "        [-0.0093,  0.0080,  0.0072,  ..., -0.0241, -0.0151,  0.0064],\n",
      "        [ 0.0040,  0.0012,  0.0035,  ...,  0.0018,  0.0026, -0.0013]])\n",
      "unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight tensor([[ 0.0019,  0.0053, -0.0046,  ...,  0.0003, -0.0073, -0.0035],\n",
      "        [ 0.0042,  0.0002, -0.0067,  ..., -0.0025, -0.0048, -0.0006],\n",
      "        [ 0.0009, -0.0004,  0.0117,  ...,  0.0050,  0.0058,  0.0007],\n",
      "        ...,\n",
      "        [-0.0031,  0.0019, -0.0032,  ...,  0.0094, -0.0015,  0.0069],\n",
      "        [-0.0069,  0.0025,  0.0071,  ...,  0.0021, -0.0105, -0.0122],\n",
      "        [-0.0011,  0.0013,  0.0117,  ...,  0.0065, -0.0029,  0.0095]])\n",
      "unet.down_blocks.2.attentions.0.proj_in.lora.down.weight tensor([[[[ 1.8302e-03]],\n",
      "\n",
      "         [[-4.3175e-03]],\n",
      "\n",
      "         [[ 6.4102e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3858e-02]],\n",
      "\n",
      "         [[-6.7051e-03]],\n",
      "\n",
      "         [[-1.7808e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9798e-03]],\n",
      "\n",
      "         [[ 4.8862e-05]],\n",
      "\n",
      "         [[-5.9207e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.3726e-04]],\n",
      "\n",
      "         [[-2.1049e-04]],\n",
      "\n",
      "         [[-1.3374e-04]]],\n",
      "\n",
      "\n",
      "        [[[-4.4988e-03]],\n",
      "\n",
      "         [[ 4.1881e-03]],\n",
      "\n",
      "         [[ 4.7734e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.6345e-03]],\n",
      "\n",
      "         [[ 2.0309e-03]],\n",
      "\n",
      "         [[ 1.9451e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 9.6485e-03]],\n",
      "\n",
      "         [[-4.5358e-03]],\n",
      "\n",
      "         [[-1.0207e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.1299e-04]],\n",
      "\n",
      "         [[ 2.8372e-03]],\n",
      "\n",
      "         [[ 6.4534e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2111e-02]],\n",
      "\n",
      "         [[ 9.8895e-03]],\n",
      "\n",
      "         [[ 1.2210e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7303e-02]],\n",
      "\n",
      "         [[ 2.2136e-03]],\n",
      "\n",
      "         [[-8.9663e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5963e-03]],\n",
      "\n",
      "         [[ 4.4418e-03]],\n",
      "\n",
      "         [[ 4.9432e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.9412e-05]],\n",
      "\n",
      "         [[-1.7877e-03]],\n",
      "\n",
      "         [[-8.1415e-04]]]])\n",
      "unet.down_blocks.2.attentions.0.proj_in.lora.up.weight tensor([[[[ 0.0158]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0108]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[ 0.0052]]],\n",
      "\n",
      "\n",
      "        [[[-0.0112]],\n",
      "\n",
      "         [[-0.0103]],\n",
      "\n",
      "         [[ 0.0102]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         [[-0.0152]]],\n",
      "\n",
      "\n",
      "        [[[-0.0031]],\n",
      "\n",
      "         [[-0.0174]],\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0071]],\n",
      "\n",
      "         [[ 0.0145]],\n",
      "\n",
      "         [[ 0.0140]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0085]],\n",
      "\n",
      "         [[ 0.0088]],\n",
      "\n",
      "         [[ 0.0087]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0112]],\n",
      "\n",
      "         [[ 0.0046]],\n",
      "\n",
      "         [[ 0.0134]]],\n",
      "\n",
      "\n",
      "        [[[-0.0034]],\n",
      "\n",
      "         [[ 0.0083]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0132]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[-0.0011]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0162]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0041]],\n",
      "\n",
      "         [[-0.0142]],\n",
      "\n",
      "         [[-0.0039]]]])\n",
      "unet.down_blocks.2.attentions.0.proj_out.lora.down.weight tensor([[[[-3.2457e-03]],\n",
      "\n",
      "         [[-1.8099e-04]],\n",
      "\n",
      "         [[ 6.4469e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.5981e-04]],\n",
      "\n",
      "         [[-9.3949e-03]],\n",
      "\n",
      "         [[-4.4000e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.1923e-03]],\n",
      "\n",
      "         [[ 6.2464e-04]],\n",
      "\n",
      "         [[-1.3712e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8235e-03]],\n",
      "\n",
      "         [[ 4.5066e-04]],\n",
      "\n",
      "         [[ 2.2314e-03]]],\n",
      "\n",
      "\n",
      "        [[[-6.4331e-03]],\n",
      "\n",
      "         [[ 6.6671e-03]],\n",
      "\n",
      "         [[-6.7633e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.6943e-03]],\n",
      "\n",
      "         [[ 2.2918e-04]],\n",
      "\n",
      "         [[ 9.4314e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 7.0136e-04]],\n",
      "\n",
      "         [[-5.9257e-03]],\n",
      "\n",
      "         [[-5.5171e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8537e-03]],\n",
      "\n",
      "         [[-2.1785e-03]],\n",
      "\n",
      "         [[-7.5557e-03]]],\n",
      "\n",
      "\n",
      "        [[[-8.6579e-03]],\n",
      "\n",
      "         [[ 2.4920e-02]],\n",
      "\n",
      "         [[-1.3346e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.4766e-03]],\n",
      "\n",
      "         [[ 4.0536e-02]],\n",
      "\n",
      "         [[ 5.5522e-03]]],\n",
      "\n",
      "\n",
      "        [[[-9.9426e-05]],\n",
      "\n",
      "         [[-6.7979e-03]],\n",
      "\n",
      "         [[ 5.5646e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.9549e-03]],\n",
      "\n",
      "         [[ 7.1561e-04]],\n",
      "\n",
      "         [[-1.3047e-03]]]])\n",
      "unet.down_blocks.2.attentions.0.proj_out.lora.up.weight tensor([[[[ 1.7518e-03]],\n",
      "\n",
      "         [[-8.0690e-03]],\n",
      "\n",
      "         [[ 1.1574e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.9206e-04]],\n",
      "\n",
      "         [[ 5.7612e-03]],\n",
      "\n",
      "         [[ 1.3872e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4803e-03]],\n",
      "\n",
      "         [[-4.8601e-03]],\n",
      "\n",
      "         [[-2.0531e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7107e-03]],\n",
      "\n",
      "         [[ 9.7001e-03]],\n",
      "\n",
      "         [[ 8.0872e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.7444e-04]],\n",
      "\n",
      "         [[-1.0607e-02]],\n",
      "\n",
      "         [[ 8.0374e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.6070e-03]],\n",
      "\n",
      "         [[-4.4673e-03]],\n",
      "\n",
      "         [[-6.1291e-05]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.0121e-03]],\n",
      "\n",
      "         [[ 1.3109e-03]],\n",
      "\n",
      "         [[-1.1982e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.2047e-03]],\n",
      "\n",
      "         [[ 2.0751e-03]],\n",
      "\n",
      "         [[ 5.3558e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 5.1275e-03]],\n",
      "\n",
      "         [[ 1.3702e-03]],\n",
      "\n",
      "         [[ 1.3434e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0621e-03]],\n",
      "\n",
      "         [[-8.7145e-03]],\n",
      "\n",
      "         [[ 1.3648e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4148e-02]],\n",
      "\n",
      "         [[-8.3146e-03]],\n",
      "\n",
      "         [[ 1.2483e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.0503e-03]],\n",
      "\n",
      "         [[-1.5981e-02]],\n",
      "\n",
      "         [[ 2.0799e-03]]]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight tensor([[-3.0385e-03, -4.7691e-03,  1.5644e-03,  ..., -5.7596e-03,\n",
      "         -1.7852e-04, -1.7824e-03],\n",
      "        [ 8.0262e-04, -1.5415e-04,  4.8849e-04,  ..., -1.2860e-03,\n",
      "         -1.6418e-04,  1.0837e-04],\n",
      "        [ 1.8379e-03,  2.2418e-03,  1.0850e-03,  ...,  1.0833e-03,\n",
      "          3.3592e-04, -1.1140e-03],\n",
      "        ...,\n",
      "        [ 2.6195e-03,  5.5653e-03,  2.9479e-03,  ..., -2.0498e-03,\n",
      "          1.3536e-03,  3.5485e-04],\n",
      "        [-1.7945e-02,  1.6798e-02, -6.3983e-03,  ..., -1.3185e-03,\n",
      "          9.9888e-04, -1.6340e-02],\n",
      "        [-5.4781e-03,  5.9897e-05, -3.6263e-03,  ...,  5.1892e-03,\n",
      "          3.7211e-03, -3.3671e-03]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight tensor([[ 0.0020,  0.0011,  0.0034,  ..., -0.0056,  0.0081,  0.0051],\n",
      "        [ 0.0026, -0.0052, -0.0067,  ..., -0.0029,  0.0076, -0.0014],\n",
      "        [ 0.0035, -0.0023, -0.0058,  ..., -0.0005, -0.0015,  0.0026],\n",
      "        ...,\n",
      "        [-0.0074, -0.0037, -0.0005,  ..., -0.0004,  0.0001, -0.0022],\n",
      "        [-0.0058, -0.0039, -0.0016,  ...,  0.0060,  0.0079, -0.0043],\n",
      "        [-0.0021, -0.0009,  0.0029,  ..., -0.0006,  0.0056, -0.0015]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight tensor([[-0.0052,  0.0045, -0.0075,  ...,  0.0015,  0.0070, -0.0078],\n",
      "        [-0.0012,  0.0011, -0.0004,  ..., -0.0006, -0.0012, -0.0023],\n",
      "        [-0.0043, -0.0009, -0.0073,  ...,  0.0009,  0.0051,  0.0040],\n",
      "        ...,\n",
      "        [ 0.0044,  0.0023,  0.0017,  ..., -0.0004, -0.0006,  0.0009],\n",
      "        [ 0.0179, -0.0213, -0.0192,  ...,  0.0066, -0.0220,  0.0257],\n",
      "        [-0.0009, -0.0002,  0.0004,  ...,  0.0019,  0.0016, -0.0021]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight tensor([[-0.0075, -0.0055, -0.0129,  ..., -0.0043,  0.0006,  0.0147],\n",
      "        [-0.0057,  0.0002,  0.0025,  ...,  0.0035,  0.0066,  0.0035],\n",
      "        [-0.0069, -0.0027, -0.0031,  ..., -0.0104, -0.0057, -0.0046],\n",
      "        ...,\n",
      "        [ 0.0133,  0.0092,  0.0215,  ..., -0.0014,  0.0009,  0.0083],\n",
      "        [-0.0014,  0.0002, -0.0112,  ...,  0.0039,  0.0093, -0.0036],\n",
      "        [ 0.0021, -0.0068, -0.0117,  ...,  0.0123, -0.0088,  0.0080]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight tensor([[ 2.4665e-03,  3.3621e-04, -1.0777e-03,  ..., -1.9507e-03,\n",
      "         -3.5520e-03,  2.5888e-03],\n",
      "        [-2.6817e-04,  9.5788e-04, -7.7624e-06,  ..., -5.6729e-04,\n",
      "          5.6654e-04,  4.5978e-04],\n",
      "        [-2.4262e-05,  5.1120e-03,  7.9824e-04,  ..., -4.6771e-04,\n",
      "         -4.5663e-04,  2.4135e-04],\n",
      "        ...,\n",
      "        [ 1.3823e-03, -1.6051e-03,  3.3257e-03,  ...,  2.9263e-03,\n",
      "         -2.4952e-03, -8.8463e-04],\n",
      "        [-1.1169e-02, -1.2125e-02,  8.6233e-03,  ..., -1.0263e-03,\n",
      "         -5.3460e-03,  8.3521e-03],\n",
      "        [ 5.3152e-03,  6.7708e-04,  1.9843e-03,  ..., -3.8620e-03,\n",
      "          9.7722e-04,  1.3376e-03]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight tensor([[-0.0001,  0.0040,  0.0017,  ..., -0.0035,  0.0070,  0.0064],\n",
      "        [-0.0077,  0.0057, -0.0084,  ...,  0.0055,  0.0020, -0.0006],\n",
      "        [ 0.0030, -0.0037,  0.0019,  ..., -0.0124, -0.0018, -0.0026],\n",
      "        ...,\n",
      "        [-0.0028, -0.0018, -0.0054,  ..., -0.0013,  0.0023,  0.0001],\n",
      "        [ 0.0024, -0.0021,  0.0043,  ...,  0.0002,  0.0008, -0.0016],\n",
      "        [-0.0046, -0.0024, -0.0005,  ...,  0.0019,  0.0051, -0.0017]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight tensor([[-0.0022, -0.0070, -0.0055,  ..., -0.0045, -0.0063, -0.0034],\n",
      "        [ 0.0009,  0.0009,  0.0007,  ...,  0.0010,  0.0008,  0.0003],\n",
      "        [ 0.0039,  0.0054, -0.0015,  ..., -0.0015, -0.0049, -0.0018],\n",
      "        ...,\n",
      "        [ 0.0048, -0.0009,  0.0017,  ..., -0.0012,  0.0014,  0.0007],\n",
      "        [ 0.0002,  0.0046, -0.0147,  ..., -0.0123,  0.0167,  0.0208],\n",
      "        [-0.0062,  0.0023, -0.0017,  ...,  0.0047, -0.0040,  0.0037]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight tensor([[ 0.0157, -0.0071,  0.0029,  ..., -0.0046, -0.0029,  0.0119],\n",
      "        [-0.0113,  0.0017, -0.0027,  ..., -0.0142,  0.0010,  0.0018],\n",
      "        [-0.0110, -0.0098,  0.0027,  ..., -0.0061,  0.0003, -0.0095],\n",
      "        ...,\n",
      "        [ 0.0102, -0.0045, -0.0026,  ..., -0.0079,  0.0187, -0.0117],\n",
      "        [-0.0087, -0.0004, -0.0095,  ..., -0.0003,  0.0066, -0.0074],\n",
      "        [-0.0013, -0.0109,  0.0041,  ..., -0.0027, -0.0045,  0.0086]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight tensor([[ 3.7646e-03, -3.3897e-03,  8.7464e-03,  ..., -9.0259e-04,\n",
      "          3.3606e-03,  1.0764e-03],\n",
      "        [-2.4060e-04,  2.3030e-04, -7.5123e-05,  ..., -3.3447e-05,\n",
      "          1.2423e-03,  4.1298e-04],\n",
      "        [-2.3548e-04, -2.2081e-03,  9.2364e-04,  ..., -5.2644e-04,\n",
      "         -3.1072e-03, -2.1905e-03],\n",
      "        ...,\n",
      "        [-3.9386e-03, -1.8192e-04,  1.4915e-04,  ...,  2.4825e-03,\n",
      "          2.6929e-03, -7.1186e-03],\n",
      "        [ 1.4737e-03, -1.0733e-02, -1.2685e-02,  ..., -7.0245e-03,\n",
      "          3.2606e-03,  1.0003e-02],\n",
      "        [-2.1529e-06, -9.4140e-04,  3.5042e-03,  ...,  3.0517e-03,\n",
      "         -1.4880e-03, -1.7109e-03]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight tensor([[-0.0036, -0.0022, -0.0001,  ..., -0.0078, -0.0073,  0.0074],\n",
      "        [-0.0012,  0.0004, -0.0019,  ..., -0.0071,  0.0045,  0.0077],\n",
      "        [ 0.0072, -0.0031, -0.0044,  ..., -0.0004,  0.0036,  0.0008],\n",
      "        ...,\n",
      "        [ 0.0007, -0.0006, -0.0038,  ...,  0.0041, -0.0017, -0.0026],\n",
      "        [ 0.0009,  0.0018,  0.0024,  ..., -0.0064, -0.0025,  0.0003],\n",
      "        [-0.0002, -0.0008, -0.0016,  ...,  0.0015,  0.0005,  0.0015]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight tensor([[ 6.2799e-06, -9.0297e-03,  4.2497e-03,  ...,  4.5468e-03,\n",
      "          3.0703e-03,  4.4673e-04],\n",
      "        [ 1.7293e-03, -1.2058e-03,  6.9796e-04,  ..., -6.9573e-04,\n",
      "          1.4560e-03, -1.6388e-05],\n",
      "        [-2.2360e-03,  2.3516e-03,  4.7333e-04,  ...,  1.7098e-03,\n",
      "          2.6301e-03,  9.1139e-04],\n",
      "        ...,\n",
      "        [-1.4157e-03, -5.2617e-03, -4.6200e-03,  ..., -5.7471e-04,\n",
      "          1.0584e-03,  3.3056e-03],\n",
      "        [-2.6480e-03,  1.3729e-02,  1.9179e-02,  ...,  9.0681e-03,\n",
      "          6.0820e-03, -2.2492e-03],\n",
      "        [-1.8018e-04,  2.0231e-03,  3.2604e-03,  ..., -2.8724e-04,\n",
      "         -6.6307e-04, -8.4896e-04]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight tensor([[ 0.0014,  0.0101, -0.0100,  ..., -0.0123, -0.0049, -0.0041],\n",
      "        [ 0.0009,  0.0148,  0.0027,  ..., -0.0041,  0.0048,  0.0038],\n",
      "        [ 0.0028,  0.0091, -0.0064,  ...,  0.0006, -0.0038, -0.0034],\n",
      "        ...,\n",
      "        [-0.0020,  0.0015,  0.0005,  ..., -0.0097, -0.0009,  0.0048],\n",
      "        [-0.0029,  0.0137, -0.0021,  ..., -0.0089, -0.0083,  0.0067],\n",
      "        [-0.0032,  0.0039,  0.0106,  ..., -0.0039, -0.0069, -0.0081]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight tensor([[ 0.0020, -0.0035,  0.0011,  ...,  0.0011, -0.0004,  0.0021],\n",
      "        [ 0.0017, -0.0003, -0.0001,  ...,  0.0004, -0.0007, -0.0003],\n",
      "        [ 0.0054, -0.0054, -0.0013,  ...,  0.0022, -0.0009, -0.0045],\n",
      "        ...,\n",
      "        [-0.0013, -0.0044, -0.0051,  ..., -0.0021,  0.0007,  0.0009],\n",
      "        [-0.0007, -0.0039,  0.0009,  ..., -0.0058, -0.0080, -0.0078],\n",
      "        [-0.0027, -0.0033,  0.0024,  ...,  0.0032, -0.0055,  0.0021]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight tensor([[ 0.0056,  0.0012, -0.0002,  ...,  0.0027,  0.0004, -0.0061],\n",
      "        [-0.0026,  0.0020, -0.0049,  ..., -0.0056, -0.0029, -0.0013],\n",
      "        [-0.0008, -0.0014,  0.0040,  ...,  0.0041,  0.0022,  0.0012],\n",
      "        ...,\n",
      "        [ 0.0015,  0.0023, -0.0007,  ..., -0.0017,  0.0013, -0.0003],\n",
      "        [ 0.0048, -0.0008,  0.0021,  ..., -0.0013,  0.0083, -0.0019],\n",
      "        [-0.0037,  0.0008, -0.0018,  ...,  0.0023,  0.0031, -0.0003]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight tensor([[-2.0797e-04,  1.8533e-03, -7.9924e-05,  ...,  3.5030e-03,\n",
      "          3.0380e-03,  6.2872e-03],\n",
      "        [ 3.2199e-04, -4.0982e-04,  3.5684e-04,  ..., -2.6719e-04,\n",
      "          7.7626e-04,  1.1528e-03],\n",
      "        [-8.1396e-04,  1.5949e-03, -1.8567e-03,  ...,  6.0305e-04,\n",
      "         -5.1888e-04,  2.7244e-03],\n",
      "        ...,\n",
      "        [ 1.4720e-03,  2.8029e-03, -4.0773e-03,  ...,  1.0206e-03,\n",
      "          1.9902e-03,  2.5657e-05],\n",
      "        [-4.6349e-03,  9.0870e-03,  4.3918e-04,  ..., -2.1558e-03,\n",
      "         -5.1839e-03, -9.9071e-03],\n",
      "        [ 8.9583e-04, -3.2106e-03, -7.6026e-04,  ...,  2.5661e-04,\n",
      "         -1.7897e-04, -1.3427e-03]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight tensor([[-3.1835e-03, -1.9873e-03, -6.8200e-03,  ..., -5.6382e-03,\n",
      "          5.8335e-03,  2.6176e-03],\n",
      "        [-4.1883e-03, -2.3373e-03,  1.0247e-03,  ..., -3.1487e-03,\n",
      "         -5.6593e-03, -3.8639e-03],\n",
      "        [ 7.5016e-03,  6.1411e-03, -6.3029e-04,  ...,  7.3772e-03,\n",
      "          3.1075e-03,  1.0719e-02],\n",
      "        ...,\n",
      "        [-9.4156e-03,  6.7632e-05,  5.1466e-03,  ...,  3.9970e-03,\n",
      "          2.0367e-03,  8.8552e-03],\n",
      "        [ 9.0130e-04, -9.1540e-03,  6.0635e-03,  ..., -5.3985e-03,\n",
      "         -4.2026e-03, -6.7829e-03],\n",
      "        [ 4.0828e-03,  1.0522e-02, -3.7746e-03,  ...,  2.0360e-03,\n",
      "          2.0646e-03, -6.9458e-04]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight tensor([[-2.9238e-03,  8.9310e-03, -1.8580e-03,  ...,  3.1569e-03,\n",
      "         -6.6627e-03,  4.4792e-03],\n",
      "        [-7.0088e-04, -3.0863e-04,  1.2888e-03,  ...,  5.2649e-05,\n",
      "          5.3705e-04, -8.0591e-04],\n",
      "        [ 8.3760e-03,  1.8553e-03, -1.6956e-03,  ..., -4.0856e-03,\n",
      "         -3.6996e-03,  4.4069e-03],\n",
      "        ...,\n",
      "        [ 7.2577e-03,  8.5216e-03,  1.4256e-03,  ...,  1.3431e-03,\n",
      "         -4.3772e-03,  1.5009e-03],\n",
      "        [-1.2252e-02, -1.4889e-02,  3.3170e-02,  ..., -8.9993e-03,\n",
      "          3.3025e-02, -1.1653e-04],\n",
      "        [ 7.3667e-04, -4.0455e-03,  1.0662e-03,  ...,  1.7583e-03,\n",
      "         -1.8191e-03,  5.6921e-03]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight tensor([[-0.0030,  0.0078,  0.0005,  ..., -0.0059, -0.0030, -0.0088],\n",
      "        [ 0.0049, -0.0090,  0.0028,  ...,  0.0006, -0.0011,  0.0040],\n",
      "        [-0.0073, -0.0019,  0.0026,  ..., -0.0003,  0.0022,  0.0043],\n",
      "        ...,\n",
      "        [ 0.0060,  0.0028,  0.0063,  ...,  0.0070, -0.0019, -0.0033],\n",
      "        [ 0.0062,  0.0019,  0.0101,  ..., -0.0022,  0.0101,  0.0118],\n",
      "        [-0.0078,  0.0028,  0.0124,  ...,  0.0063,  0.0065, -0.0111]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight tensor([[-3.6368e-03, -7.2050e-03,  1.1225e-03,  ..., -4.5994e-03,\n",
      "         -2.3994e-03, -5.3134e-03],\n",
      "        [ 8.1830e-04,  1.1333e-03, -1.4654e-03,  ..., -4.7167e-04,\n",
      "          9.3637e-04,  2.0311e-03],\n",
      "        [ 2.3822e-03,  3.9137e-03,  4.6597e-03,  ..., -8.4591e-04,\n",
      "         -9.6128e-03, -3.8628e-03],\n",
      "        ...,\n",
      "        [-4.8214e-03,  4.2641e-03,  2.1931e-03,  ..., -3.6442e-03,\n",
      "          3.1951e-04, -8.3312e-04],\n",
      "        [ 1.5440e-02, -2.1516e-03, -1.2158e-02,  ...,  1.0354e-02,\n",
      "          7.4739e-03,  4.7253e-03],\n",
      "        [ 4.9251e-05, -2.8752e-03,  4.3114e-03,  ..., -2.6764e-03,\n",
      "          6.6293e-04,  1.6524e-03]])\n",
      "unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight tensor([[-9.0809e-03, -6.5111e-03,  5.5813e-03,  ...,  1.4200e-02,\n",
      "          2.1882e-02, -1.0686e-02],\n",
      "        [ 1.2105e-02,  1.4423e-02, -1.0041e-03,  ...,  8.3353e-03,\n",
      "         -1.0921e-02, -1.3556e-03],\n",
      "        [ 3.1014e-03, -2.1291e-03,  2.8370e-03,  ...,  1.2179e-03,\n",
      "         -1.1306e-03,  2.4332e-03],\n",
      "        ...,\n",
      "        [-1.3344e-02,  6.6672e-03, -6.3778e-03,  ...,  2.2256e-03,\n",
      "         -1.1604e-02, -3.3271e-03],\n",
      "        [ 1.3781e-02,  1.7001e-05,  3.2796e-03,  ..., -1.2216e-02,\n",
      "         -3.3124e-03, -7.2194e-03],\n",
      "        [ 1.0706e-03,  7.2957e-03, -2.4914e-03,  ...,  1.3710e-03,\n",
      "          8.7425e-03,  8.1316e-04]])\n",
      "unet.down_blocks.2.attentions.1.proj_in.lora.down.weight tensor([[[[-0.0083]],\n",
      "\n",
      "         [[-0.0028]],\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         [[ 0.0063]],\n",
      "\n",
      "         [[ 0.0028]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0026]],\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0008]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[-0.0016]]],\n",
      "\n",
      "\n",
      "        [[[-0.0070]],\n",
      "\n",
      "         [[-0.0039]],\n",
      "\n",
      "         [[ 0.0062]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0074]],\n",
      "\n",
      "         [[-0.0027]],\n",
      "\n",
      "         [[ 0.0037]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0086]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0039]],\n",
      "\n",
      "         [[ 0.0143]],\n",
      "\n",
      "         [[ 0.0030]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0122]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         [[-0.0122]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0093]],\n",
      "\n",
      "         [[ 0.0294]],\n",
      "\n",
      "         [[ 0.0113]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0020]],\n",
      "\n",
      "         [[-0.0003]],\n",
      "\n",
      "         [[-0.0038]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0030]],\n",
      "\n",
      "         [[ 0.0007]],\n",
      "\n",
      "         [[-0.0037]]]])\n",
      "unet.down_blocks.2.attentions.1.proj_in.lora.up.weight tensor([[[[-0.0046]],\n",
      "\n",
      "         [[-0.0089]],\n",
      "\n",
      "         [[ 0.0072]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0101]],\n",
      "\n",
      "         [[-0.0104]],\n",
      "\n",
      "         [[-0.0025]]],\n",
      "\n",
      "\n",
      "        [[[-0.0171]],\n",
      "\n",
      "         [[-0.0040]],\n",
      "\n",
      "         [[ 0.0041]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0051]],\n",
      "\n",
      "         [[-0.0127]],\n",
      "\n",
      "         [[ 0.0060]]],\n",
      "\n",
      "\n",
      "        [[[-0.0017]],\n",
      "\n",
      "         [[-0.0090]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[-0.0101]],\n",
      "\n",
      "         [[ 0.0012]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0101]],\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         [[-0.0053]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0079]],\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         [[-0.0039]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0134]],\n",
      "\n",
      "         [[-0.0089]],\n",
      "\n",
      "         [[ 0.0101]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0078]],\n",
      "\n",
      "         [[ 0.0039]],\n",
      "\n",
      "         [[ 0.0161]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0095]],\n",
      "\n",
      "         [[-0.0053]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0073]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[-0.0176]]]])\n",
      "unet.down_blocks.2.attentions.1.proj_out.lora.down.weight tensor([[[[-0.0033]],\n",
      "\n",
      "         [[ 0.0041]],\n",
      "\n",
      "         [[-0.0003]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[ 0.0050]],\n",
      "\n",
      "         [[ 0.0009]]],\n",
      "\n",
      "\n",
      "        [[[-0.0011]],\n",
      "\n",
      "         [[ 0.0012]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[ 0.0016]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0081]],\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         [[ 0.0033]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[ 0.0033]],\n",
      "\n",
      "         [[ 0.0007]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0004]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         [[-0.0055]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[-0.0065]],\n",
      "\n",
      "         [[ 0.0020]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0071]],\n",
      "\n",
      "         [[ 0.0008]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0086]],\n",
      "\n",
      "         [[ 0.0132]],\n",
      "\n",
      "         [[ 0.0225]]],\n",
      "\n",
      "\n",
      "        [[[-0.0073]],\n",
      "\n",
      "         [[ 0.0037]],\n",
      "\n",
      "         [[ 0.0042]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0070]],\n",
      "\n",
      "         [[ 0.0039]],\n",
      "\n",
      "         [[ 0.0073]]]])\n",
      "unet.down_blocks.2.attentions.1.proj_out.lora.up.weight tensor([[[[ 0.0039]],\n",
      "\n",
      "         [[ 0.0122]],\n",
      "\n",
      "         [[-0.0150]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0033]],\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         [[-0.0007]]],\n",
      "\n",
      "\n",
      "        [[[-0.0139]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[ 0.0079]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0076]],\n",
      "\n",
      "         [[-0.0082]],\n",
      "\n",
      "         [[ 0.0158]]],\n",
      "\n",
      "\n",
      "        [[[-0.0166]],\n",
      "\n",
      "         [[ 0.0113]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0148]],\n",
      "\n",
      "         [[ 0.0048]],\n",
      "\n",
      "         [[ 0.0036]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0073]],\n",
      "\n",
      "         [[ 0.0096]],\n",
      "\n",
      "         [[ 0.0041]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0066]],\n",
      "\n",
      "         [[-0.0038]],\n",
      "\n",
      "         [[-0.0057]]],\n",
      "\n",
      "\n",
      "        [[[-0.0006]],\n",
      "\n",
      "         [[ 0.0183]],\n",
      "\n",
      "         [[-0.0166]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0041]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[ 0.0122]]],\n",
      "\n",
      "\n",
      "        [[[-0.0128]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[-0.0065]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0128]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[-0.0060]]]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight tensor([[ 1.4272e-03, -2.0288e-03, -1.0198e-03,  ...,  1.0363e-03,\n",
      "         -3.8319e-03,  9.4003e-03],\n",
      "        [ 1.7148e-03,  4.8020e-04,  7.2804e-04,  ...,  1.0776e-03,\n",
      "          1.1222e-03, -4.2469e-04],\n",
      "        [ 1.6354e-03, -5.6326e-04, -2.8678e-03,  ...,  2.6654e-03,\n",
      "         -4.0315e-03,  2.7656e-04],\n",
      "        ...,\n",
      "        [-4.6859e-03, -2.0645e-03, -3.9129e-03,  ...,  3.0346e-03,\n",
      "         -2.7598e-03, -7.2031e-04],\n",
      "        [-8.5619e-03, -6.2271e-03,  8.5957e-03,  ..., -2.3804e-03,\n",
      "          8.5897e-03, -6.5870e-03],\n",
      "        [ 3.0801e-05, -1.9233e-03, -1.5579e-03,  ...,  2.6408e-03,\n",
      "          2.5246e-03, -1.2656e-03]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight tensor([[-0.0067, -0.0025, -0.0005,  ...,  0.0024, -0.0089,  0.0001],\n",
      "        [ 0.0020, -0.0039,  0.0036,  ...,  0.0058, -0.0029,  0.0016],\n",
      "        [-0.0033, -0.0049,  0.0017,  ..., -0.0044, -0.0004,  0.0052],\n",
      "        ...,\n",
      "        [-0.0049,  0.0030, -0.0009,  ..., -0.0009,  0.0101,  0.0005],\n",
      "        [-0.0006, -0.0037, -0.0039,  ..., -0.0028,  0.0017,  0.0051],\n",
      "        [-0.0062, -0.0038, -0.0010,  ..., -0.0091,  0.0086, -0.0046]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight tensor([[ 3.7374e-03,  6.3183e-03,  4.8273e-03,  ..., -4.4430e-04,\n",
      "         -4.5182e-03,  8.5513e-03],\n",
      "        [-1.8757e-04,  4.9957e-04,  3.6411e-04,  ..., -6.9513e-04,\n",
      "         -5.6411e-04,  1.6365e-05],\n",
      "        [ 6.8390e-03, -6.0813e-03,  1.0072e-03,  ..., -4.8482e-03,\n",
      "          5.2182e-03,  1.5819e-03],\n",
      "        ...,\n",
      "        [ 2.1165e-04, -3.6314e-03,  5.0588e-03,  ..., -5.1395e-03,\n",
      "          2.8297e-04, -2.2317e-03],\n",
      "        [-9.5528e-04,  9.1975e-03, -1.3936e-02,  ...,  1.4012e-02,\n",
      "          4.9441e-03, -9.5688e-03],\n",
      "        [ 5.5472e-03, -3.7389e-04,  2.6004e-03,  ...,  3.8900e-03,\n",
      "         -6.1978e-04, -3.3171e-03]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight tensor([[-0.0036, -0.0163,  0.0053,  ..., -0.0038, -0.0002, -0.0050],\n",
      "        [-0.0059, -0.0048,  0.0110,  ...,  0.0064,  0.0013, -0.0137],\n",
      "        [-0.0035, -0.0051,  0.0049,  ..., -0.0049, -0.0055, -0.0126],\n",
      "        ...,\n",
      "        [-0.0071, -0.0096,  0.0014,  ..., -0.0128, -0.0212,  0.0129],\n",
      "        [ 0.0043,  0.0086,  0.0015,  ..., -0.0188, -0.0036,  0.0169],\n",
      "        [ 0.0017, -0.0002, -0.0061,  ..., -0.0150, -0.0054,  0.0032]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight tensor([[ 1.7668e-03, -2.9552e-03, -2.5091e-04,  ..., -3.3616e-03,\n",
      "         -3.5087e-03, -3.2697e-04],\n",
      "        [-1.8497e-04,  2.5563e-05,  3.5531e-04,  ..., -5.1973e-04,\n",
      "         -1.1369e-03, -1.1933e-03],\n",
      "        [-6.1025e-03,  3.0665e-03, -1.5360e-04,  ..., -2.9192e-03,\n",
      "         -5.8288e-04,  5.1110e-03],\n",
      "        ...,\n",
      "        [ 7.8045e-04, -3.1689e-04, -2.6105e-04,  ...,  1.0102e-03,\n",
      "         -3.4488e-03, -2.9935e-03],\n",
      "        [ 5.0552e-03,  1.7286e-02, -6.4300e-04,  ...,  7.2885e-04,\n",
      "          3.3037e-03,  3.9256e-03],\n",
      "        [-1.7708e-03,  5.0341e-04,  3.5896e-04,  ...,  5.5174e-04,\n",
      "          2.6144e-03,  6.2196e-03]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight tensor([[-3.1274e-03,  2.8085e-03,  1.6547e-04,  ...,  1.3020e-03,\n",
      "          1.7317e-03,  5.0873e-03],\n",
      "        [ 3.2448e-03,  1.1558e-03,  3.5953e-03,  ..., -1.2909e-03,\n",
      "          6.8744e-03, -4.6539e-05],\n",
      "        [ 4.7714e-03, -2.0803e-03,  3.4980e-03,  ...,  6.0124e-03,\n",
      "         -3.5742e-03,  2.1516e-03],\n",
      "        ...,\n",
      "        [-2.8466e-05, -3.5691e-03, -4.5503e-03,  ...,  3.7107e-03,\n",
      "          6.1915e-03,  4.0785e-04],\n",
      "        [-7.3194e-03, -2.5904e-03, -1.0241e-03,  ...,  1.6339e-03,\n",
      "          7.8106e-03, -4.1369e-03],\n",
      "        [ 7.0447e-03,  1.5947e-03, -4.4243e-03,  ..., -5.6199e-04,\n",
      "         -1.2052e-02, -1.5425e-03]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight tensor([[-0.0115, -0.0061, -0.0042,  ..., -0.0031,  0.0032,  0.0013],\n",
      "        [-0.0002,  0.0015, -0.0007,  ..., -0.0006,  0.0006, -0.0015],\n",
      "        [-0.0024, -0.0019,  0.0010,  ...,  0.0056, -0.0025, -0.0042],\n",
      "        ...,\n",
      "        [ 0.0016, -0.0040,  0.0030,  ..., -0.0020, -0.0005, -0.0001],\n",
      "        [ 0.0035, -0.0022,  0.0026,  ...,  0.0114,  0.0288,  0.0079],\n",
      "        [ 0.0038,  0.0027,  0.0037,  ..., -0.0032,  0.0037,  0.0014]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight tensor([[ 0.0078, -0.0093, -0.0015,  ..., -0.0033,  0.0037, -0.0034],\n",
      "        [-0.0023,  0.0110, -0.0158,  ...,  0.0061,  0.0015, -0.0061],\n",
      "        [-0.0033, -0.0047,  0.0069,  ..., -0.0067,  0.0011, -0.0034],\n",
      "        ...,\n",
      "        [ 0.0112, -0.0002, -0.0031,  ...,  0.0138, -0.0150, -0.0039],\n",
      "        [ 0.0009,  0.0045, -0.0084,  ...,  0.0040,  0.0149,  0.0016],\n",
      "        [ 0.0109,  0.0050, -0.0078,  ...,  0.0001, -0.0028, -0.0015]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight tensor([[ 2.7646e-03, -1.2083e-03,  2.4981e-03,  ..., -2.9926e-03,\n",
      "         -8.8066e-04,  6.2547e-03],\n",
      "        [ 7.3624e-04, -8.6868e-05, -6.5494e-04,  ..., -1.0907e-03,\n",
      "         -8.1232e-04,  1.8221e-03],\n",
      "        [-1.9247e-03, -4.1854e-03, -6.2222e-04,  ..., -6.5197e-03,\n",
      "          1.0600e-03,  4.0999e-03],\n",
      "        ...,\n",
      "        [ 2.4033e-03,  5.1283e-03,  2.9173e-04,  ...,  2.0032e-03,\n",
      "         -4.8267e-04,  2.6031e-03],\n",
      "        [ 1.2492e-03,  5.5447e-03, -4.0203e-03,  ..., -3.6042e-03,\n",
      "          4.8652e-03, -9.6316e-03],\n",
      "        [ 2.0822e-03, -3.4576e-03, -1.6819e-03,  ..., -1.7572e-03,\n",
      "          4.1211e-03,  6.1994e-04]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight tensor([[ 0.0164,  0.0014,  0.0175,  ...,  0.0075, -0.0013,  0.0014],\n",
      "        [ 0.0058, -0.0048, -0.0131,  ..., -0.0104, -0.0015,  0.0038],\n",
      "        [-0.0070,  0.0042, -0.0040,  ...,  0.0054, -0.0025, -0.0101],\n",
      "        ...,\n",
      "        [-0.0019, -0.0088,  0.0078,  ..., -0.0043, -0.0047, -0.0030],\n",
      "        [-0.0025,  0.0042,  0.0058,  ...,  0.0008,  0.0042,  0.0049],\n",
      "        [ 0.0057,  0.0130,  0.0093,  ..., -0.0019,  0.0085,  0.0059]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight tensor([[-0.0040,  0.0073, -0.0067,  ...,  0.0009,  0.0054,  0.0008],\n",
      "        [-0.0008, -0.0021,  0.0007,  ..., -0.0012, -0.0004,  0.0005],\n",
      "        [-0.0003,  0.0020,  0.0014,  ..., -0.0009,  0.0004, -0.0053],\n",
      "        ...,\n",
      "        [ 0.0025,  0.0053, -0.0037,  ...,  0.0039, -0.0004,  0.0012],\n",
      "        [-0.0005,  0.0084, -0.0001,  ...,  0.0161, -0.0058,  0.0032],\n",
      "        [ 0.0044,  0.0013, -0.0042,  ...,  0.0030,  0.0038,  0.0005]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight tensor([[-0.0178, -0.0008, -0.0015,  ..., -0.0072, -0.0001,  0.0003],\n",
      "        [-0.0075, -0.0058,  0.0060,  ...,  0.0028,  0.0003, -0.0117],\n",
      "        [-0.0061,  0.0006, -0.0046,  ...,  0.0083,  0.0061, -0.0050],\n",
      "        ...,\n",
      "        [-0.0054, -0.0040, -0.0130,  ..., -0.0114, -0.0015, -0.0040],\n",
      "        [-0.0022, -0.0049, -0.0003,  ..., -0.0018,  0.0111, -0.0024],\n",
      "        [-0.0018,  0.0027, -0.0107,  ..., -0.0050, -0.0015,  0.0011]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight tensor([[-4.2416e-03, -3.4652e-04,  4.2173e-03,  ..., -7.0763e-04,\n",
      "         -3.9149e-03,  9.7153e-04],\n",
      "        [-1.5906e-04, -1.0918e-03,  8.8418e-04,  ..., -4.8821e-04,\n",
      "          1.1574e-03,  1.3918e-05],\n",
      "        [ 1.1963e-04,  3.0039e-03,  4.1785e-03,  ..., -1.9310e-03,\n",
      "         -2.9645e-03, -7.5475e-03],\n",
      "        ...,\n",
      "        [-2.7486e-03,  2.4192e-03,  3.0342e-03,  ..., -1.0654e-03,\n",
      "          6.3445e-04,  5.7127e-04],\n",
      "        [-5.4296e-03,  6.2268e-03,  3.7591e-03,  ..., -1.1476e-03,\n",
      "         -3.9752e-03, -3.3121e-04],\n",
      "        [-1.4721e-03, -2.7231e-03,  1.1675e-03,  ..., -4.7086e-04,\n",
      "          7.2222e-04, -1.2394e-03]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight tensor([[ 0.0033,  0.0002,  0.0026,  ..., -0.0048,  0.0017,  0.0006],\n",
      "        [ 0.0130, -0.0053, -0.0055,  ...,  0.0038,  0.0047, -0.0030],\n",
      "        [ 0.0041,  0.0030, -0.0062,  ..., -0.0108, -0.0013,  0.0025],\n",
      "        ...,\n",
      "        [ 0.0088, -0.0017,  0.0047,  ..., -0.0017,  0.0072,  0.0010],\n",
      "        [-0.0058,  0.0026,  0.0005,  ...,  0.0012,  0.0050,  0.0026],\n",
      "        [ 0.0044,  0.0008, -0.0030,  ...,  0.0030, -0.0014, -0.0023]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight tensor([[-3.8227e-03,  2.2821e-03,  2.2709e-03,  ...,  4.6383e-03,\n",
      "          2.5941e-03, -5.3796e-03],\n",
      "        [ 7.9307e-04,  9.9895e-04, -1.0657e-03,  ...,  5.7534e-04,\n",
      "          1.2190e-05,  8.0244e-04],\n",
      "        [-8.9719e-03, -7.1853e-03, -5.3359e-03,  ...,  1.1151e-03,\n",
      "         -5.3193e-04, -1.4215e-03],\n",
      "        ...,\n",
      "        [ 1.3987e-03,  9.2300e-04,  6.3327e-03,  ..., -3.3908e-04,\n",
      "          2.7427e-03, -2.1275e-03],\n",
      "        [-6.8768e-03, -2.0380e-03,  3.7029e-03,  ..., -5.8457e-03,\n",
      "          1.6857e-03, -9.0373e-03],\n",
      "        [-1.5456e-03,  1.0641e-03,  2.8413e-03,  ...,  9.3813e-04,\n",
      "          1.6288e-03, -1.6553e-03]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight tensor([[ 0.0031, -0.0043,  0.0072,  ...,  0.0041,  0.0026, -0.0034],\n",
      "        [-0.0010, -0.0201,  0.0162,  ..., -0.0166,  0.0036,  0.0023],\n",
      "        [ 0.0068,  0.0047,  0.0141,  ...,  0.0098, -0.0080, -0.0073],\n",
      "        ...,\n",
      "        [-0.0019,  0.0022,  0.0050,  ..., -0.0077, -0.0044, -0.0038],\n",
      "        [ 0.0067,  0.0141, -0.0102,  ...,  0.0108,  0.0030,  0.0045],\n",
      "        [-0.0038, -0.0063,  0.0013,  ..., -0.0045,  0.0085, -0.0046]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight tensor([[-3.6221e-03,  2.2264e-04, -6.0743e-03,  ..., -5.6279e-03,\n",
      "         -9.2104e-05, -4.7521e-04],\n",
      "        [-3.7100e-04, -1.4217e-03, -1.0055e-03,  ...,  7.6748e-04,\n",
      "         -5.1363e-04, -1.3965e-03],\n",
      "        [ 5.7119e-03,  5.0228e-04,  1.7574e-03,  ..., -2.4684e-03,\n",
      "          1.1618e-03, -6.0418e-03],\n",
      "        ...,\n",
      "        [ 1.2207e-04,  4.0302e-03, -4.0173e-04,  ..., -3.6552e-04,\n",
      "         -3.6549e-03, -5.2217e-04],\n",
      "        [-2.6502e-03, -7.9416e-03, -5.7271e-03,  ..., -4.4121e-04,\n",
      "         -2.5656e-02,  1.3799e-03],\n",
      "        [-4.5534e-03, -2.5944e-03, -5.3171e-03,  ...,  2.4721e-04,\n",
      "         -3.9671e-03,  4.0551e-03]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight tensor([[-0.0087, -0.0011, -0.0141,  ...,  0.0013,  0.0051,  0.0052],\n",
      "        [ 0.0063, -0.0007, -0.0027,  ...,  0.0029, -0.0037,  0.0084],\n",
      "        [ 0.0072,  0.0012, -0.0037,  ...,  0.0106, -0.0037,  0.0016],\n",
      "        ...,\n",
      "        [ 0.0015, -0.0001,  0.0018,  ...,  0.0011,  0.0093,  0.0032],\n",
      "        [-0.0071,  0.0066, -0.0052,  ...,  0.0040, -0.0058, -0.0011],\n",
      "        [-0.0091, -0.0021, -0.0041,  ...,  0.0118, -0.0072, -0.0006]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight tensor([[-8.5121e-04,  3.1834e-03,  1.3502e-02,  ..., -1.6382e-03,\n",
      "          1.8044e-03, -3.6793e-03],\n",
      "        [-3.6259e-04,  2.6040e-04, -3.2160e-04,  ...,  6.7077e-04,\n",
      "         -1.4445e-03, -2.0900e-04],\n",
      "        [-1.4459e-03, -8.6691e-04,  1.4982e-03,  ...,  6.8788e-04,\n",
      "          4.0605e-04, -2.4130e-03],\n",
      "        ...,\n",
      "        [ 5.2300e-05, -5.2856e-04, -1.3776e-03,  ...,  2.6154e-03,\n",
      "          2.8448e-03,  5.3284e-03],\n",
      "        [-9.6506e-03, -1.2143e-02,  3.1734e-03,  ...,  1.2507e-02,\n",
      "          1.0488e-02,  2.1429e-02],\n",
      "        [-9.6524e-04, -3.9419e-03, -2.0848e-03,  ...,  3.2191e-03,\n",
      "         -7.9428e-04, -1.1536e-03]])\n",
      "unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight tensor([[-0.0149,  0.0005,  0.0099,  ...,  0.0081, -0.0019, -0.0060],\n",
      "        [ 0.0073, -0.0032, -0.0148,  ...,  0.0022, -0.0021,  0.0026],\n",
      "        [-0.0070,  0.0100,  0.0068,  ...,  0.0121, -0.0008,  0.0166],\n",
      "        ...,\n",
      "        [ 0.0020,  0.0057,  0.0131,  ..., -0.0019, -0.0075, -0.0076],\n",
      "        [-0.0043, -0.0175,  0.0044,  ...,  0.0005,  0.0032,  0.0012],\n",
      "        [-0.0047,  0.0002, -0.0029,  ..., -0.0040, -0.0003,  0.0016]])\n",
      "unet.mid_block.attentions.0.proj_in.lora.down.weight tensor([[[[-0.0038]],\n",
      "\n",
      "         [[-0.0008]],\n",
      "\n",
      "         [[ 0.0069]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0081]],\n",
      "\n",
      "         [[ 0.0065]],\n",
      "\n",
      "         [[-0.0001]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0007]],\n",
      "\n",
      "         [[-0.0008]],\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0002]],\n",
      "\n",
      "         [[ 0.0001]],\n",
      "\n",
      "         [[ 0.0004]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0028]],\n",
      "\n",
      "         [[ 0.0046]],\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0030]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[ 0.0012]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0007]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[ 0.0024]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0033]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[ 0.0010]]],\n",
      "\n",
      "\n",
      "        [[[-0.0068]],\n",
      "\n",
      "         [[ 0.0198]],\n",
      "\n",
      "         [[ 0.0205]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0028]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[-0.0131]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0062]],\n",
      "\n",
      "         [[ 0.0049]],\n",
      "\n",
      "         [[-0.0091]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0035]],\n",
      "\n",
      "         [[ 0.0024]],\n",
      "\n",
      "         [[ 0.0018]]]])\n",
      "unet.mid_block.attentions.0.proj_in.lora.up.weight tensor([[[[-8.9024e-03]],\n",
      "\n",
      "         [[ 6.6872e-03]],\n",
      "\n",
      "         [[ 1.0494e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8775e-03]],\n",
      "\n",
      "         [[ 3.1316e-03]],\n",
      "\n",
      "         [[ 1.3683e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1135e-02]],\n",
      "\n",
      "         [[ 1.7600e-05]],\n",
      "\n",
      "         [[-2.6806e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.8389e-03]],\n",
      "\n",
      "         [[ 4.0512e-03]],\n",
      "\n",
      "         [[ 7.7188e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4264e-03]],\n",
      "\n",
      "         [[ 3.2815e-03]],\n",
      "\n",
      "         [[-7.9349e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.1095e-03]],\n",
      "\n",
      "         [[ 9.6264e-03]],\n",
      "\n",
      "         [[ 4.5751e-05]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 4.6238e-03]],\n",
      "\n",
      "         [[ 1.0587e-03]],\n",
      "\n",
      "         [[-2.4564e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.0294e-03]],\n",
      "\n",
      "         [[ 3.9675e-03]],\n",
      "\n",
      "         [[-7.1612e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.5863e-03]],\n",
      "\n",
      "         [[-1.7447e-02]],\n",
      "\n",
      "         [[ 1.3690e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.5607e-03]],\n",
      "\n",
      "         [[ 2.6685e-03]],\n",
      "\n",
      "         [[ 9.0411e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 5.1457e-03]],\n",
      "\n",
      "         [[-5.2097e-04]],\n",
      "\n",
      "         [[-1.0486e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.6321e-03]],\n",
      "\n",
      "         [[ 1.9234e-03]],\n",
      "\n",
      "         [[ 1.6641e-03]]]])\n",
      "unet.mid_block.attentions.0.proj_out.lora.down.weight tensor([[[[ 0.0022]],\n",
      "\n",
      "         [[-0.0034]],\n",
      "\n",
      "         [[-0.0124]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0101]],\n",
      "\n",
      "         [[-0.0071]],\n",
      "\n",
      "         [[-0.0043]]],\n",
      "\n",
      "\n",
      "        [[[-0.0002]],\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0005]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[-0.0008]]],\n",
      "\n",
      "\n",
      "        [[[-0.0035]],\n",
      "\n",
      "         [[ 0.0088]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0055]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         [[ 0.0070]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0062]],\n",
      "\n",
      "         [[-0.0030]],\n",
      "\n",
      "         [[-0.0030]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0005]],\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         [[-0.0090]]],\n",
      "\n",
      "\n",
      "        [[[-0.0204]],\n",
      "\n",
      "         [[ 0.0271]],\n",
      "\n",
      "         [[ 0.0154]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[ 0.0077]],\n",
      "\n",
      "         [[ 0.0032]]],\n",
      "\n",
      "\n",
      "        [[[-0.0025]],\n",
      "\n",
      "         [[-0.0034]],\n",
      "\n",
      "         [[-0.0003]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0022]],\n",
      "\n",
      "         [[-0.0001]],\n",
      "\n",
      "         [[ 0.0008]]]])\n",
      "unet.mid_block.attentions.0.proj_out.lora.up.weight tensor([[[[ 0.0011]],\n",
      "\n",
      "         [[-0.0063]],\n",
      "\n",
      "         [[-0.0110]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0034]],\n",
      "\n",
      "         [[-0.0082]],\n",
      "\n",
      "         [[-0.0014]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0072]],\n",
      "\n",
      "         [[-0.0027]],\n",
      "\n",
      "         [[-0.0075]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0233]],\n",
      "\n",
      "         [[ 0.0048]],\n",
      "\n",
      "         [[ 0.0048]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0047]],\n",
      "\n",
      "         [[-0.0036]],\n",
      "\n",
      "         [[ 0.0103]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0088]],\n",
      "\n",
      "         [[-0.0052]],\n",
      "\n",
      "         [[-0.0004]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0035]],\n",
      "\n",
      "         [[ 0.0105]],\n",
      "\n",
      "         [[-0.0056]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0059]],\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[ 0.0076]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0103]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         [[ 0.0158]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[ 0.0141]],\n",
      "\n",
      "         [[-0.0049]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0147]],\n",
      "\n",
      "         [[-0.0035]],\n",
      "\n",
      "         [[-0.0161]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0037]],\n",
      "\n",
      "         [[-0.0093]],\n",
      "\n",
      "         [[ 0.0010]]]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight tensor([[-2.6042e-03, -2.6714e-03,  1.4218e-03,  ..., -1.0646e-03,\n",
      "          8.6120e-05,  1.1453e-03],\n",
      "        [ 7.2563e-04, -2.0928e-04,  6.9817e-04,  ...,  9.5095e-04,\n",
      "         -5.5411e-04, -2.9947e-04],\n",
      "        [-5.0585e-04,  2.0813e-03, -2.2397e-03,  ..., -3.0839e-04,\n",
      "         -3.5033e-04, -1.4481e-03],\n",
      "        ...,\n",
      "        [ 7.5926e-05, -2.5151e-03,  1.9958e-03,  ...,  1.7353e-03,\n",
      "         -4.3084e-04,  4.3051e-04],\n",
      "        [ 2.7018e-03,  7.7719e-03, -1.0388e-03,  ...,  3.2904e-03,\n",
      "         -3.6962e-03, -1.1327e-02],\n",
      "        [-5.2399e-04,  1.9165e-03,  1.0602e-03,  ..., -4.7936e-04,\n",
      "         -1.7838e-04,  1.9572e-03]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight tensor([[-6.1044e-04,  1.0133e-04, -1.1609e-03,  ..., -2.8624e-03,\n",
      "         -8.9254e-03, -7.9277e-04],\n",
      "        [-2.4698e-04, -2.5584e-04,  1.6380e-05,  ..., -1.1304e-03,\n",
      "          2.1681e-03, -6.2376e-04],\n",
      "        [-1.6036e-03,  1.2991e-03,  3.8069e-04,  ...,  2.2946e-03,\n",
      "          1.9945e-03,  6.7680e-04],\n",
      "        ...,\n",
      "        [-2.7438e-05, -1.1208e-03, -2.4730e-04,  ...,  2.6134e-03,\n",
      "          2.7318e-03, -2.2346e-03],\n",
      "        [ 2.1585e-03,  1.8580e-04, -1.6350e-03,  ..., -3.4895e-04,\n",
      "          5.7024e-03, -3.5744e-04],\n",
      "        [ 9.1397e-05,  1.2437e-03,  6.7249e-05,  ..., -1.4965e-03,\n",
      "         -7.9762e-04, -1.8115e-04]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight tensor([[ 0.0024, -0.0038, -0.0009,  ..., -0.0004,  0.0042, -0.0009],\n",
      "        [-0.0014, -0.0002,  0.0002,  ..., -0.0006,  0.0009,  0.0004],\n",
      "        [-0.0084, -0.0017,  0.0028,  ...,  0.0015,  0.0056,  0.0013],\n",
      "        ...,\n",
      "        [ 0.0023,  0.0068, -0.0035,  ..., -0.0015,  0.0008, -0.0016],\n",
      "        [-0.0016,  0.0063,  0.0055,  ..., -0.0003,  0.0020,  0.0053],\n",
      "        [-0.0045,  0.0052,  0.0007,  ..., -0.0029, -0.0063,  0.0011]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight tensor([[ 0.0058, -0.0114,  0.0046,  ..., -0.0038,  0.0006, -0.0071],\n",
      "        [-0.0032,  0.0105, -0.0029,  ...,  0.0053, -0.0062, -0.0028],\n",
      "        [-0.0027,  0.0047,  0.0005,  ...,  0.0031, -0.0030,  0.0038],\n",
      "        ...,\n",
      "        [ 0.0135, -0.0030, -0.0090,  ..., -0.0020,  0.0035, -0.0026],\n",
      "        [ 0.0003, -0.0001,  0.0010,  ...,  0.0145,  0.0010, -0.0045],\n",
      "        [ 0.0009, -0.0006, -0.0052,  ...,  0.0039,  0.0093, -0.0067]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight tensor([[-8.1125e-04,  1.8449e-03, -1.7246e-03,  ...,  2.1879e-03,\n",
      "         -4.7122e-03,  1.5884e-03],\n",
      "        [-2.9006e-05,  1.7622e-04,  5.0464e-04,  ..., -1.6422e-04,\n",
      "          5.9946e-04, -1.2658e-05],\n",
      "        [ 2.8515e-03, -1.1353e-04, -2.9056e-03,  ..., -6.7209e-04,\n",
      "          1.2512e-03, -2.2175e-03],\n",
      "        ...,\n",
      "        [-2.7740e-03,  9.0534e-04, -1.1614e-03,  ...,  2.9170e-03,\n",
      "         -2.9518e-04,  9.9488e-04],\n",
      "        [ 3.3430e-03, -1.1625e-03,  1.5860e-02,  ..., -9.5475e-03,\n",
      "          1.0120e-02,  7.9221e-03],\n",
      "        [ 1.5306e-04, -2.1981e-04, -5.4299e-04,  ..., -9.9218e-06,\n",
      "          4.9855e-05,  1.0397e-03]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight tensor([[-1.3929e-03,  2.5185e-04, -2.0897e-03,  ..., -1.3215e-03,\n",
      "          4.2353e-04, -8.6409e-04],\n",
      "        [-1.9621e-03,  2.1950e-03, -2.9684e-03,  ..., -3.8155e-04,\n",
      "         -3.6809e-04,  5.9325e-05],\n",
      "        [-6.9395e-04,  1.7823e-03,  1.7594e-03,  ...,  4.0193e-04,\n",
      "         -4.8606e-03,  1.7018e-03],\n",
      "        ...,\n",
      "        [ 1.2352e-03,  8.0010e-05, -2.7482e-04,  ..., -5.1006e-04,\n",
      "         -4.2161e-04, -4.8789e-04],\n",
      "        [ 2.5081e-03,  1.8402e-03, -1.3878e-03,  ..., -1.4500e-03,\n",
      "          2.3844e-03,  1.4453e-03],\n",
      "        [ 1.7856e-03, -1.0070e-04,  3.3630e-03,  ...,  1.2623e-03,\n",
      "          3.6883e-03,  1.7940e-04]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight tensor([[ 0.0040,  0.0021, -0.0047,  ..., -0.0090,  0.0055, -0.0038],\n",
      "        [ 0.0013,  0.0010, -0.0004,  ..., -0.0003,  0.0005, -0.0001],\n",
      "        [-0.0032, -0.0030,  0.0009,  ..., -0.0050,  0.0014, -0.0036],\n",
      "        ...,\n",
      "        [ 0.0031,  0.0035,  0.0046,  ..., -0.0004, -0.0017,  0.0002],\n",
      "        [-0.0063, -0.0142, -0.0133,  ...,  0.0182,  0.0025,  0.0001],\n",
      "        [-0.0025,  0.0002, -0.0038,  ...,  0.0016, -0.0012,  0.0018]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight tensor([[-0.0049, -0.0018,  0.0090,  ..., -0.0005, -0.0045,  0.0035],\n",
      "        [-0.0010,  0.0011, -0.0060,  ..., -0.0064, -0.0018,  0.0013],\n",
      "        [ 0.0006,  0.0022, -0.0051,  ..., -0.0018, -0.0054,  0.0026],\n",
      "        ...,\n",
      "        [ 0.0008, -0.0034, -0.0059,  ..., -0.0048, -0.0063,  0.0065],\n",
      "        [-0.0004,  0.0063,  0.0032,  ..., -0.0105,  0.0043,  0.0115],\n",
      "        [-0.0038,  0.0083,  0.0118,  ...,  0.0039,  0.0053, -0.0021]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight tensor([[-0.0012, -0.0034, -0.0019,  ...,  0.0007,  0.0043,  0.0010],\n",
      "        [ 0.0005, -0.0007,  0.0007,  ..., -0.0004, -0.0005, -0.0005],\n",
      "        [-0.0050,  0.0007, -0.0004,  ...,  0.0002,  0.0031,  0.0004],\n",
      "        ...,\n",
      "        [ 0.0002, -0.0038, -0.0014,  ...,  0.0019, -0.0041, -0.0041],\n",
      "        [-0.0025,  0.0047, -0.0006,  ..., -0.0221, -0.0047,  0.0022],\n",
      "        [ 0.0024, -0.0019,  0.0035,  ..., -0.0022, -0.0017, -0.0013]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight tensor([[ 6.0931e-03,  7.4842e-04,  5.7825e-03,  ..., -2.2145e-03,\n",
      "         -7.2144e-03, -5.2707e-03],\n",
      "        [-1.5265e-03,  3.9843e-03,  4.7792e-03,  ...,  5.3051e-03,\n",
      "          1.8440e-03, -3.1283e-03],\n",
      "        [ 2.8721e-05, -3.7147e-04, -3.3152e-03,  ..., -8.0260e-03,\n",
      "         -1.6502e-03,  4.1924e-03],\n",
      "        ...,\n",
      "        [-3.9359e-03, -2.4793e-03, -3.1997e-03,  ..., -5.7245e-04,\n",
      "          2.1263e-03,  1.3856e-03],\n",
      "        [-7.3962e-03, -2.2483e-03,  6.1942e-03,  ...,  3.4271e-03,\n",
      "         -4.6234e-03, -1.3703e-03],\n",
      "        [ 5.1001e-03, -1.1903e-03,  5.1867e-03,  ...,  2.5306e-03,\n",
      "         -2.8806e-03,  2.0476e-03]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight tensor([[-0.0007,  0.0010, -0.0021,  ...,  0.0018, -0.0009,  0.0003],\n",
      "        [-0.0015,  0.0011, -0.0002,  ...,  0.0006,  0.0012,  0.0003],\n",
      "        [ 0.0035, -0.0032,  0.0068,  ..., -0.0015,  0.0090,  0.0008],\n",
      "        ...,\n",
      "        [ 0.0010, -0.0071, -0.0004,  ..., -0.0028,  0.0043, -0.0028],\n",
      "        [-0.0169, -0.0113, -0.0188,  ...,  0.0041,  0.0039, -0.0038],\n",
      "        [ 0.0069,  0.0008, -0.0024,  ...,  0.0041, -0.0015, -0.0064]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight tensor([[-8.0207e-03,  8.1669e-03, -4.5865e-03,  ...,  3.2856e-03,\n",
      "          8.3568e-05, -5.0045e-03],\n",
      "        [-3.4536e-03,  1.7035e-03,  8.6462e-03,  ...,  5.3860e-03,\n",
      "          1.0488e-02, -3.5370e-03],\n",
      "        [-3.6435e-04, -5.3456e-04,  8.2383e-03,  ...,  4.6588e-03,\n",
      "          5.4754e-03,  5.0106e-03],\n",
      "        ...,\n",
      "        [-5.1346e-05,  1.3723e-03, -1.8591e-03,  ...,  1.4558e-03,\n",
      "         -1.0607e-03, -3.4297e-03],\n",
      "        [ 3.1557e-03,  1.6444e-02, -1.6590e-02,  ...,  2.4075e-03,\n",
      "          5.4311e-03,  1.8708e-03],\n",
      "        [-4.9509e-03, -1.6695e-02,  7.4616e-04,  ...,  2.3942e-03,\n",
      "         -5.3638e-04, -7.9061e-03]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight tensor([[ 0.0014, -0.0004, -0.0006,  ...,  0.0022, -0.0017, -0.0030],\n",
      "        [ 0.0003, -0.0005,  0.0004,  ...,  0.0001, -0.0007,  0.0002],\n",
      "        [ 0.0021, -0.0018,  0.0019,  ...,  0.0029, -0.0009, -0.0001],\n",
      "        ...,\n",
      "        [-0.0007,  0.0011, -0.0014,  ..., -0.0014, -0.0033, -0.0003],\n",
      "        [ 0.0058, -0.0127,  0.0058,  ...,  0.0055, -0.0074,  0.0051],\n",
      "        [-0.0019, -0.0016,  0.0019,  ..., -0.0016,  0.0011,  0.0002]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight tensor([[-0.0047,  0.0006,  0.0060,  ..., -0.0027,  0.0063,  0.0005],\n",
      "        [ 0.0010, -0.0026, -0.0003,  ...,  0.0034,  0.0048,  0.0010],\n",
      "        [-0.0073, -0.0011, -0.0030,  ...,  0.0001, -0.0035,  0.0017],\n",
      "        ...,\n",
      "        [-0.0050,  0.0001, -0.0003,  ..., -0.0012,  0.0066,  0.0014],\n",
      "        [ 0.0017,  0.0004,  0.0005,  ...,  0.0028,  0.0062,  0.0009],\n",
      "        [ 0.0067,  0.0012,  0.0007,  ...,  0.0018, -0.0056,  0.0013]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight tensor([[ 0.0081, -0.0016, -0.0006,  ..., -0.0006, -0.0011, -0.0011],\n",
      "        [ 0.0003, -0.0004,  0.0002,  ..., -0.0012, -0.0002, -0.0006],\n",
      "        [ 0.0009,  0.0033,  0.0030,  ...,  0.0055,  0.0036,  0.0006],\n",
      "        ...,\n",
      "        [ 0.0018, -0.0080,  0.0002,  ...,  0.0036, -0.0009, -0.0022],\n",
      "        [-0.0142, -0.0024,  0.0003,  ...,  0.0056, -0.0039,  0.0107],\n",
      "        [-0.0004, -0.0013, -0.0050,  ..., -0.0001,  0.0009,  0.0065]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight tensor([[-0.0130, -0.0069, -0.0026,  ..., -0.0081,  0.0065, -0.0100],\n",
      "        [ 0.0002, -0.0096,  0.0008,  ...,  0.0011,  0.0037,  0.0072],\n",
      "        [ 0.0012, -0.0007, -0.0009,  ..., -0.0018,  0.0137,  0.0065],\n",
      "        ...,\n",
      "        [ 0.0059, -0.0033,  0.0007,  ..., -0.0092,  0.0111, -0.0024],\n",
      "        [ 0.0099, -0.0076, -0.0110,  ...,  0.0086,  0.0055, -0.0048],\n",
      "        [ 0.0019, -0.0050, -0.0047,  ...,  0.0076,  0.0086, -0.0087]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight tensor([[-0.0009,  0.0051,  0.0024,  ...,  0.0009,  0.0021, -0.0039],\n",
      "        [ 0.0003,  0.0003,  0.0004,  ...,  0.0002,  0.0002,  0.0006],\n",
      "        [ 0.0058,  0.0019,  0.0045,  ...,  0.0026, -0.0028, -0.0003],\n",
      "        ...,\n",
      "        [ 0.0022, -0.0006,  0.0019,  ...,  0.0024,  0.0082, -0.0003],\n",
      "        [-0.0023,  0.0099, -0.0035,  ..., -0.0171,  0.0072, -0.0113],\n",
      "        [ 0.0047, -0.0012, -0.0020,  ...,  0.0013,  0.0009,  0.0011]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight tensor([[-4.7455e-03,  2.4244e-03, -1.1252e-02,  ..., -2.5323e-04,\n",
      "          2.9204e-03,  3.7550e-03],\n",
      "        [ 7.0348e-04, -2.5024e-03, -4.4276e-03,  ...,  3.1305e-03,\n",
      "          1.1403e-05,  2.3659e-03],\n",
      "        [ 2.9885e-03,  2.4557e-03, -4.6327e-04,  ..., -7.4211e-03,\n",
      "          5.3843e-03,  1.3786e-03],\n",
      "        ...,\n",
      "        [-1.7972e-02,  6.1774e-03,  8.2241e-03,  ...,  4.0710e-03,\n",
      "          8.3718e-03, -2.7259e-03],\n",
      "        [-2.8197e-03, -8.0492e-04,  6.1896e-03,  ...,  6.5395e-03,\n",
      "          5.1770e-03,  5.9827e-04],\n",
      "        [ 3.9563e-03, -2.7363e-04,  3.1404e-03,  ...,  1.5235e-02,\n",
      "         -3.0644e-03, -1.4426e-02]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight tensor([[ 4.6309e-03,  1.5321e-04,  6.2329e-04,  ...,  3.2648e-03,\n",
      "          1.6514e-03,  5.9317e-03],\n",
      "        [-2.4203e-03,  4.9725e-04,  2.5036e-03,  ...,  2.2185e-04,\n",
      "          5.9257e-04,  3.1811e-05],\n",
      "        [-1.5216e-03, -5.0775e-03,  2.1209e-03,  ..., -8.7143e-04,\n",
      "          2.7766e-04,  2.8857e-03],\n",
      "        ...,\n",
      "        [ 4.4470e-03,  1.3011e-03, -2.4024e-03,  ..., -7.5349e-04,\n",
      "          3.3624e-03, -2.4239e-03],\n",
      "        [ 1.3349e-02, -1.7198e-02, -1.5471e-02,  ..., -1.5259e-03,\n",
      "         -8.1639e-03, -7.6672e-03],\n",
      "        [-4.7263e-03,  3.0734e-03,  6.0964e-03,  ...,  2.5848e-03,\n",
      "          2.6018e-03,  4.1840e-03]])\n",
      "unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight tensor([[ 0.0086,  0.0100, -0.0031,  ..., -0.0028,  0.0130,  0.0071],\n",
      "        [-0.0101,  0.0065,  0.0065,  ..., -0.0105,  0.0053, -0.0081],\n",
      "        [ 0.0060,  0.0087, -0.0069,  ..., -0.0096, -0.0041,  0.0011],\n",
      "        ...,\n",
      "        [ 0.0070, -0.0087,  0.0062,  ..., -0.0103,  0.0078, -0.0009],\n",
      "        [ 0.0044,  0.0086,  0.0064,  ..., -0.0188, -0.0109, -0.0119],\n",
      "        [ 0.0033, -0.0034,  0.0059,  ..., -0.0036,  0.0038, -0.0056]])\n",
      "unet.up_blocks.1.attentions.0.proj_in.lora.down.weight tensor([[[[ 0.0036]],\n",
      "\n",
      "         [[-0.0111]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[-0.0063]]],\n",
      "\n",
      "\n",
      "        [[[-0.0001]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         [[-0.0001]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[ 0.0009]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0095]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[ 0.0036]],\n",
      "\n",
      "         [[-0.0008]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0018]],\n",
      "\n",
      "         [[ 0.0034]],\n",
      "\n",
      "         [[-0.0028]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         [[ 0.0038]],\n",
      "\n",
      "         [[-0.0008]]],\n",
      "\n",
      "\n",
      "        [[[-0.0216]],\n",
      "\n",
      "         [[-0.0383]],\n",
      "\n",
      "         [[ 0.0177]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0191]],\n",
      "\n",
      "         [[-0.0118]],\n",
      "\n",
      "         [[-0.0216]]],\n",
      "\n",
      "\n",
      "        [[[-0.0001]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         [[-0.0065]]]])\n",
      "unet.up_blocks.1.attentions.0.proj_in.lora.up.weight tensor([[[[-0.0044]],\n",
      "\n",
      "         [[-0.0099]],\n",
      "\n",
      "         [[ 0.0125]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0055]],\n",
      "\n",
      "         [[-0.0036]],\n",
      "\n",
      "         [[ 0.0141]]],\n",
      "\n",
      "\n",
      "        [[[-0.0206]],\n",
      "\n",
      "         [[-0.0189]],\n",
      "\n",
      "         [[ 0.0119]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0084]],\n",
      "\n",
      "         [[-0.0050]],\n",
      "\n",
      "         [[ 0.0010]]],\n",
      "\n",
      "\n",
      "        [[[-0.0134]],\n",
      "\n",
      "         [[ 0.0010]],\n",
      "\n",
      "         [[-0.0082]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0100]],\n",
      "\n",
      "         [[-0.0102]],\n",
      "\n",
      "         [[ 0.0021]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0225]],\n",
      "\n",
      "         [[ 0.0141]],\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0034]],\n",
      "\n",
      "         [[ 0.0138]],\n",
      "\n",
      "         [[-0.0012]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0119]],\n",
      "\n",
      "         [[-0.0189]],\n",
      "\n",
      "         [[ 0.0131]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0098]],\n",
      "\n",
      "         [[ 0.0057]],\n",
      "\n",
      "         [[ 0.0075]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0051]],\n",
      "\n",
      "         [[ 0.0038]],\n",
      "\n",
      "         [[ 0.0099]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0099]],\n",
      "\n",
      "         [[-0.0057]],\n",
      "\n",
      "         [[ 0.0111]]]])\n",
      "unet.up_blocks.1.attentions.0.proj_out.lora.down.weight tensor([[[[ 0.0017]],\n",
      "\n",
      "         [[ 0.0082]],\n",
      "\n",
      "         [[ 0.0113]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0131]],\n",
      "\n",
      "         [[-0.0084]],\n",
      "\n",
      "         [[-0.0158]]],\n",
      "\n",
      "\n",
      "        [[[-0.0017]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0063]],\n",
      "\n",
      "         [[-0.0027]],\n",
      "\n",
      "         [[-0.0012]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0041]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0045]],\n",
      "\n",
      "         [[-0.0051]],\n",
      "\n",
      "         [[ 0.0036]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0079]],\n",
      "\n",
      "         [[-0.0071]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0047]],\n",
      "\n",
      "         [[ 0.0046]],\n",
      "\n",
      "         [[ 0.0026]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0367]],\n",
      "\n",
      "         [[ 0.0083]],\n",
      "\n",
      "         [[-0.0036]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0391]],\n",
      "\n",
      "         [[ 0.0162]],\n",
      "\n",
      "         [[-0.0166]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0018]],\n",
      "\n",
      "         [[-0.0048]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0023]],\n",
      "\n",
      "         [[-0.0085]],\n",
      "\n",
      "         [[-0.0050]]]])\n",
      "unet.up_blocks.1.attentions.0.proj_out.lora.up.weight tensor([[[[ 0.0114]],\n",
      "\n",
      "         [[-0.0168]],\n",
      "\n",
      "         [[ 0.0047]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0231]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[-0.0092]]],\n",
      "\n",
      "\n",
      "        [[[-0.0006]],\n",
      "\n",
      "         [[ 0.0121]],\n",
      "\n",
      "         [[ 0.0039]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0074]],\n",
      "\n",
      "         [[-0.0070]],\n",
      "\n",
      "         [[ 0.0112]]],\n",
      "\n",
      "\n",
      "        [[[-0.0243]],\n",
      "\n",
      "         [[ 0.0118]],\n",
      "\n",
      "         [[ 0.0104]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0234]],\n",
      "\n",
      "         [[ 0.0037]],\n",
      "\n",
      "         [[ 0.0107]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0045]],\n",
      "\n",
      "         [[-0.0255]],\n",
      "\n",
      "         [[-0.0045]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0139]],\n",
      "\n",
      "         [[-0.0171]],\n",
      "\n",
      "         [[-0.0026]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0004]],\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         [[ 0.0040]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0116]],\n",
      "\n",
      "         [[ 0.0174]],\n",
      "\n",
      "         [[ 0.0043]]],\n",
      "\n",
      "\n",
      "        [[[-0.0047]],\n",
      "\n",
      "         [[ 0.0113]],\n",
      "\n",
      "         [[ 0.0137]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0177]],\n",
      "\n",
      "         [[-0.0038]],\n",
      "\n",
      "         [[ 0.0014]]]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight tensor([[-4.0921e-03, -7.7982e-04, -6.8061e-03,  ..., -5.7841e-05,\n",
      "         -1.4486e-03, -8.5298e-04],\n",
      "        [-1.5077e-03, -8.6403e-04,  1.5335e-03,  ...,  5.0876e-04,\n",
      "         -1.7306e-04, -8.5533e-04],\n",
      "        [-8.3387e-03,  2.1048e-03, -2.0223e-04,  ..., -5.1376e-04,\n",
      "          6.3056e-03, -7.8654e-03],\n",
      "        ...,\n",
      "        [-3.8701e-04,  9.6226e-04,  1.8141e-03,  ...,  1.2491e-03,\n",
      "         -2.2884e-03,  4.4533e-04],\n",
      "        [ 2.4847e-03,  1.9958e-03,  4.0870e-03,  ...,  4.7332e-03,\n",
      "          6.4547e-03,  3.8492e-03],\n",
      "        [-6.4708e-04, -1.6263e-03,  1.9851e-03,  ..., -6.7281e-04,\n",
      "         -5.5386e-04,  1.9294e-03]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight tensor([[ 0.0012, -0.0048, -0.0044,  ..., -0.0031, -0.0013, -0.0020],\n",
      "        [ 0.0050,  0.0023, -0.0048,  ...,  0.0101,  0.0055,  0.0052],\n",
      "        [ 0.0010,  0.0013,  0.0009,  ..., -0.0010,  0.0099,  0.0017],\n",
      "        ...,\n",
      "        [-0.0079,  0.0020,  0.0090,  ...,  0.0035, -0.0097,  0.0005],\n",
      "        [ 0.0022, -0.0043,  0.0074,  ...,  0.0066,  0.0081,  0.0035],\n",
      "        [ 0.0088, -0.0001,  0.0120,  ..., -0.0005,  0.0020,  0.0006]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight tensor([[-3.4919e-03, -5.8243e-04,  8.4677e-04,  ..., -1.0513e-02,\n",
      "         -6.3507e-03,  3.8976e-03],\n",
      "        [ 1.1838e-04, -3.2921e-03, -1.1389e-04,  ..., -7.2477e-04,\n",
      "         -1.7920e-04,  1.7169e-03],\n",
      "        [ 1.9696e-04, -2.3563e-03,  3.4286e-03,  ...,  1.7246e-03,\n",
      "         -4.2701e-03, -1.4616e-03],\n",
      "        ...,\n",
      "        [ 2.6190e-03, -3.1797e-03, -1.2518e-03,  ...,  9.4869e-04,\n",
      "         -7.7846e-04,  9.5095e-04],\n",
      "        [-1.3509e-02,  4.7835e-03,  4.5797e-03,  ...,  1.3328e-02,\n",
      "          2.4049e-02,  1.8351e-02],\n",
      "        [ 1.7086e-03,  5.2011e-04,  4.1205e-04,  ...,  7.2314e-05,\n",
      "         -3.2827e-03,  5.0803e-03]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight tensor([[-7.9631e-03,  1.7846e-03,  2.3745e-03,  ..., -1.3183e-02,\n",
      "         -8.6515e-03,  6.4493e-03],\n",
      "        [ 8.8993e-04, -5.3066e-03,  1.3056e-03,  ...,  7.7257e-05,\n",
      "          2.2783e-03,  6.6798e-03],\n",
      "        [-7.0251e-03,  8.3071e-03, -8.4317e-04,  ...,  5.6561e-03,\n",
      "         -2.9453e-03, -4.9190e-03],\n",
      "        ...,\n",
      "        [ 5.7574e-03, -1.2859e-02, -2.1005e-03,  ...,  7.4630e-03,\n",
      "          1.4031e-02, -2.5195e-02],\n",
      "        [-1.2323e-03,  7.4374e-03, -5.2860e-04,  ...,  5.0365e-03,\n",
      "         -7.0651e-03, -2.1274e-03],\n",
      "        [ 2.1594e-04,  9.1092e-03,  1.0522e-02,  ..., -9.6625e-03,\n",
      "         -3.1017e-03,  3.1395e-03]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight tensor([[ 6.1550e-03, -3.2972e-03,  7.0696e-05,  ...,  1.0158e-03,\n",
      "          2.4551e-04,  3.0896e-04],\n",
      "        [ 1.3912e-03,  4.4685e-04,  3.7800e-04,  ..., -6.1543e-05,\n",
      "          7.2914e-04,  5.2944e-04],\n",
      "        [-2.0108e-04,  1.7837e-03, -7.5162e-04,  ..., -2.0961e-04,\n",
      "         -1.9440e-04, -4.1011e-04],\n",
      "        ...,\n",
      "        [-5.1450e-04,  8.8984e-04, -1.9018e-03,  ...,  4.3634e-04,\n",
      "         -3.7031e-03, -1.6504e-03],\n",
      "        [-2.4925e-03,  2.1524e-03, -4.7435e-03,  ...,  1.1637e-03,\n",
      "         -2.5281e-02, -2.9946e-02],\n",
      "        [ 4.8707e-03, -1.4469e-03, -1.4049e-03,  ...,  5.0269e-03,\n",
      "         -2.3114e-03,  4.4964e-04]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight tensor([[-0.0067, -0.0083,  0.0067,  ..., -0.0046,  0.0068, -0.0002],\n",
      "        [-0.0068, -0.0030,  0.0008,  ..., -0.0049,  0.0081,  0.0006],\n",
      "        [-0.0018,  0.0005,  0.0086,  ...,  0.0023, -0.0123,  0.0063],\n",
      "        ...,\n",
      "        [-0.0006,  0.0013, -0.0059,  ..., -0.0008,  0.0041, -0.0028],\n",
      "        [-0.0057,  0.0020,  0.0018,  ...,  0.0066, -0.0025,  0.0046],\n",
      "        [-0.0111,  0.0049, -0.0073,  ...,  0.0015,  0.0039, -0.0006]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight tensor([[-4.1122e-03, -3.5257e-03,  4.4510e-03,  ..., -2.9514e-03,\n",
      "         -6.1431e-03, -8.6337e-04],\n",
      "        [ 8.8518e-04,  5.3978e-04,  1.3387e-03,  ..., -4.8439e-04,\n",
      "         -5.0694e-04, -1.6332e-04],\n",
      "        [-6.6353e-03,  3.8689e-03,  5.1624e-03,  ..., -8.9243e-03,\n",
      "          4.7431e-03, -3.0187e-03],\n",
      "        ...,\n",
      "        [ 8.1849e-04, -3.3558e-03,  8.7079e-04,  ..., -2.6896e-03,\n",
      "         -3.1910e-03, -1.4775e-03],\n",
      "        [ 1.7530e-02, -9.4519e-05, -1.0301e-02,  ...,  1.6725e-02,\n",
      "         -4.4651e-03,  2.0790e-03],\n",
      "        [-1.6362e-04, -7.5358e-04,  1.2342e-03,  ...,  2.0192e-03,\n",
      "          6.5023e-03,  5.9470e-03]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight tensor([[ 0.0069, -0.0008, -0.0003,  ...,  0.0111,  0.0024,  0.0025],\n",
      "        [-0.0033, -0.0035,  0.0119,  ...,  0.0019, -0.0100, -0.0047],\n",
      "        [-0.0050,  0.0008,  0.0085,  ..., -0.0071, -0.0103, -0.0083],\n",
      "        ...,\n",
      "        [ 0.0060, -0.0006,  0.0080,  ...,  0.0009, -0.0026, -0.0046],\n",
      "        [ 0.0016,  0.0053,  0.0062,  ..., -0.0059, -0.0100,  0.0052],\n",
      "        [ 0.0137,  0.0010, -0.0135,  ..., -0.0016, -0.0011, -0.0079]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight tensor([[ 1.9550e-03, -2.1712e-03, -4.2080e-03,  ..., -4.5305e-03,\n",
      "          2.1853e-03, -4.3685e-03],\n",
      "        [-3.1000e-04,  1.2477e-03, -4.8082e-04,  ..., -3.9758e-04,\n",
      "         -5.1865e-04,  5.4601e-05],\n",
      "        [ 1.9499e-03, -7.9894e-04,  4.9373e-04,  ...,  1.2373e-03,\n",
      "         -3.6515e-04,  1.6397e-03],\n",
      "        ...,\n",
      "        [ 7.6129e-04,  4.4575e-04, -1.4748e-03,  ...,  2.3206e-03,\n",
      "          2.5107e-03,  5.2213e-03],\n",
      "        [-6.2679e-03, -1.6061e-02,  8.4044e-03,  ..., -4.8415e-04,\n",
      "          1.3274e-02, -1.9381e-02],\n",
      "        [ 3.9175e-04,  2.2682e-03, -5.1600e-04,  ..., -2.8949e-03,\n",
      "          1.9541e-03, -1.6315e-03]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight tensor([[-2.3619e-03, -7.3645e-03, -4.3130e-03,  ...,  4.1699e-04,\n",
      "         -5.9550e-04,  9.9022e-04],\n",
      "        [-3.6522e-03, -2.2345e-03,  8.8894e-03,  ...,  4.1242e-03,\n",
      "         -5.4749e-03, -5.8275e-03],\n",
      "        [-6.0304e-03, -9.3700e-04,  6.7148e-03,  ..., -6.8157e-03,\n",
      "          5.0945e-03, -1.0340e-03],\n",
      "        ...,\n",
      "        [-3.3725e-03,  3.8165e-03, -8.3610e-03,  ...,  8.6055e-03,\n",
      "         -8.8668e-05,  6.4720e-03],\n",
      "        [ 6.2286e-03, -3.7728e-03, -1.1693e-03,  ..., -1.0893e-03,\n",
      "         -6.0814e-03,  2.7757e-03],\n",
      "        [ 2.4222e-03, -1.0199e-02,  4.6103e-03,  ...,  2.0599e-03,\n",
      "          7.6255e-03,  3.8558e-03]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight tensor([[ 5.3050e-03, -5.4401e-04, -9.4578e-04,  ..., -3.4109e-03,\n",
      "         -3.5912e-04, -4.2484e-03],\n",
      "        [-4.7017e-05,  2.3190e-04,  3.2519e-04,  ...,  9.3527e-04,\n",
      "         -1.0989e-04, -4.5969e-04],\n",
      "        [ 7.1966e-05,  5.1520e-03, -1.8808e-03,  ..., -3.5706e-03,\n",
      "          7.5919e-03,  3.2548e-03],\n",
      "        ...,\n",
      "        [-4.8555e-03,  1.4751e-03, -1.7582e-03,  ...,  6.6773e-03,\n",
      "          4.8245e-03,  4.8506e-03],\n",
      "        [-8.0908e-03,  3.6814e-03, -9.1904e-03,  ..., -1.8414e-03,\n",
      "         -2.6283e-02,  1.1555e-02],\n",
      "        [ 2.3922e-03, -4.1762e-03, -7.0790e-03,  ..., -3.3515e-03,\n",
      "          8.9305e-05, -6.0137e-04]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight tensor([[-1.5647e-03, -5.2197e-03, -1.3790e-03,  ..., -1.6810e-03,\n",
      "          1.7768e-03, -4.4873e-03],\n",
      "        [ 1.1614e-03, -6.3443e-03, -3.0028e-03,  ...,  1.3045e-03,\n",
      "         -3.8541e-03, -3.2987e-03],\n",
      "        [ 2.3773e-04, -6.0071e-03, -1.1217e-02,  ..., -7.4820e-03,\n",
      "          1.6931e-03, -5.0326e-03],\n",
      "        ...,\n",
      "        [ 2.9189e-03, -1.0854e-02,  4.9161e-04,  ...,  7.8999e-03,\n",
      "         -3.4423e-03,  7.0787e-03],\n",
      "        [-5.6995e-03,  2.2246e-03,  1.5587e-03,  ..., -4.7484e-03,\n",
      "          6.0396e-04,  3.3165e-05],\n",
      "        [-8.1553e-03, -6.0537e-03, -7.3933e-03,  ..., -2.0786e-03,\n",
      "         -2.1076e-03, -2.7183e-03]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight tensor([[-2.7276e-03, -6.0107e-03, -2.6153e-03,  ...,  2.6338e-03,\n",
      "          2.0833e-03, -2.2593e-05],\n",
      "        [-8.8045e-04,  1.2127e-04, -1.1296e-04,  ...,  1.2445e-05,\n",
      "         -1.4585e-04, -7.2708e-05],\n",
      "        [ 2.8788e-03,  3.1549e-03, -2.3999e-03,  ..., -1.1887e-03,\n",
      "          2.1933e-03, -1.2237e-04],\n",
      "        ...,\n",
      "        [-3.2453e-03, -5.4619e-04,  1.3600e-03,  ..., -2.5006e-03,\n",
      "          5.4082e-03,  5.4617e-03],\n",
      "        [ 3.8811e-03,  7.4758e-03,  1.3794e-02,  ..., -8.4405e-03,\n",
      "          2.8696e-03, -8.8617e-03],\n",
      "        [-2.9576e-04,  4.9866e-04,  1.1432e-03,  ...,  2.0886e-03,\n",
      "          8.7458e-04, -1.2858e-03]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight tensor([[ 0.0001, -0.0024,  0.0018,  ...,  0.0031,  0.0032, -0.0034],\n",
      "        [ 0.0006,  0.0056,  0.0024,  ..., -0.0030,  0.0031, -0.0032],\n",
      "        [ 0.0004, -0.0038, -0.0014,  ...,  0.0001, -0.0025, -0.0012],\n",
      "        ...,\n",
      "        [-0.0014,  0.0011, -0.0026,  ..., -0.0019,  0.0020, -0.0034],\n",
      "        [ 0.0053, -0.0057, -0.0041,  ...,  0.0052,  0.0093, -0.0025],\n",
      "        [-0.0046, -0.0040, -0.0084,  ...,  0.0030,  0.0021, -0.0015]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight tensor([[ 1.4408e-04, -1.0901e-03, -6.8997e-03,  ...,  2.8168e-03,\n",
      "         -1.4310e-03, -1.3741e-03],\n",
      "        [-8.8815e-04, -1.7713e-03, -8.8042e-04,  ...,  1.0221e-03,\n",
      "         -3.0419e-04,  1.0729e-05],\n",
      "        [-8.1050e-04,  4.9225e-03, -4.2531e-04,  ..., -1.8381e-03,\n",
      "         -2.7674e-03,  5.4929e-03],\n",
      "        ...,\n",
      "        [ 1.8845e-03,  1.3129e-03,  2.3169e-03,  ...,  2.2670e-03,\n",
      "         -8.2918e-04,  1.4753e-03],\n",
      "        [-1.4489e-02, -7.1710e-04,  1.2706e-04,  ...,  4.8597e-03,\n",
      "         -2.4569e-03,  1.3731e-03],\n",
      "        [-9.6764e-04, -4.2748e-04, -2.0125e-03,  ..., -2.3553e-04,\n",
      "         -1.7936e-03,  2.4638e-04]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight tensor([[-0.0068, -0.0124,  0.0007,  ...,  0.0032, -0.0102,  0.0033],\n",
      "        [ 0.0034,  0.0066,  0.0091,  ...,  0.0061,  0.0042, -0.0091],\n",
      "        [-0.0050, -0.0033, -0.0050,  ...,  0.0012, -0.0041,  0.0008],\n",
      "        ...,\n",
      "        [-0.0035, -0.0118, -0.0017,  ..., -0.0024,  0.0028, -0.0103],\n",
      "        [-0.0022, -0.0072, -0.0082,  ..., -0.0134,  0.0031, -0.0137],\n",
      "        [-0.0014,  0.0029,  0.0049,  ...,  0.0051, -0.0057,  0.0083]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight tensor([[ 2.9817e-03,  1.2969e-03,  2.8019e-03,  ...,  3.5279e-04,\n",
      "         -3.2583e-03,  2.7186e-03],\n",
      "        [-1.1321e-05, -1.8264e-04, -6.9069e-05,  ..., -1.1278e-03,\n",
      "         -3.0699e-05, -1.2591e-03],\n",
      "        [-3.2541e-03,  3.4436e-03,  3.5542e-03,  ...,  8.3122e-03,\n",
      "         -2.4231e-03, -3.6246e-05],\n",
      "        ...,\n",
      "        [-4.2713e-04, -2.3921e-03, -8.0141e-04,  ..., -1.4102e-03,\n",
      "          8.7739e-03, -8.9798e-03],\n",
      "        [ 7.2803e-03, -1.9573e-03, -9.6195e-05,  ...,  2.6711e-04,\n",
      "         -1.2289e-02, -5.3618e-03],\n",
      "        [-1.7541e-03, -2.5238e-03, -3.2395e-03,  ..., -6.4659e-03,\n",
      "          4.2378e-03,  5.0455e-04]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight tensor([[-6.8299e-03,  3.5168e-03, -2.7979e-03,  ...,  5.0428e-03,\n",
      "          1.5753e-02,  1.7749e-04],\n",
      "        [-2.1451e-04, -1.8407e-03,  3.7372e-03,  ..., -8.4341e-04,\n",
      "         -7.2853e-03, -5.4199e-03],\n",
      "        [-3.0573e-03,  1.3430e-02,  1.1887e-02,  ...,  3.7338e-03,\n",
      "          4.6224e-04, -4.5027e-03],\n",
      "        ...,\n",
      "        [ 2.9776e-03,  7.2526e-03, -2.8375e-03,  ...,  5.0002e-05,\n",
      "         -9.0122e-03, -1.0783e-02],\n",
      "        [-7.0852e-03,  9.2234e-03,  1.4133e-03,  ..., -6.7681e-03,\n",
      "          5.6952e-03,  4.4072e-04],\n",
      "        [-5.6076e-03, -2.0268e-02,  6.1415e-03,  ..., -2.4359e-03,\n",
      "         -9.7957e-04,  7.3966e-03]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight tensor([[-0.0065,  0.0022,  0.0015,  ..., -0.0028, -0.0069, -0.0072],\n",
      "        [ 0.0006,  0.0024, -0.0013,  ...,  0.0007, -0.0015, -0.0013],\n",
      "        [-0.0037, -0.0003,  0.0037,  ..., -0.0077,  0.0059,  0.0045],\n",
      "        ...,\n",
      "        [ 0.0017,  0.0038,  0.0021,  ...,  0.0036,  0.0036,  0.0009],\n",
      "        [ 0.0188,  0.0082,  0.0041,  ...,  0.0006,  0.0194,  0.0043],\n",
      "        [ 0.0042,  0.0074,  0.0046,  ...,  0.0076,  0.0010, -0.0005]])\n",
      "unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight tensor([[-0.0122, -0.0119,  0.0110,  ..., -0.0032,  0.0026,  0.0091],\n",
      "        [ 0.0035, -0.0035, -0.0093,  ..., -0.0049, -0.0020, -0.0017],\n",
      "        [ 0.0038,  0.0149, -0.0042,  ...,  0.0045,  0.0158,  0.0065],\n",
      "        ...,\n",
      "        [-0.0046,  0.0135, -0.0046,  ..., -0.0057, -0.0011, -0.0168],\n",
      "        [ 0.0017,  0.0030, -0.0011,  ...,  0.0001, -0.0047,  0.0013],\n",
      "        [-0.0079, -0.0174, -0.0059,  ...,  0.0146,  0.0012,  0.0018]])\n",
      "unet.up_blocks.1.attentions.1.proj_in.lora.down.weight tensor([[[[ 0.0003]],\n",
      "\n",
      "         [[ 0.0203]],\n",
      "\n",
      "         [[-0.0002]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0144]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[ 0.0025]]],\n",
      "\n",
      "\n",
      "        [[[-0.0014]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0047]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         [[ 0.0010]]],\n",
      "\n",
      "\n",
      "        [[[-0.0003]],\n",
      "\n",
      "         [[-0.0056]],\n",
      "\n",
      "         [[ 0.0121]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0037]],\n",
      "\n",
      "         [[-0.0107]],\n",
      "\n",
      "         [[ 0.0094]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0019]],\n",
      "\n",
      "         [[-0.0027]],\n",
      "\n",
      "         [[-0.0049]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0045]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[-0.0094]]],\n",
      "\n",
      "\n",
      "        [[[-0.0010]],\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         [[ 0.0040]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0236]],\n",
      "\n",
      "         [[ 0.0039]],\n",
      "\n",
      "         [[-0.0004]]],\n",
      "\n",
      "\n",
      "        [[[-0.0059]],\n",
      "\n",
      "         [[ 0.0001]],\n",
      "\n",
      "         [[-0.0120]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[-0.0038]],\n",
      "\n",
      "         [[-0.0067]]]])\n",
      "unet.up_blocks.1.attentions.1.proj_in.lora.up.weight tensor([[[[ 5.4002e-03]],\n",
      "\n",
      "         [[ 1.6900e-02]],\n",
      "\n",
      "         [[-2.8416e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5117e-02]],\n",
      "\n",
      "         [[-7.1701e-03]],\n",
      "\n",
      "         [[-8.5818e-03]]],\n",
      "\n",
      "\n",
      "        [[[-8.7011e-03]],\n",
      "\n",
      "         [[ 2.1645e-03]],\n",
      "\n",
      "         [[ 2.7532e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.6549e-03]],\n",
      "\n",
      "         [[-1.2116e-02]],\n",
      "\n",
      "         [[-2.3314e-03]]],\n",
      "\n",
      "\n",
      "        [[[-7.4275e-07]],\n",
      "\n",
      "         [[-2.8992e-02]],\n",
      "\n",
      "         [[ 1.9608e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.5483e-02]],\n",
      "\n",
      "         [[ 4.2839e-03]],\n",
      "\n",
      "         [[-2.0445e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.2491e-02]],\n",
      "\n",
      "         [[-1.2597e-02]],\n",
      "\n",
      "         [[ 3.3417e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.8411e-04]],\n",
      "\n",
      "         [[ 2.0915e-03]],\n",
      "\n",
      "         [[-2.2226e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1586e-02]],\n",
      "\n",
      "         [[-1.9423e-03]],\n",
      "\n",
      "         [[-1.7032e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.3507e-03]],\n",
      "\n",
      "         [[-7.5259e-04]],\n",
      "\n",
      "         [[ 2.5961e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.0169e-02]],\n",
      "\n",
      "         [[-1.4623e-02]],\n",
      "\n",
      "         [[ 1.1999e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.6317e-03]],\n",
      "\n",
      "         [[-8.1078e-03]],\n",
      "\n",
      "         [[ 9.9079e-03]]]])\n",
      "unet.up_blocks.1.attentions.1.proj_out.lora.down.weight tensor([[[[ 0.0094]],\n",
      "\n",
      "         [[ 0.0032]],\n",
      "\n",
      "         [[-0.0097]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0002]],\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         [[ 0.0041]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0016]],\n",
      "\n",
      "         [[ 0.0037]],\n",
      "\n",
      "         [[ 0.0030]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0010]],\n",
      "\n",
      "         [[ 0.0022]],\n",
      "\n",
      "         [[ 0.0009]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0058]],\n",
      "\n",
      "         [[-0.0093]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0141]],\n",
      "\n",
      "         [[-0.0118]],\n",
      "\n",
      "         [[-0.0026]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0015]],\n",
      "\n",
      "         [[-0.0034]],\n",
      "\n",
      "         [[ 0.0030]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0132]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[-0.0052]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0115]],\n",
      "\n",
      "         [[ 0.0183]],\n",
      "\n",
      "         [[-0.0197]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0240]],\n",
      "\n",
      "         [[ 0.0242]],\n",
      "\n",
      "         [[-0.0158]]],\n",
      "\n",
      "\n",
      "        [[[-0.0097]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[-0.0073]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         [[ 0.0052]],\n",
      "\n",
      "         [[ 0.0128]]]])\n",
      "unet.up_blocks.1.attentions.1.proj_out.lora.up.weight tensor([[[[ 0.0045]],\n",
      "\n",
      "         [[-0.0256]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0051]],\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[-0.0189]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0014]],\n",
      "\n",
      "         [[-0.0135]],\n",
      "\n",
      "         [[-0.0030]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0040]],\n",
      "\n",
      "         [[-0.0201]],\n",
      "\n",
      "         [[-0.0102]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0249]],\n",
      "\n",
      "         [[-0.0117]],\n",
      "\n",
      "         [[ 0.0023]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0060]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         [[ 0.0047]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0040]],\n",
      "\n",
      "         [[ 0.0174]],\n",
      "\n",
      "         [[ 0.0391]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0154]],\n",
      "\n",
      "         [[ 0.0048]],\n",
      "\n",
      "         [[ 0.0179]]],\n",
      "\n",
      "\n",
      "        [[[-0.0064]],\n",
      "\n",
      "         [[-0.0107]],\n",
      "\n",
      "         [[ 0.0252]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0068]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[ 0.0231]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0143]],\n",
      "\n",
      "         [[-0.0337]],\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0203]],\n",
      "\n",
      "         [[-0.0204]],\n",
      "\n",
      "         [[-0.0102]]]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight tensor([[ 2.3253e-03,  1.4403e-02, -7.1982e-03,  ..., -1.9291e-02,\n",
      "          1.0456e-02,  1.1339e-02],\n",
      "        [-1.9855e-03,  1.3475e-03,  2.2044e-05,  ...,  6.8492e-04,\n",
      "          1.0732e-03,  2.9279e-03],\n",
      "        [-2.9831e-03, -3.7733e-03,  4.8323e-03,  ...,  5.4866e-03,\n",
      "         -5.7043e-03, -1.7429e-02],\n",
      "        ...,\n",
      "        [ 4.1846e-03,  1.3840e-02,  5.7917e-03,  ...,  3.1731e-03,\n",
      "          6.8969e-03,  1.0830e-02],\n",
      "        [-2.0692e-02, -9.1993e-04, -9.3598e-03,  ..., -2.0073e-02,\n",
      "          1.0110e-02, -1.3113e-03],\n",
      "        [-3.7466e-03, -9.2851e-03,  6.7246e-03,  ..., -5.2364e-03,\n",
      "         -3.9588e-03, -2.7705e-03]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight tensor([[-0.0048, -0.0076,  0.0045,  ...,  0.0078,  0.0141,  0.0116],\n",
      "        [ 0.0137, -0.0004,  0.0020,  ...,  0.0092, -0.0090,  0.0015],\n",
      "        [-0.0051,  0.0106,  0.0011,  ...,  0.0007,  0.0089, -0.0029],\n",
      "        ...,\n",
      "        [-0.0221, -0.0242,  0.0214,  ...,  0.0264,  0.0132, -0.0006],\n",
      "        [-0.0051, -0.0017, -0.0096,  ..., -0.0126,  0.0226,  0.0003],\n",
      "        [-0.0141,  0.0031, -0.0092,  ...,  0.0077,  0.0036,  0.0191]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight tensor([[-9.5870e-03, -7.4317e-03,  5.4968e-03,  ..., -7.5641e-03,\n",
      "         -7.2478e-03, -2.2294e-03],\n",
      "        [-7.2834e-05,  1.0537e-03,  4.5346e-04,  ..., -2.8553e-03,\n",
      "         -3.3615e-03, -1.5751e-03],\n",
      "        [ 1.8329e-03,  5.2376e-04,  1.7756e-03,  ..., -2.1005e-03,\n",
      "          5.4185e-03,  1.7131e-02],\n",
      "        ...,\n",
      "        [-1.3015e-03, -3.0392e-04, -4.6812e-03,  ...,  2.1552e-02,\n",
      "          1.9926e-02,  8.6793e-03],\n",
      "        [ 9.1944e-03,  8.7567e-03, -6.0372e-03,  ..., -1.9747e-02,\n",
      "         -1.7009e-02, -3.1007e-02],\n",
      "        [ 3.0565e-03, -4.1050e-03,  7.8310e-03,  ...,  3.7442e-03,\n",
      "         -2.1719e-03, -7.8299e-03]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight tensor([[ 0.0227,  0.0029, -0.0174,  ..., -0.0080,  0.0089, -0.0063],\n",
      "        [ 0.0074, -0.0025,  0.0179,  ..., -0.0127,  0.0221,  0.0222],\n",
      "        [-0.0023,  0.0009,  0.0077,  ..., -0.0029, -0.0200,  0.0073],\n",
      "        ...,\n",
      "        [ 0.0072,  0.0114, -0.0066,  ...,  0.0046, -0.0160,  0.0075],\n",
      "        [ 0.0021, -0.0057,  0.0159,  ..., -0.0199,  0.0103, -0.0206],\n",
      "        [ 0.0113,  0.0026, -0.0180,  ...,  0.0103, -0.0077,  0.0077]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight tensor([[-0.0011,  0.0114, -0.0004,  ...,  0.0066,  0.0048, -0.0043],\n",
      "        [-0.0026, -0.0027,  0.0012,  ...,  0.0003, -0.0026,  0.0012],\n",
      "        [ 0.0002,  0.0004,  0.0072,  ...,  0.0134, -0.0006,  0.0036],\n",
      "        ...,\n",
      "        [ 0.0051, -0.0031,  0.0009,  ...,  0.0052, -0.0011,  0.0005],\n",
      "        [-0.0416,  0.0023,  0.0338,  ..., -0.0138, -0.0148, -0.0011],\n",
      "        [-0.0007, -0.0021, -0.0049,  ...,  0.0048, -0.0036,  0.0123]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight tensor([[-5.4812e-03,  8.8524e-03,  3.5497e-04,  ...,  2.1644e-02,\n",
      "          1.7290e-03, -3.8991e-03],\n",
      "        [ 1.0360e-02, -3.8887e-03,  1.0130e-02,  ...,  2.4370e-03,\n",
      "         -1.1361e-02,  6.8954e-04],\n",
      "        [ 4.8484e-04, -2.3881e-03,  2.5662e-03,  ...,  2.6538e-03,\n",
      "         -4.7201e-05, -7.7430e-03],\n",
      "        ...,\n",
      "        [ 4.4026e-02,  2.5481e-02,  1.0207e-02,  ..., -3.4509e-02,\n",
      "          1.9353e-02, -2.3496e-02],\n",
      "        [ 2.1754e-02,  8.8647e-03,  2.0521e-02,  ..., -4.2108e-02,\n",
      "          1.5222e-02, -2.0172e-02],\n",
      "        [ 2.7500e-03, -1.5899e-03,  3.1190e-02,  ...,  5.8647e-03,\n",
      "          5.0802e-02,  5.0591e-02]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight tensor([[-0.0066,  0.0017,  0.0048,  ...,  0.0100,  0.0009,  0.0129],\n",
      "        [ 0.0018, -0.0024,  0.0012,  ..., -0.0014, -0.0011,  0.0009],\n",
      "        [-0.0033, -0.0079,  0.0038,  ..., -0.0049, -0.0084, -0.0086],\n",
      "        ...,\n",
      "        [-0.0028,  0.0002, -0.0025,  ...,  0.0023,  0.0013, -0.0010],\n",
      "        [-0.0047, -0.0324,  0.0026,  ..., -0.0184, -0.0187, -0.0172],\n",
      "        [ 0.0032,  0.0021,  0.0011,  ...,  0.0025,  0.0044, -0.0008]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight tensor([[-0.0136, -0.0100, -0.0280,  ...,  0.0082, -0.0221,  0.0038],\n",
      "        [ 0.0149,  0.0013,  0.0022,  ..., -0.0106, -0.0040,  0.0172],\n",
      "        [ 0.0074,  0.0116, -0.0144,  ...,  0.0006,  0.0088, -0.0211],\n",
      "        ...,\n",
      "        [ 0.0061, -0.0234, -0.0180,  ..., -0.0145, -0.0051, -0.0069],\n",
      "        [ 0.0157, -0.0092,  0.0052,  ..., -0.0163,  0.0443,  0.0172],\n",
      "        [-0.0024, -0.0123,  0.0361,  ..., -0.0011, -0.0084, -0.0294]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight tensor([[ 0.0006, -0.0040, -0.0081,  ..., -0.0004, -0.0005, -0.0095],\n",
      "        [ 0.0005, -0.0004,  0.0001,  ..., -0.0006, -0.0004,  0.0005],\n",
      "        [ 0.0070,  0.0029, -0.0009,  ...,  0.0002, -0.0007,  0.0013],\n",
      "        ...,\n",
      "        [ 0.0036, -0.0013,  0.0009,  ..., -0.0036,  0.0031,  0.0031],\n",
      "        [ 0.0143,  0.0012,  0.0085,  ...,  0.0020,  0.0166,  0.0159],\n",
      "        [-0.0063,  0.0008, -0.0002,  ..., -0.0017,  0.0028,  0.0023]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight tensor([[ 2.7922e-03,  1.2293e-03,  1.0850e-03,  ...,  1.9659e-03,\n",
      "          1.2258e-02,  1.7970e-03],\n",
      "        [-8.7735e-04, -3.1045e-03, -1.1308e-02,  ...,  3.9487e-04,\n",
      "         -8.0590e-03,  2.1497e-03],\n",
      "        [-1.0380e-02, -7.4528e-04,  1.0911e-02,  ...,  7.6804e-03,\n",
      "         -4.9613e-03,  9.5314e-04],\n",
      "        ...,\n",
      "        [-1.0518e-02,  2.6193e-03,  6.4105e-03,  ..., -4.3174e-03,\n",
      "          6.8665e-03, -8.5419e-04],\n",
      "        [ 7.3258e-03,  2.7759e-03,  3.8781e-03,  ..., -3.7889e-03,\n",
      "          4.4429e-06,  1.2209e-03],\n",
      "        [-4.2222e-03, -3.7067e-03,  1.9953e-03,  ..., -1.1179e-03,\n",
      "          5.4659e-03, -8.7156e-03]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight tensor([[-7.7101e-03,  2.1952e-03, -1.6969e-04,  ...,  3.9882e-04,\n",
      "          2.5273e-03,  6.5357e-03],\n",
      "        [-2.2877e-04, -3.0638e-04,  9.8030e-05,  ...,  5.1004e-04,\n",
      "         -8.2571e-05,  4.3117e-06],\n",
      "        [ 7.9482e-03,  4.2293e-03, -3.1602e-03,  ...,  1.8495e-03,\n",
      "         -9.0202e-04, -3.0362e-03],\n",
      "        ...,\n",
      "        [-2.1106e-03, -5.1101e-03, -1.0950e-03,  ...,  8.6363e-04,\n",
      "          1.5825e-03,  1.3266e-03],\n",
      "        [ 9.4259e-04, -9.5253e-03,  1.8307e-02,  ..., -1.0189e-02,\n",
      "         -2.5506e-03,  2.5832e-03],\n",
      "        [-1.7127e-03, -3.1304e-03, -4.6555e-04,  ..., -5.8910e-03,\n",
      "          1.5885e-04, -8.1888e-04]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight tensor([[ 0.0015, -0.0076,  0.0021,  ...,  0.0060,  0.0086, -0.0009],\n",
      "        [ 0.0135, -0.0235,  0.0011,  ...,  0.0027, -0.0045,  0.0204],\n",
      "        [-0.0182, -0.0032, -0.0109,  ...,  0.0070,  0.0005, -0.0116],\n",
      "        ...,\n",
      "        [-0.0045, -0.0032, -0.0019,  ..., -0.0078,  0.0059,  0.0101],\n",
      "        [ 0.0055,  0.0057,  0.0082,  ...,  0.0075, -0.0069, -0.0049],\n",
      "        [ 0.0008, -0.0005,  0.0022,  ...,  0.0036,  0.0025,  0.0046]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight tensor([[-3.5188e-03, -3.5653e-03, -1.7833e-04,  ..., -2.8214e-03,\n",
      "          1.3943e-03,  3.7722e-04],\n",
      "        [-5.6805e-04, -9.6424e-04, -2.2949e-05,  ..., -2.6647e-04,\n",
      "          2.7453e-04,  5.7172e-04],\n",
      "        [-2.5117e-03, -2.1245e-03,  4.0577e-03,  ...,  4.7614e-03,\n",
      "          4.0787e-03,  1.1134e-04],\n",
      "        ...,\n",
      "        [-1.0136e-03, -2.3869e-03,  3.4384e-04,  ..., -1.5861e-03,\n",
      "          9.5112e-04,  1.8928e-03],\n",
      "        [-1.3700e-02, -7.9306e-03,  1.4557e-02,  ..., -3.3628e-03,\n",
      "         -4.9083e-04, -2.8734e-03],\n",
      "        [-1.0595e-03,  4.7679e-04,  1.0465e-03,  ..., -1.4923e-03,\n",
      "         -3.8113e-03,  1.1920e-03]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight tensor([[-1.1202e-02,  2.4543e-03, -4.9489e-03,  ..., -6.3129e-03,\n",
      "         -5.5477e-03,  1.6582e-03],\n",
      "        [ 4.5499e-03,  4.0890e-03, -1.3424e-03,  ...,  9.6658e-04,\n",
      "         -1.0667e-02, -5.6284e-03],\n",
      "        [-9.5147e-03, -2.8654e-03, -6.1126e-06,  ..., -1.9828e-03,\n",
      "         -1.7464e-03,  2.1355e-03],\n",
      "        ...,\n",
      "        [ 1.5033e-02,  2.2018e-03,  5.0893e-04,  ...,  1.0355e-03,\n",
      "         -8.8419e-04,  3.3655e-05],\n",
      "        [ 7.2919e-03, -1.3538e-03,  2.2380e-03,  ..., -2.0116e-03,\n",
      "          5.6815e-03,  1.4981e-03],\n",
      "        [ 7.2879e-03, -1.1236e-03,  1.4502e-03,  ..., -3.1881e-03,\n",
      "         -1.1107e-02,  1.4010e-03]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight tensor([[ 2.5999e-03, -6.4927e-03,  2.1448e-03,  ..., -1.6348e-03,\n",
      "          4.5690e-04, -2.3228e-04],\n",
      "        [ 6.4881e-05, -8.8545e-05, -9.4120e-04,  ..., -4.2499e-04,\n",
      "         -3.0710e-04,  2.3408e-04],\n",
      "        [ 1.5155e-03, -4.0195e-03,  6.4046e-03,  ...,  1.6262e-03,\n",
      "         -4.2544e-03,  2.4891e-03],\n",
      "        ...,\n",
      "        [-6.2651e-04,  2.5654e-03,  2.5477e-03,  ...,  1.1180e-03,\n",
      "          3.5658e-04, -3.3944e-03],\n",
      "        [-3.8668e-03, -1.5030e-03, -2.5912e-03,  ..., -5.3578e-04,\n",
      "         -7.5018e-03, -5.7011e-03],\n",
      "        [ 1.8309e-03, -1.2938e-03,  2.8734e-04,  ..., -1.5817e-03,\n",
      "          3.2682e-03, -2.2197e-03]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight tensor([[-0.0062, -0.0070, -0.0062,  ...,  0.0018,  0.0031, -0.0113],\n",
      "        [-0.0027, -0.0073,  0.0074,  ...,  0.0061, -0.0009, -0.0032],\n",
      "        [ 0.0001,  0.0081,  0.0024,  ..., -0.0063,  0.0026, -0.0005],\n",
      "        ...,\n",
      "        [ 0.0018, -0.0012, -0.0033,  ...,  0.0125, -0.0037,  0.0147],\n",
      "        [-0.0071, -0.0040, -0.0022,  ..., -0.0198,  0.0113, -0.0031],\n",
      "        [ 0.0028, -0.0020,  0.0071,  ...,  0.0063,  0.0045, -0.0091]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight tensor([[-1.0443e-02, -1.1317e-02,  1.4464e-02,  ..., -1.1444e-02,\n",
      "          4.6047e-03,  3.0159e-03],\n",
      "        [ 3.9232e-03,  1.7009e-03, -4.8812e-04,  ...,  1.8019e-03,\n",
      "          3.1183e-07,  1.6324e-03],\n",
      "        [-9.9956e-03, -4.6083e-03, -1.1011e-02,  ...,  1.6406e-02,\n",
      "          1.2620e-02, -2.2068e-03],\n",
      "        ...,\n",
      "        [-1.0034e-02, -2.1156e-02,  2.4528e-02,  ..., -5.7122e-03,\n",
      "         -1.2957e-02,  2.8302e-03],\n",
      "        [-9.4055e-03, -1.6691e-02,  4.2650e-03,  ...,  7.4773e-02,\n",
      "          5.2773e-03,  2.7927e-02],\n",
      "        [-1.5593e-02, -1.5749e-02,  3.7109e-03,  ...,  8.6281e-03,\n",
      "         -1.0602e-03,  2.4194e-03]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight tensor([[ 2.4530e-02,  5.4597e-03,  4.3652e-03,  ...,  7.6302e-03,\n",
      "          2.0237e-03,  8.2038e-03],\n",
      "        [-1.2212e-02,  5.4459e-04, -7.9642e-03,  ..., -1.4327e-03,\n",
      "         -6.5477e-05, -2.4867e-03],\n",
      "        [-1.1038e-02,  7.2463e-03, -3.0941e-03,  ..., -1.5992e-03,\n",
      "          6.9425e-03, -1.9975e-02],\n",
      "        ...,\n",
      "        [-8.0334e-03, -5.4120e-03,  4.9689e-03,  ..., -1.6479e-02,\n",
      "         -5.5123e-03, -3.1745e-03],\n",
      "        [-2.4546e-03, -4.7437e-03,  7.9488e-04,  ..., -2.9228e-03,\n",
      "         -1.0854e-02,  7.1632e-03],\n",
      "        [-7.6806e-04, -5.2572e-03,  2.9609e-03,  ..., -3.7547e-03,\n",
      "         -1.0311e-03, -1.3879e-04]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight tensor([[-0.0113,  0.0045, -0.0009,  ...,  0.0066,  0.0030,  0.0056],\n",
      "        [-0.0019,  0.0005, -0.0021,  ..., -0.0014, -0.0012,  0.0020],\n",
      "        [-0.0039, -0.0037, -0.0094,  ..., -0.0030,  0.0067,  0.0060],\n",
      "        ...,\n",
      "        [ 0.0018, -0.0036,  0.0032,  ..., -0.0027,  0.0027,  0.0014],\n",
      "        [ 0.0101,  0.0195, -0.0042,  ...,  0.0119, -0.0119, -0.0185],\n",
      "        [ 0.0029, -0.0045,  0.0038,  ...,  0.0092, -0.0107,  0.0033]])\n",
      "unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight tensor([[-0.0163, -0.0523, -0.0055,  ..., -0.0216,  0.0298, -0.0054],\n",
      "        [ 0.0057,  0.0224, -0.0138,  ..., -0.0007, -0.0165,  0.0091],\n",
      "        [ 0.0003,  0.0269,  0.0199,  ..., -0.0119, -0.0055, -0.0389],\n",
      "        ...,\n",
      "        [-0.0140,  0.0289,  0.0150,  ...,  0.0035, -0.0303, -0.0126],\n",
      "        [ 0.0026,  0.0085,  0.0351,  ...,  0.0274, -0.0014, -0.0207],\n",
      "        [ 0.0143,  0.0436, -0.0287,  ...,  0.0254,  0.0008, -0.0100]])\n",
      "unet.up_blocks.1.attentions.2.proj_in.lora.down.weight tensor([[[[ 0.0032]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[ 0.0066]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0002]],\n",
      "\n",
      "         [[ 0.0097]],\n",
      "\n",
      "         [[-0.0155]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0026]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         [[-0.0001]]],\n",
      "\n",
      "\n",
      "        [[[-0.0033]],\n",
      "\n",
      "         [[ 0.0066]],\n",
      "\n",
      "         [[-0.0005]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0091]],\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         [[ 0.0113]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0024]],\n",
      "\n",
      "         [[-0.0072]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0065]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[ 0.0037]]],\n",
      "\n",
      "\n",
      "        [[[-0.0021]],\n",
      "\n",
      "         [[-0.0111]],\n",
      "\n",
      "         [[-0.0307]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0180]],\n",
      "\n",
      "         [[-0.0097]],\n",
      "\n",
      "         [[ 0.0155]]],\n",
      "\n",
      "\n",
      "        [[[-0.0067]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0023]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         [[-0.0003]]]])\n",
      "unet.up_blocks.1.attentions.2.proj_in.lora.up.weight tensor([[[[-4.0847e-03]],\n",
      "\n",
      "         [[-1.0622e-02]],\n",
      "\n",
      "         [[-5.2075e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.6703e-03]],\n",
      "\n",
      "         [[ 8.2085e-03]],\n",
      "\n",
      "         [[ 2.0030e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 8.2769e-03]],\n",
      "\n",
      "         [[ 6.6260e-03]],\n",
      "\n",
      "         [[-1.0670e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4341e-02]],\n",
      "\n",
      "         [[-4.1229e-03]],\n",
      "\n",
      "         [[-1.1380e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.2777e-05]],\n",
      "\n",
      "         [[-1.1589e-02]],\n",
      "\n",
      "         [[-1.0301e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.3485e-03]],\n",
      "\n",
      "         [[ 1.9340e-02]],\n",
      "\n",
      "         [[ 6.8144e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.2270e-03]],\n",
      "\n",
      "         [[ 1.1570e-02]],\n",
      "\n",
      "         [[ 1.2436e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4496e-03]],\n",
      "\n",
      "         [[ 8.6815e-05]],\n",
      "\n",
      "         [[ 1.6021e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4051e-02]],\n",
      "\n",
      "         [[ 1.0768e-02]],\n",
      "\n",
      "         [[-1.4109e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2170e-02]],\n",
      "\n",
      "         [[-1.2420e-02]],\n",
      "\n",
      "         [[ 7.3214e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 9.9667e-03]],\n",
      "\n",
      "         [[-8.6488e-03]],\n",
      "\n",
      "         [[-3.3548e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3546e-02]],\n",
      "\n",
      "         [[-8.7962e-03]],\n",
      "\n",
      "         [[-1.2889e-02]]]])\n",
      "unet.up_blocks.1.attentions.2.proj_out.lora.down.weight tensor([[[[ 0.0080]],\n",
      "\n",
      "         [[ 0.0253]],\n",
      "\n",
      "         [[-0.0067]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0047]],\n",
      "\n",
      "         [[-0.0115]],\n",
      "\n",
      "         [[-0.0113]]],\n",
      "\n",
      "\n",
      "        [[[-0.0009]],\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         [[-0.0038]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0041]],\n",
      "\n",
      "         [[ 0.0021]],\n",
      "\n",
      "         [[ 0.0001]]],\n",
      "\n",
      "\n",
      "        [[[-0.0068]],\n",
      "\n",
      "         [[ 0.0079]],\n",
      "\n",
      "         [[-0.0106]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0167]],\n",
      "\n",
      "         [[-0.0029]],\n",
      "\n",
      "         [[-0.0086]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0068]],\n",
      "\n",
      "         [[ 0.0030]],\n",
      "\n",
      "         [[-0.0103]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0089]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[ 0.0004]]],\n",
      "\n",
      "\n",
      "        [[[-0.0021]],\n",
      "\n",
      "         [[ 0.0114]],\n",
      "\n",
      "         [[-0.0401]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0378]],\n",
      "\n",
      "         [[-0.0251]],\n",
      "\n",
      "         [[-0.0127]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0008]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         [[ 0.0016]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0093]],\n",
      "\n",
      "         [[-0.0065]],\n",
      "\n",
      "         [[ 0.0026]]]])\n",
      "unet.up_blocks.1.attentions.2.proj_out.lora.up.weight tensor([[[[ 1.2261e-03]],\n",
      "\n",
      "         [[-5.2648e-03]],\n",
      "\n",
      "         [[-2.8793e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.6925e-03]],\n",
      "\n",
      "         [[-1.0950e-02]],\n",
      "\n",
      "         [[-4.4632e-03]]],\n",
      "\n",
      "\n",
      "        [[[-9.6809e-03]],\n",
      "\n",
      "         [[ 2.5453e-03]],\n",
      "\n",
      "         [[-3.5502e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.9005e-03]],\n",
      "\n",
      "         [[ 6.2793e-03]],\n",
      "\n",
      "         [[-8.1507e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0304e-02]],\n",
      "\n",
      "         [[ 3.6324e-02]],\n",
      "\n",
      "         [[-2.5758e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2901e-02]],\n",
      "\n",
      "         [[-7.5059e-03]],\n",
      "\n",
      "         [[-2.2103e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.1133e-02]],\n",
      "\n",
      "         [[ 1.3668e-02]],\n",
      "\n",
      "         [[ 2.0667e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.0119e-03]],\n",
      "\n",
      "         [[-3.2841e-03]],\n",
      "\n",
      "         [[ 6.5095e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5505e-02]],\n",
      "\n",
      "         [[ 2.3954e-02]],\n",
      "\n",
      "         [[-2.0317e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.9737e-04]],\n",
      "\n",
      "         [[ 5.3483e-03]],\n",
      "\n",
      "         [[-5.0596e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 8.8962e-03]],\n",
      "\n",
      "         [[-4.3976e-03]],\n",
      "\n",
      "         [[ 1.2435e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7200e-02]],\n",
      "\n",
      "         [[ 2.6212e-03]],\n",
      "\n",
      "         [[ 9.4935e-05]]]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight tensor([[-2.5552e-03, -8.2721e-03, -1.3982e-04,  ...,  8.0196e-03,\n",
      "          2.4026e-03,  6.5403e-03],\n",
      "        [-1.6279e-04,  6.5840e-04, -4.0714e-04,  ..., -4.7559e-04,\n",
      "          1.7304e-05, -1.0460e-03],\n",
      "        [ 3.2488e-03,  4.4530e-03, -2.2062e-04,  ..., -1.0452e-03,\n",
      "         -1.6379e-03,  2.9968e-03],\n",
      "        ...,\n",
      "        [-5.3035e-03,  8.6334e-04,  6.1019e-03,  ...,  2.7395e-04,\n",
      "          5.9404e-03,  5.0360e-03],\n",
      "        [-1.5564e-02,  1.9723e-03, -8.8220e-03,  ..., -1.4355e-02,\n",
      "          4.6762e-03,  1.0137e-02],\n",
      "        [ 6.4771e-04,  2.1998e-03, -2.3346e-03,  ...,  3.0182e-04,\n",
      "         -8.6102e-04,  1.4151e-03]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight tensor([[-0.0128, -0.0010, -0.0095,  ..., -0.0075,  0.0127, -0.0019],\n",
      "        [-0.0072,  0.0004, -0.0030,  ...,  0.0155,  0.0017, -0.0077],\n",
      "        [-0.0174, -0.0036,  0.0032,  ...,  0.0018, -0.0034, -0.0085],\n",
      "        ...,\n",
      "        [ 0.0050, -0.0058,  0.0022,  ..., -0.0119, -0.0008, -0.0080],\n",
      "        [ 0.0146, -0.0013,  0.0162,  ...,  0.0254, -0.0034, -0.0051],\n",
      "        [ 0.0107,  0.0022,  0.0024,  ...,  0.0087, -0.0071,  0.0031]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight tensor([[-0.0131, -0.0018,  0.0013,  ..., -0.0030, -0.0137, -0.0065],\n",
      "        [ 0.0006,  0.0005, -0.0017,  ..., -0.0005, -0.0020, -0.0004],\n",
      "        [ 0.0003, -0.0006, -0.0046,  ..., -0.0054, -0.0103,  0.0061],\n",
      "        ...,\n",
      "        [-0.0032,  0.0014, -0.0040,  ..., -0.0038, -0.0071,  0.0013],\n",
      "        [-0.0199,  0.0030, -0.0149,  ..., -0.0113,  0.0136, -0.0088],\n",
      "        [ 0.0038, -0.0022,  0.0001,  ..., -0.0040, -0.0054,  0.0032]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight tensor([[-0.0020,  0.0077, -0.0077,  ...,  0.0100, -0.0068,  0.0073],\n",
      "        [ 0.0021,  0.0057,  0.0114,  ..., -0.0075,  0.0151,  0.0091],\n",
      "        [-0.0071, -0.0081, -0.0136,  ..., -0.0104,  0.0079,  0.0036],\n",
      "        ...,\n",
      "        [ 0.0011, -0.0036, -0.0064,  ..., -0.0004, -0.0018,  0.0105],\n",
      "        [ 0.0093,  0.0091,  0.0125,  ...,  0.0082, -0.0070, -0.0007],\n",
      "        [-0.0103,  0.0012, -0.0005,  ...,  0.0022,  0.0112, -0.0116]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight tensor([[-6.5876e-03,  6.6804e-03,  1.1117e-03,  ..., -5.6729e-03,\n",
      "         -2.0235e-04, -3.4314e-03],\n",
      "        [-5.9557e-04, -4.9320e-04, -9.7478e-04,  ...,  1.5696e-03,\n",
      "          7.8502e-04,  6.6406e-05],\n",
      "        [ 8.2907e-04,  2.5426e-03, -2.4301e-03,  ...,  2.4310e-03,\n",
      "         -1.8805e-03, -1.8635e-03],\n",
      "        ...,\n",
      "        [-2.8594e-03, -6.7512e-03, -4.4441e-03,  ..., -5.6941e-03,\n",
      "         -9.2654e-03,  4.1958e-03],\n",
      "        [ 2.3588e-02, -3.2120e-02, -5.5638e-03,  ...,  1.8479e-02,\n",
      "         -8.9412e-03, -5.4314e-03],\n",
      "        [ 2.8676e-03,  3.3051e-04,  3.2419e-03,  ...,  1.4894e-03,\n",
      "          2.5751e-03,  1.8947e-03]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight tensor([[-8.8012e-03, -4.2580e-03, -2.0101e-03,  ...,  2.0225e-02,\n",
      "          3.3484e-03,  2.2496e-03],\n",
      "        [ 2.4626e-03, -2.6511e-04, -4.5653e-03,  ..., -1.1799e-03,\n",
      "          3.5586e-02, -6.6313e-03],\n",
      "        [-6.1930e-03, -5.1253e-03,  9.8299e-03,  ...,  2.2537e-03,\n",
      "         -2.0756e-02,  5.9021e-03],\n",
      "        ...,\n",
      "        [ 2.2048e-03,  1.9024e-04,  1.6128e-03,  ...,  1.4085e-02,\n",
      "         -2.4762e-03,  5.3547e-03],\n",
      "        [ 7.8111e-03,  4.1567e-03,  6.5918e-03,  ...,  6.7823e-03,\n",
      "         -2.8355e-02,  7.9132e-03],\n",
      "        [-9.8468e-03,  4.4175e-05, -5.8124e-03,  ...,  8.0223e-03,\n",
      "          2.1477e-03,  3.9906e-03]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight tensor([[-7.4068e-04, -1.2592e-03, -2.9576e-03,  ..., -3.3317e-03,\n",
      "          5.5781e-03, -8.3471e-03],\n",
      "        [ 1.5274e-03,  5.5456e-04, -1.8254e-04,  ..., -2.2305e-04,\n",
      "          3.7361e-04,  1.1562e-03],\n",
      "        [-5.1470e-03, -1.6634e-03,  4.0840e-03,  ...,  5.2282e-04,\n",
      "         -6.1519e-03,  3.7638e-03],\n",
      "        ...,\n",
      "        [-2.6969e-03, -3.3621e-03,  1.3322e-04,  ..., -2.1566e-05,\n",
      "          3.7655e-03, -2.3727e-03],\n",
      "        [-9.5506e-03, -4.9821e-03,  8.7926e-03,  ..., -1.5502e-02,\n",
      "         -3.1625e-03,  7.1546e-04],\n",
      "        [-2.6518e-03, -4.7934e-03,  1.4137e-04,  ..., -1.1586e-02,\n",
      "         -1.8599e-03, -9.8980e-04]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight tensor([[-1.3521e-02, -5.2566e-03,  8.2834e-05,  ...,  3.9700e-03,\n",
      "         -2.4714e-02,  5.6425e-03],\n",
      "        [ 5.3647e-03, -3.9780e-03,  1.1363e-03,  ..., -2.6274e-03,\n",
      "          1.1859e-02, -1.5207e-03],\n",
      "        [-4.6608e-03,  7.6118e-03, -8.2130e-03,  ...,  8.3805e-03,\n",
      "          5.7905e-03, -1.4018e-03],\n",
      "        ...,\n",
      "        [-2.8080e-03, -5.2501e-03,  1.5291e-03,  ..., -2.8932e-03,\n",
      "          1.0336e-02, -8.6514e-03],\n",
      "        [-3.0098e-03,  3.7082e-03, -1.8333e-02,  ..., -5.5763e-03,\n",
      "         -1.6010e-03,  8.5177e-03],\n",
      "        [-7.1441e-03,  5.8024e-03, -1.6728e-02,  ..., -6.7664e-03,\n",
      "          4.5467e-03,  8.1397e-04]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight tensor([[-6.5476e-04,  8.9998e-04,  2.5039e-04,  ...,  1.1623e-03,\n",
      "         -3.1894e-03, -3.2538e-03],\n",
      "        [-4.1946e-06,  7.3533e-04,  5.9157e-04,  ...,  2.5544e-04,\n",
      "         -9.4377e-04, -4.5053e-04],\n",
      "        [ 1.8821e-03,  6.4294e-03,  3.7526e-03,  ...,  3.7664e-03,\n",
      "          2.4049e-03,  1.9420e-03],\n",
      "        ...,\n",
      "        [-4.1350e-03, -2.8443e-04, -1.2010e-03,  ...,  3.9223e-04,\n",
      "         -2.8052e-04, -8.4888e-04],\n",
      "        [ 1.6529e-03, -1.0288e-02, -7.3387e-03,  ...,  1.2477e-02,\n",
      "         -1.7880e-02, -3.5922e-03],\n",
      "        [ 3.2693e-03,  2.6272e-04, -2.3779e-03,  ...,  4.3224e-03,\n",
      "          2.2861e-03, -1.5186e-03]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight tensor([[-1.7103e-03,  3.5235e-03,  3.7541e-03,  ...,  2.4146e-03,\n",
      "         -1.7023e-03,  1.2110e-03],\n",
      "        [ 2.2403e-05,  1.0194e-03,  3.8790e-03,  ...,  5.6143e-03,\n",
      "          1.1421e-03,  2.2163e-03],\n",
      "        [ 3.0510e-03, -6.8961e-04,  1.2214e-03,  ...,  1.8595e-03,\n",
      "         -8.1495e-04,  3.4779e-03],\n",
      "        ...,\n",
      "        [ 4.7084e-05,  6.1929e-05, -4.0608e-03,  ...,  1.6322e-03,\n",
      "          3.6949e-03,  5.6440e-04],\n",
      "        [-1.0823e-03, -8.9679e-04, -2.5372e-03,  ..., -2.7243e-03,\n",
      "          1.2713e-03,  1.8942e-03],\n",
      "        [-9.8324e-04, -2.6609e-03, -4.1390e-03,  ..., -1.8792e-03,\n",
      "          3.5590e-03, -5.3517e-04]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight tensor([[ 3.8062e-03, -1.1493e-03,  1.0937e-03,  ..., -1.2235e-02,\n",
      "          2.4536e-03,  8.6697e-04],\n",
      "        [-5.6811e-04,  2.1935e-05, -2.3923e-04,  ...,  9.3526e-04,\n",
      "          4.8245e-04,  9.3006e-04],\n",
      "        [ 5.6220e-04,  5.9909e-03,  5.5014e-03,  ...,  5.4602e-03,\n",
      "          3.1678e-03, -2.2290e-04],\n",
      "        ...,\n",
      "        [ 1.8833e-03, -2.0331e-03,  1.0762e-03,  ..., -8.2866e-04,\n",
      "          2.6322e-03,  2.7645e-04],\n",
      "        [ 9.3213e-03,  6.9504e-03,  1.3377e-02,  ..., -1.7914e-02,\n",
      "         -1.5344e-02,  1.8683e-03],\n",
      "        [-2.5371e-03, -2.8033e-03, -8.2367e-05,  ...,  3.1020e-03,\n",
      "          4.8189e-05,  1.6437e-03]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight tensor([[ 1.5019e-02,  3.7960e-03,  6.8748e-03,  ..., -8.9889e-06,\n",
      "         -2.9306e-03, -2.2784e-03],\n",
      "        [ 1.0662e-02, -5.0301e-03, -4.5806e-03,  ...,  6.6586e-03,\n",
      "          5.2075e-04,  1.1672e-03],\n",
      "        [ 6.1147e-03, -4.1000e-04, -1.3508e-02,  ..., -3.2415e-03,\n",
      "         -1.6054e-03,  6.0155e-03],\n",
      "        ...,\n",
      "        [-9.3519e-03,  5.2042e-03,  6.8908e-04,  ..., -5.6371e-04,\n",
      "          2.2591e-03,  5.0544e-03],\n",
      "        [ 7.3921e-03,  1.7795e-03, -1.4213e-02,  ...,  3.8239e-03,\n",
      "          5.4411e-03, -4.3132e-03],\n",
      "        [-9.3384e-03,  2.8627e-03,  1.8107e-03,  ...,  5.7946e-04,\n",
      "          6.0392e-03, -2.4269e-03]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight tensor([[-3.1216e-03,  3.4629e-03,  2.3633e-03,  ..., -8.6555e-04,\n",
      "          2.3628e-03,  4.2303e-03],\n",
      "        [ 1.3434e-03, -2.0690e-04, -1.5730e-04,  ...,  9.9803e-05,\n",
      "          5.5350e-04,  6.6810e-04],\n",
      "        [ 1.3347e-04,  3.3021e-03,  7.9021e-04,  ..., -4.8315e-05,\n",
      "         -1.2941e-03,  2.5379e-03],\n",
      "        ...,\n",
      "        [ 1.1693e-03,  8.0387e-04, -3.3585e-03,  ..., -1.0377e-03,\n",
      "          2.6211e-03, -1.6627e-03],\n",
      "        [-8.9746e-03,  7.8094e-04, -3.2988e-03,  ..., -1.2362e-02,\n",
      "         -1.1313e-02,  1.3179e-02],\n",
      "        [-1.1181e-03, -1.2997e-03, -4.7508e-05,  ..., -1.3720e-03,\n",
      "         -1.2047e-04,  2.2320e-03]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight tensor([[-5.5240e-03, -1.3351e-03,  2.1352e-03,  ..., -4.3374e-03,\n",
      "         -3.2530e-03, -8.3595e-05],\n",
      "        [-1.1267e-03,  4.7684e-05,  5.2374e-03,  ..., -1.0360e-03,\n",
      "          6.5220e-04,  1.3042e-03],\n",
      "        [-4.3942e-03, -2.6190e-03, -5.1245e-03,  ..., -2.9900e-04,\n",
      "          6.4661e-03,  4.8695e-04],\n",
      "        ...,\n",
      "        [ 3.1249e-03, -3.3106e-04, -1.4800e-03,  ..., -3.6560e-03,\n",
      "         -4.7462e-03, -4.2050e-05],\n",
      "        [ 7.4008e-03,  7.4791e-04, -1.4430e-03,  ...,  3.6129e-03,\n",
      "         -9.4434e-03, -1.2908e-03],\n",
      "        [-4.6113e-03,  1.6097e-03,  1.3643e-03,  ..., -1.4854e-03,\n",
      "          2.6099e-03, -6.1185e-04]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight tensor([[ 1.3338e-03,  9.3597e-04,  1.8619e-03,  ..., -3.7684e-04,\n",
      "          3.9421e-03,  7.5562e-04],\n",
      "        [ 3.4214e-04,  7.4973e-04,  3.7320e-04,  ..., -3.9194e-04,\n",
      "         -6.3208e-04, -7.7511e-04],\n",
      "        [ 4.6821e-04,  1.0918e-03, -1.2991e-04,  ...,  1.8053e-03,\n",
      "          5.7801e-04,  2.7390e-03],\n",
      "        ...,\n",
      "        [-2.5832e-04,  3.6196e-04, -1.2734e-04,  ...,  2.0647e-03,\n",
      "         -1.4613e-03, -2.1848e-03],\n",
      "        [-6.5984e-03, -2.9223e-02, -7.9335e-03,  ..., -7.9121e-04,\n",
      "         -8.1950e-03, -5.4975e-03],\n",
      "        [ 1.6734e-03, -2.9656e-03, -3.0866e-03,  ..., -9.9762e-06,\n",
      "         -5.1141e-04, -2.2674e-04]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight tensor([[ 3.1101e-03,  1.0766e-03,  5.3525e-04,  ..., -8.3977e-05,\n",
      "         -3.3776e-03, -2.1481e-03],\n",
      "        [ 2.7015e-03, -2.0734e-03,  6.2003e-03,  ...,  3.1092e-03,\n",
      "          7.7014e-03, -7.9213e-03],\n",
      "        [-2.1490e-03, -2.4573e-03,  5.8028e-03,  ...,  3.9264e-03,\n",
      "         -1.4949e-03,  5.1256e-03],\n",
      "        ...,\n",
      "        [-1.0644e-03,  2.9242e-03,  2.0324e-03,  ...,  9.2029e-03,\n",
      "         -1.8442e-03,  6.7368e-03],\n",
      "        [ 5.3472e-03, -1.2216e-03,  9.7018e-03,  ..., -4.0042e-04,\n",
      "         -6.2596e-03, -6.2586e-03],\n",
      "        [ 2.4485e-03, -1.2513e-02,  1.9024e-03,  ..., -3.5470e-03,\n",
      "         -3.6130e-03,  1.5950e-03]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.down.weight tensor([[ 0.0012,  0.0136,  0.0009,  ...,  0.0172,  0.0084, -0.0083],\n",
      "        [-0.0011,  0.0034,  0.0009,  ..., -0.0002,  0.0013, -0.0012],\n",
      "        [-0.0036,  0.0257,  0.0059,  ...,  0.0135,  0.0131, -0.0144],\n",
      "        ...,\n",
      "        [ 0.0100,  0.0020,  0.0022,  ...,  0.0021,  0.0020, -0.0109],\n",
      "        [-0.0170, -0.0048, -0.0072,  ...,  0.0292,  0.0075, -0.0353],\n",
      "        [-0.0103, -0.0030, -0.0031,  ..., -0.0042, -0.0020,  0.0047]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.up.weight tensor([[-7.5227e-03,  9.8086e-03, -1.9822e-02,  ..., -6.6350e-03,\n",
      "         -4.1031e-03, -8.5627e-03],\n",
      "        [-7.6180e-03,  7.4284e-03, -7.0328e-03,  ...,  9.4195e-04,\n",
      "         -4.4491e-03, -4.7140e-04],\n",
      "        [-4.1146e-03,  1.7523e-03,  3.3409e-03,  ..., -4.1162e-03,\n",
      "         -2.3639e-03, -4.0591e-03],\n",
      "        ...,\n",
      "        [ 1.3876e-02, -5.0067e-05, -1.8590e-03,  ..., -2.6007e-05,\n",
      "         -1.4032e-04,  3.9330e-03],\n",
      "        [-6.7881e-02,  1.5841e-02,  1.1399e-02,  ..., -4.9545e-02,\n",
      "         -9.1475e-02, -9.6641e-02],\n",
      "        [ 2.9681e-03, -1.4588e-03,  1.9327e-02,  ..., -4.9692e-03,\n",
      "         -6.0627e-03, -4.8431e-03]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.lora.down.weight tensor([[-2.1751e-03, -1.0186e-02,  6.6952e-03,  ..., -4.9137e-03,\n",
      "         -2.3623e-02,  2.3128e-03],\n",
      "        [ 2.1611e-03,  7.0980e-04,  1.5915e-03,  ..., -1.2468e-03,\n",
      "          3.1002e-03,  1.3136e-04],\n",
      "        [ 2.7198e-03,  8.2685e-03,  4.3383e-03,  ..., -8.3469e-03,\n",
      "         -6.4805e-03, -9.6282e-03],\n",
      "        ...,\n",
      "        [-1.8040e-03, -7.2504e-03, -1.8273e-03,  ...,  4.9076e-03,\n",
      "         -4.4898e-02, -3.2013e-04],\n",
      "        [ 1.5459e-02, -1.1376e-02,  1.0984e-02,  ..., -1.1271e-02,\n",
      "         -8.9831e-02,  7.4465e-03],\n",
      "        [-6.9294e-03,  1.7087e-05, -3.4050e-03,  ...,  6.9586e-03,\n",
      "          1.1239e-02,  7.2272e-04]])\n",
      "unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.lora.up.weight tensor([[-0.0193, -0.0104,  0.0173,  ...,  0.0071,  0.0044, -0.0156],\n",
      "        [-0.0103,  0.0104, -0.0112,  ..., -0.0109,  0.0127,  0.0165],\n",
      "        [-0.0181, -0.0100,  0.0020,  ..., -0.0014, -0.0105, -0.0020],\n",
      "        ...,\n",
      "        [-0.0001, -0.0012, -0.0254,  ...,  0.0185, -0.0032,  0.0088],\n",
      "        [ 0.0094,  0.0025, -0.0074,  ..., -0.0040,  0.0256,  0.0022],\n",
      "        [-0.0012, -0.0074, -0.0015,  ...,  0.0025, -0.0141,  0.0168]])\n",
      "unet.up_blocks.2.attentions.0.proj_in.lora.down.weight tensor([[[[ 0.0021]],\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         [[ 0.0036]],\n",
      "\n",
      "         [[ 0.0054]]],\n",
      "\n",
      "\n",
      "        [[[-0.0009]],\n",
      "\n",
      "         [[-0.0029]],\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[-0.0005]],\n",
      "\n",
      "         [[ 0.0015]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0059]],\n",
      "\n",
      "         [[ 0.0007]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0059]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[-0.0049]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0004]],\n",
      "\n",
      "         [[-0.0045]],\n",
      "\n",
      "         [[ 0.0050]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         [[ 0.0012]]],\n",
      "\n",
      "\n",
      "        [[[-0.0071]],\n",
      "\n",
      "         [[-0.0195]],\n",
      "\n",
      "         [[-0.0114]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0171]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[-0.0092]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0005]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         [[ 0.0023]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         [[-0.0082]]]])\n",
      "unet.up_blocks.2.attentions.0.proj_in.lora.up.weight tensor([[[[ 1.9808e-03]],\n",
      "\n",
      "         [[-1.0490e-04]],\n",
      "\n",
      "         [[-5.2898e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5450e-03]],\n",
      "\n",
      "         [[ 8.0303e-03]],\n",
      "\n",
      "         [[ 6.2732e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 3.2847e-03]],\n",
      "\n",
      "         [[ 3.3707e-03]],\n",
      "\n",
      "         [[ 1.0252e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.7739e-03]],\n",
      "\n",
      "         [[-6.7410e-03]],\n",
      "\n",
      "         [[-1.6066e-03]]],\n",
      "\n",
      "\n",
      "        [[[-5.7984e-03]],\n",
      "\n",
      "         [[ 2.0624e-03]],\n",
      "\n",
      "         [[ 1.1180e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.8924e-04]],\n",
      "\n",
      "         [[-2.4905e-03]],\n",
      "\n",
      "         [[-6.4852e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-7.2571e-05]],\n",
      "\n",
      "         [[-2.4276e-03]],\n",
      "\n",
      "         [[ 8.2450e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.5758e-03]],\n",
      "\n",
      "         [[-5.4426e-03]],\n",
      "\n",
      "         [[ 4.8368e-05]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5768e-04]],\n",
      "\n",
      "         [[ 7.3250e-03]],\n",
      "\n",
      "         [[ 6.1540e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.9346e-03]],\n",
      "\n",
      "         [[ 1.3081e-02]],\n",
      "\n",
      "         [[ 3.5773e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2348e-02]],\n",
      "\n",
      "         [[-1.4097e-04]],\n",
      "\n",
      "         [[ 6.1836e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0348e-03]],\n",
      "\n",
      "         [[-5.6561e-03]],\n",
      "\n",
      "         [[ 7.8694e-03]]]])\n",
      "unet.up_blocks.2.attentions.0.proj_out.lora.down.weight tensor([[[[ 4.5261e-04]],\n",
      "\n",
      "         [[-1.0482e-03]],\n",
      "\n",
      "         [[ 3.1654e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.7646e-03]],\n",
      "\n",
      "         [[-1.5837e-03]],\n",
      "\n",
      "         [[-2.4947e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 7.4527e-04]],\n",
      "\n",
      "         [[ 6.0523e-04]],\n",
      "\n",
      "         [[-2.5180e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.7202e-03]],\n",
      "\n",
      "         [[-3.0298e-04]],\n",
      "\n",
      "         [[ 1.7662e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.8891e-03]],\n",
      "\n",
      "         [[ 9.1786e-03]],\n",
      "\n",
      "         [[ 4.3905e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2063e-03]],\n",
      "\n",
      "         [[-6.2326e-05]],\n",
      "\n",
      "         [[-3.0017e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.0465e-02]],\n",
      "\n",
      "         [[ 8.6961e-04]],\n",
      "\n",
      "         [[ 8.8727e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.8064e-03]],\n",
      "\n",
      "         [[ 3.4522e-03]],\n",
      "\n",
      "         [[-7.8769e-03]]],\n",
      "\n",
      "\n",
      "        [[[-5.4992e-03]],\n",
      "\n",
      "         [[-4.3752e-02]],\n",
      "\n",
      "         [[-2.6375e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.8454e-03]],\n",
      "\n",
      "         [[-5.7414e-03]],\n",
      "\n",
      "         [[ 1.3894e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.1852e-04]],\n",
      "\n",
      "         [[-6.3091e-03]],\n",
      "\n",
      "         [[-4.3095e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.1364e-04]],\n",
      "\n",
      "         [[-3.7074e-03]],\n",
      "\n",
      "         [[-4.9291e-05]]]])\n",
      "unet.up_blocks.2.attentions.0.proj_out.lora.up.weight tensor([[[[ 0.0105]],\n",
      "\n",
      "         [[ 0.0046]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0048]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[-0.0121]]],\n",
      "\n",
      "\n",
      "        [[[-0.0106]],\n",
      "\n",
      "         [[-0.0098]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[ 0.0047]]],\n",
      "\n",
      "\n",
      "        [[[-0.0044]],\n",
      "\n",
      "         [[ 0.0147]],\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[ 0.0036]],\n",
      "\n",
      "         [[-0.0015]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0005]],\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         [[-0.0079]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0040]],\n",
      "\n",
      "         [[ 0.0100]],\n",
      "\n",
      "         [[-0.0042]]],\n",
      "\n",
      "\n",
      "        [[[-0.0023]],\n",
      "\n",
      "         [[-0.0035]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[ 0.0070]],\n",
      "\n",
      "         [[-0.0044]]],\n",
      "\n",
      "\n",
      "        [[[-0.0021]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[-0.0030]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0054]],\n",
      "\n",
      "         [[ 0.0032]],\n",
      "\n",
      "         [[ 0.0063]]]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight tensor([[ 9.5124e-04,  5.6756e-03,  2.8262e-03,  ...,  1.6922e-03,\n",
      "          6.0454e-03, -3.8483e-03],\n",
      "        [-7.4510e-05, -6.6892e-04,  5.5990e-04,  ...,  2.8513e-04,\n",
      "         -7.9157e-04, -1.2223e-03],\n",
      "        [ 5.7397e-03, -3.0129e-03,  2.4398e-03,  ..., -4.2524e-03,\n",
      "         -1.4720e-03, -1.5844e-03],\n",
      "        ...,\n",
      "        [-1.9036e-03,  1.7418e-03, -3.7802e-03,  ...,  2.3114e-03,\n",
      "         -4.5894e-04, -3.2095e-03],\n",
      "        [ 6.3885e-03,  6.8753e-03, -1.0735e-02,  ...,  4.2798e-04,\n",
      "          9.1120e-03, -5.2784e-04],\n",
      "        [ 9.7866e-04, -1.9595e-03,  5.6402e-03,  ..., -2.1068e-03,\n",
      "         -2.3216e-03, -7.0143e-04]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight tensor([[-8.1533e-03,  2.6005e-05,  1.3727e-03,  ...,  7.8287e-03,\n",
      "          1.7004e-03,  3.1319e-03],\n",
      "        [ 8.3804e-03,  1.9652e-03, -4.4545e-03,  ...,  6.2585e-04,\n",
      "          1.3353e-02,  1.7682e-04],\n",
      "        [ 4.8141e-03,  1.4939e-03, -1.0258e-02,  ...,  5.9062e-03,\n",
      "         -8.5100e-03, -1.4654e-03],\n",
      "        ...,\n",
      "        [ 3.6862e-03, -1.5105e-03,  3.2238e-03,  ..., -6.9192e-03,\n",
      "          4.6648e-04,  3.9273e-03],\n",
      "        [ 8.5383e-03, -8.0796e-03, -7.4369e-04,  ..., -2.6945e-03,\n",
      "         -2.4115e-03,  3.8117e-04],\n",
      "        [-4.8097e-04, -5.4189e-03,  3.6823e-03,  ...,  6.5656e-03,\n",
      "          6.2739e-03, -4.0685e-03]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight tensor([[-5.4530e-04, -2.8254e-03,  5.6652e-03,  ...,  1.1296e-02,\n",
      "         -6.0266e-03,  7.7806e-04],\n",
      "        [-3.7764e-04,  4.2069e-04, -3.5639e-04,  ...,  1.6074e-04,\n",
      "         -6.7776e-05,  1.5933e-03],\n",
      "        [ 3.3434e-03, -1.5706e-03, -7.1982e-05,  ..., -3.6241e-03,\n",
      "          4.9839e-04,  3.0024e-03],\n",
      "        ...,\n",
      "        [-3.2471e-03, -3.2835e-03,  2.5963e-03,  ..., -4.7384e-03,\n",
      "          2.7194e-04,  2.5295e-03],\n",
      "        [ 6.9375e-03,  1.4563e-03,  7.8758e-03,  ..., -4.0409e-03,\n",
      "         -1.5098e-03, -1.8602e-03],\n",
      "        [-1.3815e-03,  1.3382e-03, -1.4718e-03,  ...,  4.5016e-03,\n",
      "         -2.9463e-03, -2.4797e-03]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight tensor([[ 0.0069,  0.0024, -0.0080,  ..., -0.0010, -0.0024, -0.0104],\n",
      "        [-0.0040,  0.0081, -0.0008,  ..., -0.0064, -0.0038,  0.0050],\n",
      "        [ 0.0066,  0.0065, -0.0070,  ...,  0.0114,  0.0039,  0.0085],\n",
      "        ...,\n",
      "        [ 0.0048, -0.0057,  0.0061,  ...,  0.0066, -0.0141, -0.0041],\n",
      "        [ 0.0014,  0.0008, -0.0048,  ...,  0.0031,  0.0042, -0.0029],\n",
      "        [ 0.0086,  0.0077, -0.0051,  ...,  0.0040, -0.0018,  0.0059]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight tensor([[-3.9793e-03,  9.9947e-03, -3.6236e-03,  ..., -1.3619e-03,\n",
      "         -7.9405e-05,  5.2186e-03],\n",
      "        [ 1.1293e-04, -9.9471e-04,  1.0275e-04,  ...,  2.3303e-04,\n",
      "         -5.1813e-05,  2.4258e-04],\n",
      "        [ 1.2103e-03,  1.3444e-03,  5.1473e-04,  ...,  1.1604e-03,\n",
      "          1.4595e-03, -6.9584e-03],\n",
      "        ...,\n",
      "        [ 3.2701e-03, -1.7437e-03, -7.2411e-04,  ...,  3.0353e-04,\n",
      "          5.0942e-04,  8.7194e-04],\n",
      "        [ 1.0162e-02,  5.9416e-03,  6.4483e-04,  ..., -2.1634e-02,\n",
      "          7.6896e-03,  7.1335e-03],\n",
      "        [-1.9902e-03, -3.1535e-04,  3.2012e-03,  ..., -1.8327e-03,\n",
      "         -5.5539e-03,  1.5910e-03]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight tensor([[-0.0004, -0.0016,  0.0015,  ..., -0.0032,  0.0144,  0.0028],\n",
      "        [ 0.0008,  0.0011, -0.0030,  ..., -0.0127, -0.0048,  0.0064],\n",
      "        [ 0.0031, -0.0088, -0.0005,  ...,  0.0084,  0.0028,  0.0002],\n",
      "        ...,\n",
      "        [ 0.0073, -0.0071, -0.0051,  ..., -0.0033,  0.0020,  0.0067],\n",
      "        [-0.0024, -0.0034,  0.0022,  ..., -0.0069,  0.0019,  0.0029],\n",
      "        [-0.0002, -0.0041, -0.0091,  ..., -0.0018, -0.0064, -0.0022]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight tensor([[ 3.2297e-03,  7.1871e-04,  4.4681e-03,  ...,  1.4115e-03,\n",
      "         -1.1886e-03, -2.7467e-03],\n",
      "        [-3.6742e-04,  8.9430e-05,  9.1077e-05,  ..., -1.8522e-04,\n",
      "          8.6676e-04, -1.0690e-04],\n",
      "        [ 5.8359e-04,  4.6221e-03,  1.1771e-03,  ...,  1.4827e-03,\n",
      "          1.0853e-03, -2.1818e-03],\n",
      "        ...,\n",
      "        [ 2.8275e-03,  3.6284e-03,  6.8273e-04,  ..., -5.2154e-03,\n",
      "          7.2324e-03, -3.3067e-03],\n",
      "        [-1.8667e-02,  8.3379e-03,  5.5138e-03,  ..., -5.1177e-03,\n",
      "          4.6816e-03, -9.2520e-03],\n",
      "        [ 6.0181e-05, -7.9833e-04, -1.4906e-03,  ..., -3.1682e-03,\n",
      "          3.9822e-03, -1.5611e-04]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight tensor([[-0.0113, -0.0008, -0.0030,  ..., -0.0050,  0.0106, -0.0056],\n",
      "        [ 0.0065,  0.0028,  0.0021,  ..., -0.0022,  0.0040,  0.0010],\n",
      "        [ 0.0017,  0.0052,  0.0001,  ..., -0.0036, -0.0003,  0.0043],\n",
      "        ...,\n",
      "        [-0.0002, -0.0014, -0.0053,  ...,  0.0042,  0.0066,  0.0005],\n",
      "        [-0.0065,  0.0058, -0.0041,  ..., -0.0008, -0.0023, -0.0045],\n",
      "        [-0.0012,  0.0062, -0.0075,  ..., -0.0119,  0.0015,  0.0044]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight tensor([[-7.6478e-03, -9.8399e-04,  2.9183e-03,  ..., -2.7077e-03,\n",
      "          4.9993e-04,  9.2420e-04],\n",
      "        [-1.1272e-03,  6.9931e-04, -3.6772e-04,  ...,  7.0285e-05,\n",
      "         -5.0663e-04, -1.1063e-04],\n",
      "        [ 1.1712e-03, -2.3454e-04, -9.9356e-04,  ...,  2.8354e-04,\n",
      "          9.3234e-04,  1.7006e-03],\n",
      "        ...,\n",
      "        [ 3.3306e-03,  1.8347e-03,  1.8911e-03,  ..., -1.2731e-03,\n",
      "         -5.8007e-04, -1.6256e-03],\n",
      "        [ 1.1041e-03,  3.6912e-03,  5.7732e-03,  ..., -5.3130e-03,\n",
      "         -1.3087e-03,  2.6902e-03],\n",
      "        [ 1.7317e-03, -2.2814e-04, -9.0159e-04,  ..., -9.6892e-04,\n",
      "          1.9928e-03,  9.9860e-04]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight tensor([[-1.4668e-04,  9.3848e-04,  4.7011e-04,  ...,  8.0149e-06,\n",
      "          1.7784e-03,  3.2960e-04],\n",
      "        [-1.0430e-04, -4.0805e-04,  1.9878e-04,  ...,  1.8932e-04,\n",
      "         -1.7919e-03, -5.9293e-04],\n",
      "        [-1.3573e-03,  1.6914e-04, -1.3577e-04,  ...,  4.0135e-04,\n",
      "          5.5383e-04,  9.6047e-04],\n",
      "        ...,\n",
      "        [-3.0584e-04,  1.0233e-03, -5.8246e-05,  ...,  5.1365e-04,\n",
      "          1.9298e-03, -5.1543e-05],\n",
      "        [ 6.1624e-04, -1.1869e-03,  1.9935e-04,  ...,  2.5650e-04,\n",
      "         -1.1196e-03, -2.6923e-04],\n",
      "        [ 1.4160e-04,  5.2983e-04, -7.9546e-04,  ...,  7.6105e-04,\n",
      "         -3.5495e-03, -5.1207e-04]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight tensor([[-7.7114e-04, -5.7087e-03,  7.2319e-04,  ...,  7.1181e-05,\n",
      "          2.0855e-03, -9.0526e-04],\n",
      "        [-5.6008e-04, -3.3020e-04,  1.1586e-04,  ..., -5.3535e-04,\n",
      "         -1.6025e-04, -2.4892e-04],\n",
      "        [ 9.5094e-04,  3.7771e-04,  9.0078e-04,  ...,  1.9157e-03,\n",
      "          2.1281e-03, -9.9776e-04],\n",
      "        ...,\n",
      "        [ 1.3714e-03, -1.2122e-03, -1.4352e-03,  ..., -3.6686e-04,\n",
      "         -3.6920e-04,  2.0197e-03],\n",
      "        [ 9.0108e-03,  1.6815e-03,  7.1931e-03,  ..., -5.1788e-03,\n",
      "          4.3494e-04, -2.0942e-03],\n",
      "        [ 3.2405e-04,  3.9593e-03, -9.2464e-04,  ..., -1.5647e-03,\n",
      "          1.2786e-03, -5.4308e-04]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight tensor([[ 2.4427e-03, -3.2469e-03,  1.9743e-03,  ...,  1.8834e-03,\n",
      "         -3.1304e-03,  4.7226e-04],\n",
      "        [-3.0650e-03,  3.6852e-03,  2.6078e-03,  ...,  7.7884e-04,\n",
      "          4.5360e-03,  2.9185e-04],\n",
      "        [-1.4074e-03,  4.0814e-03, -3.9619e-03,  ...,  1.3674e-03,\n",
      "          2.1401e-03,  1.3935e-03],\n",
      "        ...,\n",
      "        [-4.1373e-03,  3.0541e-03, -5.8452e-03,  ..., -4.8170e-04,\n",
      "         -3.7488e-03, -3.2517e-04],\n",
      "        [-3.7510e-03,  5.1559e-03,  7.0505e-04,  ..., -3.7212e-03,\n",
      "         -3.9467e-03, -3.8433e-03],\n",
      "        [ 4.6835e-04,  3.2791e-03, -1.4483e-06,  ..., -2.7418e-03,\n",
      "         -1.1646e-03,  2.4479e-03]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight tensor([[ 4.4919e-03, -1.7619e-04,  1.3648e-03,  ...,  2.2336e-03,\n",
      "         -2.1839e-05, -1.8383e-04],\n",
      "        [-1.8522e-04,  2.2256e-05, -3.8984e-04,  ..., -8.2174e-05,\n",
      "          4.1169e-04, -2.6404e-04],\n",
      "        [ 2.1906e-03, -6.1819e-04, -1.5439e-03,  ...,  2.7635e-03,\n",
      "          2.7405e-03,  2.9886e-04],\n",
      "        ...,\n",
      "        [-2.3836e-03,  1.4007e-03, -3.1391e-03,  ..., -1.6008e-04,\n",
      "         -1.1605e-03, -1.5877e-03],\n",
      "        [-7.9471e-03, -1.1073e-04, -3.7301e-03,  ..., -5.2535e-03,\n",
      "          5.8757e-03, -2.9366e-03],\n",
      "        [ 4.3999e-04, -2.9964e-03,  3.7728e-04,  ...,  1.3126e-03,\n",
      "          1.5302e-03, -1.0189e-03]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight tensor([[-1.8611e-03, -4.8055e-04,  5.7555e-04,  ..., -7.1805e-04,\n",
      "         -1.9734e-03, -3.3067e-04],\n",
      "        [-8.7081e-04,  5.7245e-04, -3.2252e-04,  ...,  4.0710e-04,\n",
      "          1.1171e-03,  4.3185e-04],\n",
      "        [-3.8598e-04, -1.7762e-04,  3.7802e-05,  ..., -1.1212e-04,\n",
      "         -3.6564e-03,  4.9143e-04],\n",
      "        ...,\n",
      "        [ 1.0891e-04, -1.7083e-04, -2.3224e-04,  ...,  4.8952e-04,\n",
      "         -9.1130e-04,  7.3670e-04],\n",
      "        [-7.9848e-05,  6.1596e-04, -7.9457e-04,  ...,  5.1024e-04,\n",
      "          1.7271e-03,  3.6210e-04],\n",
      "        [-5.6249e-04, -4.1258e-04, -6.7102e-04,  ..., -6.5279e-04,\n",
      "          1.7527e-03,  5.1182e-04]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight tensor([[ 2.4357e-03, -2.9865e-03, -2.0024e-03,  ..., -1.1310e-03,\n",
      "          8.3655e-04, -2.9745e-03],\n",
      "        [-4.4709e-04,  3.1785e-05,  5.4863e-04,  ..., -3.6160e-04,\n",
      "         -8.6078e-05,  5.5259e-04],\n",
      "        [ 3.7877e-04,  1.1464e-03, -1.7843e-04,  ...,  1.5352e-03,\n",
      "          1.1386e-03,  1.1941e-03],\n",
      "        ...,\n",
      "        [ 3.5928e-03, -6.6504e-04,  3.4769e-03,  ..., -2.3328e-03,\n",
      "          4.7071e-05, -3.1243e-04],\n",
      "        [ 2.4526e-03,  8.1502e-03, -3.9828e-03,  ...,  7.4503e-03,\n",
      "         -8.4576e-03, -1.3393e-02],\n",
      "        [-2.0158e-03,  1.2233e-03,  2.7115e-04,  ..., -4.6630e-04,\n",
      "          1.2322e-03, -5.3750e-04]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight tensor([[ 0.0011, -0.0039,  0.0018,  ...,  0.0049,  0.0046, -0.0031],\n",
      "        [ 0.0008, -0.0038,  0.0036,  ..., -0.0114, -0.0016,  0.0006],\n",
      "        [-0.0009,  0.0031, -0.0072,  ...,  0.0029,  0.0029,  0.0053],\n",
      "        ...,\n",
      "        [ 0.0022,  0.0078, -0.0091,  ..., -0.0054,  0.0028,  0.0006],\n",
      "        [-0.0013, -0.0032,  0.0086,  ...,  0.0071,  0.0029,  0.0021],\n",
      "        [-0.0060,  0.0004, -0.0008,  ..., -0.0027,  0.0032, -0.0015]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight tensor([[ 4.4811e-03, -3.9399e-03,  1.4887e-04,  ..., -3.6217e-03,\n",
      "          4.4082e-03, -1.3972e-03],\n",
      "        [ 1.3865e-04, -7.9593e-04, -2.7205e-04,  ..., -6.7032e-05,\n",
      "         -9.2251e-04, -2.2518e-04],\n",
      "        [ 3.3525e-03,  2.1445e-03,  4.5876e-04,  ...,  2.5277e-03,\n",
      "         -1.8875e-03, -3.3364e-03],\n",
      "        ...,\n",
      "        [-6.4933e-04, -7.7521e-04, -3.5589e-03,  ...,  7.5684e-04,\n",
      "          1.2113e-03,  1.8201e-03],\n",
      "        [ 4.8710e-03,  2.0087e-02,  4.2909e-03,  ..., -5.3890e-03,\n",
      "         -7.6342e-03, -2.9507e-03],\n",
      "        [ 2.1703e-04, -2.0922e-03, -3.4547e-03,  ..., -5.2920e-03,\n",
      "          1.7826e-03, -5.4133e-03]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight tensor([[ 0.0107,  0.0041, -0.0024,  ...,  0.0037,  0.0061,  0.0017],\n",
      "        [-0.0015, -0.0030,  0.0019,  ...,  0.0018, -0.0153,  0.0012],\n",
      "        [ 0.0017,  0.0106,  0.0054,  ...,  0.0016, -0.0035, -0.0061],\n",
      "        ...,\n",
      "        [ 0.0032,  0.0098,  0.0014,  ...,  0.0036, -0.0045,  0.0063],\n",
      "        [ 0.0054, -0.0032, -0.0087,  ..., -0.0060,  0.0030,  0.0006],\n",
      "        [ 0.0118,  0.0046,  0.0114,  ...,  0.0071,  0.0111, -0.0083]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight tensor([[ 0.0083, -0.0017,  0.0011,  ...,  0.0026, -0.0007,  0.0013],\n",
      "        [ 0.0010, -0.0006,  0.0004,  ...,  0.0007, -0.0018,  0.0020],\n",
      "        [-0.0005,  0.0034,  0.0003,  ...,  0.0010, -0.0001,  0.0010],\n",
      "        ...,\n",
      "        [ 0.0009, -0.0055, -0.0041,  ..., -0.0013, -0.0030, -0.0001],\n",
      "        [-0.0077, -0.0247, -0.0083,  ..., -0.0126,  0.0077, -0.0043],\n",
      "        [-0.0042,  0.0017, -0.0009,  ...,  0.0049,  0.0027,  0.0010]])\n",
      "unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight tensor([[ 2.5552e-03,  8.0036e-03, -2.8387e-03,  ..., -1.5711e-03,\n",
      "          6.0962e-03, -1.0727e-03],\n",
      "        [ 8.0791e-03,  4.4323e-03, -1.5549e-03,  ...,  1.0708e-02,\n",
      "          5.0678e-03,  1.1023e-02],\n",
      "        [ 6.7060e-03,  2.4234e-03, -5.0706e-03,  ...,  3.7482e-03,\n",
      "          4.3543e-03,  1.2713e-03],\n",
      "        ...,\n",
      "        [ 3.6537e-03,  1.0832e-02,  4.7005e-03,  ...,  3.5815e-03,\n",
      "          1.4507e-02, -4.3961e-03],\n",
      "        [-1.8272e-03, -4.1467e-03,  1.4494e-03,  ..., -4.7656e-03,\n",
      "         -2.3520e-03,  5.4510e-03],\n",
      "        [-4.3411e-03,  5.9939e-05,  6.8421e-04,  ..., -3.5674e-03,\n",
      "          1.2288e-03, -6.8232e-03]])\n",
      "unet.up_blocks.2.attentions.1.proj_in.lora.down.weight tensor([[[[ 3.0830e-03]],\n",
      "\n",
      "         [[-1.2615e-02]],\n",
      "\n",
      "         [[ 4.9452e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.3432e-03]],\n",
      "\n",
      "         [[-2.1891e-03]],\n",
      "\n",
      "         [[-9.3675e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.2097e-03]],\n",
      "\n",
      "         [[ 2.0795e-03]],\n",
      "\n",
      "         [[ 3.1055e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0785e-03]],\n",
      "\n",
      "         [[-8.7036e-04]],\n",
      "\n",
      "         [[-4.2769e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 8.1267e-04]],\n",
      "\n",
      "         [[-5.3285e-03]],\n",
      "\n",
      "         [[ 7.1078e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.4879e-03]],\n",
      "\n",
      "         [[ 6.9419e-05]],\n",
      "\n",
      "         [[ 3.2442e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-3.8660e-03]],\n",
      "\n",
      "         [[ 1.6802e-03]],\n",
      "\n",
      "         [[ 2.1509e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.7252e-04]],\n",
      "\n",
      "         [[-4.9110e-03]],\n",
      "\n",
      "         [[ 5.3880e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 6.6238e-03]],\n",
      "\n",
      "         [[ 1.9735e-02]],\n",
      "\n",
      "         [[ 2.3188e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.6504e-04]],\n",
      "\n",
      "         [[ 4.6232e-03]],\n",
      "\n",
      "         [[ 2.8881e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.9692e-03]],\n",
      "\n",
      "         [[-3.1475e-03]],\n",
      "\n",
      "         [[-3.3942e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.4388e-03]],\n",
      "\n",
      "         [[-2.6196e-03]],\n",
      "\n",
      "         [[-3.2343e-03]]]])\n",
      "unet.up_blocks.2.attentions.1.proj_in.lora.up.weight tensor([[[[ 0.0015]],\n",
      "\n",
      "         [[-0.0040]],\n",
      "\n",
      "         [[-0.0070]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0057]],\n",
      "\n",
      "         [[ 0.0058]],\n",
      "\n",
      "         [[ 0.0021]]],\n",
      "\n",
      "\n",
      "        [[[-0.0069]],\n",
      "\n",
      "         [[-0.0056]],\n",
      "\n",
      "         [[ 0.0111]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0003]],\n",
      "\n",
      "         [[-0.0014]],\n",
      "\n",
      "         [[ 0.0049]]],\n",
      "\n",
      "\n",
      "        [[[-0.0036]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         [[-0.0062]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0133]],\n",
      "\n",
      "         [[ 0.0092]],\n",
      "\n",
      "         [[-0.0019]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0010]],\n",
      "\n",
      "         [[-0.0102]],\n",
      "\n",
      "         [[-0.0002]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         [[ 0.0039]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0050]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[ 0.0095]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0073]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[ 0.0166]]],\n",
      "\n",
      "\n",
      "        [[[-0.0091]],\n",
      "\n",
      "         [[ 0.0187]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0075]],\n",
      "\n",
      "         [[ 0.0050]],\n",
      "\n",
      "         [[-0.0038]]]])\n",
      "unet.up_blocks.2.attentions.1.proj_out.lora.down.weight tensor([[[[-2.7607e-03]],\n",
      "\n",
      "         [[-2.9833e-03]],\n",
      "\n",
      "         [[-4.6399e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.6675e-03]],\n",
      "\n",
      "         [[ 5.9621e-03]],\n",
      "\n",
      "         [[ 3.8353e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 5.9803e-05]],\n",
      "\n",
      "         [[-1.6145e-03]],\n",
      "\n",
      "         [[ 7.6845e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0205e-03]],\n",
      "\n",
      "         [[ 2.5432e-04]],\n",
      "\n",
      "         [[-1.7330e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 4.3257e-03]],\n",
      "\n",
      "         [[-7.6766e-03]],\n",
      "\n",
      "         [[ 1.8043e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5795e-04]],\n",
      "\n",
      "         [[ 1.1141e-03]],\n",
      "\n",
      "         [[ 3.7474e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.1037e-03]],\n",
      "\n",
      "         [[-3.0424e-03]],\n",
      "\n",
      "         [[ 1.6168e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.2531e-04]],\n",
      "\n",
      "         [[ 6.0890e-04]],\n",
      "\n",
      "         [[-4.4972e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9830e-03]],\n",
      "\n",
      "         [[ 2.1411e-02]],\n",
      "\n",
      "         [[ 2.5160e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.1918e-03]],\n",
      "\n",
      "         [[ 2.7126e-03]],\n",
      "\n",
      "         [[ 1.7486e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.7535e-04]],\n",
      "\n",
      "         [[ 2.6701e-03]],\n",
      "\n",
      "         [[-5.9302e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.0341e-03]],\n",
      "\n",
      "         [[-3.6424e-03]],\n",
      "\n",
      "         [[-3.6007e-03]]]])\n",
      "unet.up_blocks.2.attentions.1.proj_out.lora.up.weight tensor([[[[-0.0031]],\n",
      "\n",
      "         [[ 0.0021]],\n",
      "\n",
      "         [[ 0.0032]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[-0.0250]],\n",
      "\n",
      "         [[-0.0071]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0024]],\n",
      "\n",
      "         [[ 0.0111]],\n",
      "\n",
      "         [[ 0.0097]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0069]],\n",
      "\n",
      "         [[-0.0230]],\n",
      "\n",
      "         [[-0.0029]]],\n",
      "\n",
      "\n",
      "        [[[-0.0028]],\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         [[ 0.0064]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         [[-0.0001]],\n",
      "\n",
      "         [[ 0.0061]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0044]],\n",
      "\n",
      "         [[-0.0028]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[ 0.0111]],\n",
      "\n",
      "         [[-0.0002]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0041]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[ 0.0073]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[ 0.0054]],\n",
      "\n",
      "         [[ 0.0043]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0103]],\n",
      "\n",
      "         [[ 0.0071]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0022]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         [[ 0.0058]]]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight tensor([[-9.2610e-04,  6.6834e-04, -4.7172e-03,  ...,  3.1680e-03,\n",
      "          2.3657e-03,  2.0304e-04],\n",
      "        [-4.8647e-04,  3.6839e-04,  3.1200e-04,  ...,  1.2768e-03,\n",
      "         -5.0841e-04,  4.7087e-04],\n",
      "        [ 9.4571e-04,  1.0139e-03,  5.3555e-04,  ..., -9.4997e-05,\n",
      "         -1.4213e-03, -2.4930e-04],\n",
      "        ...,\n",
      "        [ 6.4291e-04,  1.9941e-03, -4.4838e-03,  ...,  1.7973e-03,\n",
      "          4.9697e-05,  4.3924e-03],\n",
      "        [-1.3343e-02, -1.8570e-03, -1.4825e-03,  ..., -1.5122e-02,\n",
      "          1.9162e-02, -5.9677e-03],\n",
      "        [ 2.8567e-04,  1.4851e-03, -5.6259e-04,  ...,  1.3092e-03,\n",
      "          5.7107e-03, -1.5568e-03]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight tensor([[-0.0131, -0.0033, -0.0022,  ..., -0.0023, -0.0010,  0.0064],\n",
      "        [-0.0210,  0.0025, -0.0019,  ..., -0.0070, -0.0049,  0.0028],\n",
      "        [ 0.0037, -0.0041, -0.0014,  ...,  0.0003,  0.0034,  0.0028],\n",
      "        ...,\n",
      "        [-0.0134,  0.0017, -0.0074,  ..., -0.0019, -0.0017,  0.0055],\n",
      "        [ 0.0089,  0.0073,  0.0049,  ..., -0.0029, -0.0071,  0.0046],\n",
      "        [-0.0008,  0.0021,  0.0010,  ...,  0.0011, -0.0135, -0.0014]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight tensor([[ 5.4617e-03,  3.1346e-03, -1.1645e-03,  ..., -2.0751e-03,\n",
      "         -4.2365e-03,  7.9588e-03],\n",
      "        [ 1.8880e-04,  7.9440e-04, -1.9633e-03,  ..., -7.2041e-04,\n",
      "         -8.1682e-04, -3.7732e-04],\n",
      "        [-5.3090e-05,  2.0944e-03, -5.1564e-03,  ...,  1.2129e-03,\n",
      "          4.1294e-04, -3.1079e-05],\n",
      "        ...,\n",
      "        [ 1.5397e-03,  6.6282e-03,  1.6149e-04,  ..., -2.3271e-03,\n",
      "          7.2131e-04, -2.7530e-03],\n",
      "        [-1.1956e-02,  8.2835e-03, -1.5358e-03,  ...,  8.0312e-03,\n",
      "          8.9945e-04,  8.9577e-03],\n",
      "        [ 3.1056e-04,  1.1628e-03,  1.6693e-03,  ..., -7.6342e-04,\n",
      "         -1.2792e-03,  5.6018e-03]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight tensor([[ 3.0484e-03, -1.7148e-03, -5.0160e-03,  ...,  3.4344e-03,\n",
      "          5.4638e-03, -6.0302e-03],\n",
      "        [ 4.3335e-03,  5.5627e-03, -1.2835e-02,  ..., -9.2687e-03,\n",
      "         -4.1220e-03, -5.4366e-03],\n",
      "        [-2.3192e-03,  1.2921e-03, -2.4881e-03,  ...,  6.1044e-03,\n",
      "          4.5650e-06, -2.0734e-03],\n",
      "        ...,\n",
      "        [-1.4674e-03, -1.6142e-03, -8.5965e-03,  ...,  1.4465e-02,\n",
      "         -3.6695e-03, -2.8239e-03],\n",
      "        [-5.1644e-05, -5.6616e-03, -6.4942e-03,  ..., -2.8591e-03,\n",
      "          8.9056e-03, -5.1170e-03],\n",
      "        [-5.0713e-03, -2.2004e-04,  4.9203e-03,  ..., -3.9018e-03,\n",
      "          6.8889e-03,  4.2229e-03]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight tensor([[ 1.6352e-03,  3.7502e-04,  1.8028e-03,  ...,  2.1830e-03,\n",
      "          2.3077e-03, -5.5162e-03],\n",
      "        [-5.7271e-04,  5.4627e-04,  1.6551e-03,  ..., -3.7297e-04,\n",
      "         -2.1494e-04, -1.4331e-04],\n",
      "        [ 2.0069e-03, -3.6647e-03,  1.4298e-03,  ..., -8.6954e-04,\n",
      "         -9.8413e-04, -2.8340e-03],\n",
      "        ...,\n",
      "        [-1.7530e-04,  2.0634e-03,  2.4330e-03,  ...,  1.6801e-03,\n",
      "          3.4806e-03,  2.3457e-04],\n",
      "        [ 7.1150e-03, -1.0244e-02,  1.2679e-02,  ...,  1.9218e-02,\n",
      "          2.0149e-04, -1.9192e-02],\n",
      "        [ 5.9001e-05, -1.8810e-03,  6.9409e-04,  ...,  2.5427e-04,\n",
      "         -3.3036e-03, -2.3393e-03]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight tensor([[-1.4325e-02,  4.1298e-03, -1.4997e-03,  ..., -2.8709e-03,\n",
      "          2.5996e-03, -3.3676e-04],\n",
      "        [-5.0538e-04,  1.1141e-03,  4.4664e-03,  ...,  8.8944e-03,\n",
      "          1.0870e-03,  2.1740e-03],\n",
      "        [ 5.9392e-03, -6.6520e-03,  1.3390e-02,  ...,  6.8909e-03,\n",
      "          5.9717e-03,  5.2671e-03],\n",
      "        ...,\n",
      "        [-1.3177e-02, -1.2515e-03, -9.7220e-05,  ..., -6.8055e-03,\n",
      "         -2.8435e-03, -8.3947e-03],\n",
      "        [-5.1695e-03, -5.0319e-03,  9.6934e-04,  ...,  7.0745e-03,\n",
      "         -8.7720e-03, -4.5946e-03],\n",
      "        [ 2.7885e-04, -2.3679e-03, -5.0826e-03,  ...,  3.2016e-03,\n",
      "         -6.8770e-03,  1.0644e-03]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight tensor([[-5.5463e-03, -3.4072e-03, -3.5609e-03,  ...,  8.9272e-04,\n",
      "         -1.7385e-03,  1.8888e-03],\n",
      "        [ 3.4302e-04, -5.2343e-05, -2.0749e-05,  ..., -4.8497e-04,\n",
      "         -4.9241e-05, -1.8701e-04],\n",
      "        [-2.6908e-03,  6.5845e-03, -9.6558e-04,  ..., -1.3861e-03,\n",
      "          2.5543e-03, -1.3957e-03],\n",
      "        ...,\n",
      "        [ 2.9790e-03,  7.4027e-04, -1.6019e-03,  ...,  1.1916e-03,\n",
      "         -5.1099e-03, -9.1539e-05],\n",
      "        [-5.8691e-03,  3.7205e-03, -5.0374e-03,  ...,  1.6721e-04,\n",
      "         -3.9138e-04,  4.6039e-03],\n",
      "        [-2.2478e-04, -2.8863e-03,  2.9942e-03,  ..., -4.1037e-05,\n",
      "         -8.2924e-04,  5.1637e-03]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight tensor([[ 1.8862e-04, -1.5686e-03,  1.1347e-02,  ...,  1.3675e-03,\n",
      "         -8.8817e-03,  1.7772e-03],\n",
      "        [-7.0919e-03, -2.7653e-03, -8.1631e-03,  ..., -6.3662e-03,\n",
      "          4.8003e-03, -1.5892e-06],\n",
      "        [-3.0044e-03,  4.3559e-03,  4.9296e-03,  ..., -9.1521e-03,\n",
      "          6.7515e-03, -3.5758e-03],\n",
      "        ...,\n",
      "        [-4.2915e-03, -6.8056e-03, -4.3166e-03,  ..., -4.5628e-03,\n",
      "         -3.4892e-04, -8.0051e-05],\n",
      "        [-1.4154e-04, -4.7925e-04,  1.0282e-03,  ...,  4.4873e-03,\n",
      "          5.6207e-03,  2.2630e-03],\n",
      "        [ 2.9825e-03, -1.2966e-03,  5.3112e-04,  ..., -2.8682e-03,\n",
      "          3.8911e-04, -5.0516e-04]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight tensor([[ 2.6935e-03, -2.7335e-04,  2.1085e-03,  ..., -4.8150e-03,\n",
      "          1.8468e-03, -5.3912e-03],\n",
      "        [ 3.7350e-04, -5.0796e-05, -3.3159e-05,  ...,  1.3355e-04,\n",
      "         -3.2468e-04, -8.8595e-04],\n",
      "        [-6.2869e-04, -7.5768e-04, -7.0125e-04,  ..., -1.1068e-03,\n",
      "          1.0861e-03, -1.2692e-03],\n",
      "        ...,\n",
      "        [ 4.4433e-04, -2.8119e-03, -1.6911e-03,  ...,  2.4964e-04,\n",
      "         -1.2150e-03, -4.5869e-04],\n",
      "        [-4.5231e-03,  6.9171e-03, -3.9285e-03,  ...,  3.8091e-03,\n",
      "          7.4442e-04,  1.3172e-02],\n",
      "        [ 3.6689e-03, -4.3546e-04,  1.4434e-03,  ...,  2.7715e-03,\n",
      "         -1.7814e-03, -1.8219e-03]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight tensor([[-1.3608e-03,  1.1378e-03, -3.1571e-03,  ...,  4.0765e-04,\n",
      "         -8.3166e-04,  3.2410e-03],\n",
      "        [-3.3391e-04,  7.9433e-04,  5.5215e-04,  ..., -2.3727e-04,\n",
      "         -1.1658e-03,  6.9003e-05],\n",
      "        [-1.4893e-03,  2.6056e-04, -1.6581e-03,  ...,  7.5406e-04,\n",
      "          2.5883e-03, -3.2234e-04],\n",
      "        ...,\n",
      "        [ 2.1051e-03, -1.2517e-03,  1.5053e-03,  ...,  8.4603e-04,\n",
      "         -2.2417e-03,  1.2687e-03],\n",
      "        [ 1.3448e-03, -1.9400e-03, -7.3272e-04,  ...,  1.8369e-03,\n",
      "          3.9937e-04, -8.4960e-04],\n",
      "        [ 7.7279e-04, -1.1256e-03, -1.9701e-03,  ...,  6.5775e-05,\n",
      "         -1.9894e-03,  3.2162e-03]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight tensor([[ 4.8073e-03,  4.0563e-03,  9.8606e-05,  ...,  2.4709e-03,\n",
      "          2.2935e-03,  1.2603e-03],\n",
      "        [ 6.8150e-04,  1.3548e-04,  4.1002e-04,  ...,  7.1189e-04,\n",
      "          3.5645e-04, -8.4364e-04],\n",
      "        [-2.3979e-03,  8.0432e-04, -3.3611e-03,  ...,  2.4628e-05,\n",
      "         -3.9599e-03, -3.6358e-04],\n",
      "        ...,\n",
      "        [ 1.8278e-03, -4.0434e-04,  2.0609e-03,  ..., -4.8702e-04,\n",
      "         -1.3380e-03,  5.7864e-04],\n",
      "        [-2.1783e-03, -1.4205e-02,  4.0649e-03,  ...,  4.5099e-03,\n",
      "          5.4914e-04, -1.6251e-03],\n",
      "        [-1.1937e-03, -3.2392e-03,  1.7305e-03,  ..., -1.6217e-03,\n",
      "         -3.4317e-05,  4.8202e-05]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight tensor([[-6.4872e-03, -1.1978e-03,  3.6698e-03,  ...,  1.5850e-03,\n",
      "          1.9610e-03,  9.5350e-03],\n",
      "        [-6.9855e-03, -5.2760e-03,  3.4579e-03,  ..., -7.3822e-05,\n",
      "         -3.8027e-03, -5.9683e-03],\n",
      "        [ 1.2386e-02, -3.2248e-03, -1.5678e-03,  ...,  2.8485e-03,\n",
      "          3.6499e-03,  7.7312e-03],\n",
      "        ...,\n",
      "        [ 3.2764e-03, -3.8484e-03,  8.3375e-03,  ...,  4.0306e-03,\n",
      "         -5.7399e-03, -3.3466e-04],\n",
      "        [-4.2626e-03,  6.8530e-03, -1.0046e-03,  ..., -3.0010e-03,\n",
      "         -6.4654e-03,  3.0941e-03],\n",
      "        [-5.8502e-03,  8.7605e-03,  5.6446e-04,  ...,  2.0290e-03,\n",
      "          1.0805e-02,  7.7365e-04]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight tensor([[ 1.8729e-03,  8.9224e-04,  3.2484e-03,  ..., -3.6373e-03,\n",
      "          1.8726e-03, -1.7047e-03],\n",
      "        [ 1.0634e-03,  8.5308e-04, -3.9353e-04,  ...,  1.0477e-04,\n",
      "          1.3813e-04, -6.9474e-04],\n",
      "        [-1.5781e-03,  3.8866e-04,  2.3903e-04,  ..., -3.0987e-03,\n",
      "          1.9286e-03,  2.8861e-04],\n",
      "        ...,\n",
      "        [ 4.2648e-03, -1.9313e-03,  1.4779e-03,  ...,  8.6245e-04,\n",
      "          1.0594e-03, -3.0109e-04],\n",
      "        [-1.2387e-03, -1.5823e-03,  3.2137e-03,  ..., -3.3576e-03,\n",
      "         -5.0677e-03,  2.4385e-03],\n",
      "        [ 8.6981e-04, -1.9899e-04, -2.0331e-03,  ...,  2.2309e-04,\n",
      "          2.5137e-05,  2.1757e-03]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight tensor([[ 1.0056e-03, -2.3981e-03, -2.2403e-04,  ...,  1.5996e-04,\n",
      "         -6.0577e-03,  6.6870e-04],\n",
      "        [ 3.2110e-04,  1.3169e-04,  5.8952e-04,  ..., -3.3214e-05,\n",
      "          9.1319e-03,  9.1187e-04],\n",
      "        [-1.5893e-03, -9.6425e-04, -1.5989e-03,  ...,  5.1846e-04,\n",
      "          4.2572e-03, -1.6136e-03],\n",
      "        ...,\n",
      "        [ 7.2990e-04, -3.5652e-04, -1.5505e-03,  ..., -4.8429e-04,\n",
      "          3.5277e-04, -1.1274e-03],\n",
      "        [-2.7022e-04,  3.0686e-04, -3.8474e-04,  ..., -6.9108e-04,\n",
      "         -2.7691e-03, -1.8204e-03],\n",
      "        [-9.7846e-04,  7.6710e-05, -8.7116e-05,  ..., -1.4117e-03,\n",
      "         -1.0281e-03, -2.0907e-04]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight tensor([[ 1.2490e-03, -1.4485e-04,  2.9550e-03,  ...,  6.1253e-04,\n",
      "         -6.7472e-04, -6.5242e-03],\n",
      "        [-6.4334e-04, -1.6795e-05,  4.2096e-05,  ...,  3.9316e-04,\n",
      "         -3.0966e-04,  7.1881e-04],\n",
      "        [ 3.9375e-04,  1.7654e-03, -2.6814e-03,  ...,  2.1913e-03,\n",
      "          5.7678e-04, -8.2853e-05],\n",
      "        ...,\n",
      "        [-1.2118e-03,  4.9859e-04, -1.2926e-03,  ..., -1.2531e-03,\n",
      "         -2.0006e-03, -3.2019e-03],\n",
      "        [ 1.6402e-03, -4.7364e-03,  7.4576e-03,  ...,  6.1098e-04,\n",
      "          6.4740e-03, -3.5293e-03],\n",
      "        [-1.2439e-03, -1.3849e-03, -5.9548e-04,  ..., -6.8256e-04,\n",
      "          6.3933e-04, -1.3263e-03]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight tensor([[ 0.0018,  0.0004, -0.0004,  ..., -0.0023, -0.0060,  0.0053],\n",
      "        [ 0.0014,  0.0043,  0.0125,  ..., -0.0002, -0.0040,  0.0038],\n",
      "        [-0.0014, -0.0075,  0.0007,  ...,  0.0027, -0.0010,  0.0049],\n",
      "        ...,\n",
      "        [-0.0028, -0.0048, -0.0021,  ..., -0.0088,  0.0045,  0.0086],\n",
      "        [ 0.0042, -0.0070, -0.0062,  ..., -0.0030, -0.0054,  0.0018],\n",
      "        [-0.0025, -0.0117, -0.0059,  ...,  0.0024, -0.0021,  0.0007]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight tensor([[ 0.0078, -0.0108, -0.0013,  ..., -0.0098,  0.0007,  0.0019],\n",
      "        [ 0.0008, -0.0004,  0.0010,  ...,  0.0008,  0.0005,  0.0013],\n",
      "        [-0.0029, -0.0005, -0.0057,  ..., -0.0005, -0.0004, -0.0071],\n",
      "        ...,\n",
      "        [-0.0027,  0.0012,  0.0006,  ..., -0.0018,  0.0029, -0.0025],\n",
      "        [-0.0147,  0.0123,  0.0232,  ...,  0.0120, -0.0098,  0.0307],\n",
      "        [ 0.0024, -0.0043,  0.0008,  ..., -0.0019,  0.0029, -0.0024]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight tensor([[ 0.0117, -0.0087,  0.0088,  ..., -0.0002, -0.0262, -0.0120],\n",
      "        [-0.0036, -0.0029,  0.0065,  ..., -0.0066,  0.0010, -0.0023],\n",
      "        [ 0.0040,  0.0017,  0.0019,  ...,  0.0015, -0.0031,  0.0063],\n",
      "        ...,\n",
      "        [ 0.0047, -0.0020,  0.0038,  ..., -0.0005, -0.0031, -0.0017],\n",
      "        [ 0.0019,  0.0066, -0.0006,  ..., -0.0037, -0.0015,  0.0088],\n",
      "        [ 0.0069, -0.0087, -0.0016,  ..., -0.0025, -0.0006, -0.0043]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight tensor([[ 1.9959e-02, -1.1293e-03,  2.1627e-05,  ...,  1.9156e-03,\n",
      "          1.9512e-03,  2.3291e-03],\n",
      "        [ 6.9683e-04, -3.4189e-05,  4.7850e-04,  ..., -4.3065e-04,\n",
      "          1.0730e-03, -1.7737e-03],\n",
      "        [ 7.8167e-03, -6.1925e-03, -1.4841e-04,  ..., -4.0279e-03,\n",
      "         -2.0474e-03, -5.1249e-03],\n",
      "        ...,\n",
      "        [ 1.2588e-04,  1.2719e-03, -1.2782e-03,  ...,  4.0541e-03,\n",
      "          3.3600e-04,  1.1554e-04],\n",
      "        [-3.2281e-02, -2.4336e-03,  1.9198e-02,  ..., -1.6340e-03,\n",
      "         -4.8809e-03, -1.7780e-02],\n",
      "        [-5.2721e-03, -2.1021e-03,  1.0799e-03,  ..., -1.5829e-03,\n",
      "          3.2581e-03,  1.8562e-03]])\n",
      "unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight tensor([[ 0.0048, -0.0026,  0.0027,  ...,  0.0018, -0.0128,  0.0005],\n",
      "        [-0.0004,  0.0042, -0.0008,  ...,  0.0016, -0.0078, -0.0037],\n",
      "        [-0.0062, -0.0063,  0.0077,  ...,  0.0020,  0.0005, -0.0065],\n",
      "        ...,\n",
      "        [-0.0024,  0.0075,  0.0072,  ..., -0.0001,  0.0051, -0.0042],\n",
      "        [-0.0075, -0.0112, -0.0001,  ...,  0.0007,  0.0014, -0.0064],\n",
      "        [ 0.0127,  0.0045,  0.0066,  ...,  0.0064, -0.0119, -0.0036]])\n",
      "unet.up_blocks.2.attentions.2.proj_in.lora.down.weight tensor([[[[-4.6996e-03]],\n",
      "\n",
      "         [[ 1.4627e-03]],\n",
      "\n",
      "         [[-4.2345e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.9085e-03]],\n",
      "\n",
      "         [[-3.2989e-03]],\n",
      "\n",
      "         [[-1.3004e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 3.5034e-04]],\n",
      "\n",
      "         [[ 2.1437e-03]],\n",
      "\n",
      "         [[-3.1335e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.0054e-04]],\n",
      "\n",
      "         [[ 3.2032e-04]],\n",
      "\n",
      "         [[-1.1811e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 5.0403e-03]],\n",
      "\n",
      "         [[-3.7362e-04]],\n",
      "\n",
      "         [[ 3.4774e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.1856e-03]],\n",
      "\n",
      "         [[ 5.8016e-04]],\n",
      "\n",
      "         [[ 2.3237e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.5954e-05]],\n",
      "\n",
      "         [[-4.4180e-04]],\n",
      "\n",
      "         [[ 7.8382e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0437e-03]],\n",
      "\n",
      "         [[-6.1804e-03]],\n",
      "\n",
      "         [[ 3.7589e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.5458e-03]],\n",
      "\n",
      "         [[-6.7813e-03]],\n",
      "\n",
      "         [[-9.8564e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.7869e-03]],\n",
      "\n",
      "         [[ 1.7491e-02]],\n",
      "\n",
      "         [[-1.3972e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.0635e-04]],\n",
      "\n",
      "         [[-2.9801e-03]],\n",
      "\n",
      "         [[ 2.5334e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.6382e-03]],\n",
      "\n",
      "         [[-1.9792e-03]],\n",
      "\n",
      "         [[ 3.5498e-03]]]])\n",
      "unet.up_blocks.2.attentions.2.proj_in.lora.up.weight tensor([[[[ 0.0088]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[-0.0112]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[-0.0104]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0079]],\n",
      "\n",
      "         [[ 0.0059]],\n",
      "\n",
      "         [[ 0.0032]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0055]],\n",
      "\n",
      "         [[-0.0038]],\n",
      "\n",
      "         [[ 0.0026]]],\n",
      "\n",
      "\n",
      "        [[[-0.0059]],\n",
      "\n",
      "         [[-0.0058]],\n",
      "\n",
      "         [[-0.0030]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[ 0.0037]],\n",
      "\n",
      "         [[ 0.0022]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0038]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0056]],\n",
      "\n",
      "         [[ 0.0046]],\n",
      "\n",
      "         [[-0.0035]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0014]],\n",
      "\n",
      "         [[-0.0048]],\n",
      "\n",
      "         [[-0.0042]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0057]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[-0.0019]]],\n",
      "\n",
      "\n",
      "        [[[-0.0054]],\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         [[-0.0059]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         [[ 0.0079]],\n",
      "\n",
      "         [[-0.0007]]]])\n",
      "unet.up_blocks.2.attentions.2.proj_out.lora.down.weight tensor([[[[ 0.0048]],\n",
      "\n",
      "         [[-0.0097]],\n",
      "\n",
      "         [[ 0.0065]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0074]],\n",
      "\n",
      "         [[ 0.0115]],\n",
      "\n",
      "         [[-0.0076]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0006]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[ 0.0012]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0010]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[ 0.0063]],\n",
      "\n",
      "         [[-0.0033]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0017]],\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         [[ 0.0024]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0050]],\n",
      "\n",
      "         [[ 0.0035]],\n",
      "\n",
      "         [[ 0.0006]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0004]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[-0.0184]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0095]],\n",
      "\n",
      "         [[ 0.0071]],\n",
      "\n",
      "         [[-0.0188]]],\n",
      "\n",
      "\n",
      "        [[[-0.0013]],\n",
      "\n",
      "         [[ 0.0040]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0073]],\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         [[ 0.0103]]]])\n",
      "unet.up_blocks.2.attentions.2.proj_out.lora.up.weight tensor([[[[-5.3034e-05]],\n",
      "\n",
      "         [[ 8.8798e-03]],\n",
      "\n",
      "         [[ 9.6420e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.3607e-03]],\n",
      "\n",
      "         [[-1.2391e-02]],\n",
      "\n",
      "         [[-3.5910e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4709e-02]],\n",
      "\n",
      "         [[ 6.7392e-03]],\n",
      "\n",
      "         [[ 1.1677e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.2003e-03]],\n",
      "\n",
      "         [[ 1.6279e-02]],\n",
      "\n",
      "         [[-6.3076e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.4802e-03]],\n",
      "\n",
      "         [[-4.6874e-03]],\n",
      "\n",
      "         [[ 1.4449e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1404e-03]],\n",
      "\n",
      "         [[-1.7124e-03]],\n",
      "\n",
      "         [[ 1.4797e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 5.2937e-03]],\n",
      "\n",
      "         [[ 3.4030e-03]],\n",
      "\n",
      "         [[-4.4616e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.7555e-03]],\n",
      "\n",
      "         [[-1.9094e-03]],\n",
      "\n",
      "         [[ 8.4024e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6098e-03]],\n",
      "\n",
      "         [[-1.4711e-03]],\n",
      "\n",
      "         [[-1.9273e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0085e-03]],\n",
      "\n",
      "         [[-6.2312e-03]],\n",
      "\n",
      "         [[ 3.0164e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 7.1903e-03]],\n",
      "\n",
      "         [[-2.4448e-03]],\n",
      "\n",
      "         [[ 1.4598e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1802e-02]],\n",
      "\n",
      "         [[ 9.0687e-03]],\n",
      "\n",
      "         [[ 9.8370e-04]]]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight tensor([[-5.9477e-04, -2.6481e-03, -1.5577e-03,  ..., -2.2165e-04,\n",
      "         -6.4200e-03, -9.2461e-06],\n",
      "        [-1.5644e-03, -6.7291e-04, -1.2551e-03,  ...,  2.1328e-04,\n",
      "          2.7718e-04,  1.2497e-03],\n",
      "        [ 2.6442e-03, -2.0540e-03,  2.9105e-03,  ..., -4.8136e-05,\n",
      "         -4.0714e-03, -1.9020e-03],\n",
      "        ...,\n",
      "        [-4.3520e-04, -1.5096e-03, -2.1876e-03,  ..., -4.6109e-03,\n",
      "         -2.4952e-04,  4.2277e-03],\n",
      "        [-1.0954e-02, -5.8989e-03,  7.0067e-04,  ..., -1.4291e-02,\n",
      "          2.8050e-02,  5.0547e-03],\n",
      "        [-2.2269e-03, -1.7199e-03,  3.4831e-04,  ...,  1.0279e-03,\n",
      "         -2.9781e-03, -4.8206e-05]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight tensor([[-0.0004, -0.0006, -0.0014,  ..., -0.0077,  0.0020,  0.0019],\n",
      "        [-0.0104, -0.0114,  0.0007,  ..., -0.0005, -0.0071,  0.0056],\n",
      "        [ 0.0024,  0.0015, -0.0050,  ...,  0.0012, -0.0045,  0.0030],\n",
      "        ...,\n",
      "        [-0.0001, -0.0021,  0.0068,  ...,  0.0047, -0.0151, -0.0012],\n",
      "        [ 0.0161,  0.0028, -0.0027,  ..., -0.0001, -0.0072, -0.0007],\n",
      "        [ 0.0065, -0.0022, -0.0091,  ..., -0.0024,  0.0031,  0.0027]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight tensor([[-0.0085, -0.0059,  0.0037,  ..., -0.0058,  0.0040,  0.0054],\n",
      "        [-0.0003,  0.0003,  0.0005,  ..., -0.0001, -0.0005,  0.0002],\n",
      "        [-0.0026,  0.0006,  0.0011,  ..., -0.0026,  0.0018, -0.0013],\n",
      "        ...,\n",
      "        [-0.0003,  0.0033, -0.0052,  ..., -0.0049, -0.0006, -0.0022],\n",
      "        [-0.0149,  0.0020,  0.0153,  ..., -0.0122,  0.0011,  0.0086],\n",
      "        [-0.0006, -0.0038, -0.0028,  ..., -0.0033, -0.0017,  0.0021]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight tensor([[-0.0013,  0.0026, -0.0027,  ...,  0.0014,  0.0027,  0.0071],\n",
      "        [-0.0090, -0.0043,  0.0002,  ...,  0.0006, -0.0001, -0.0001],\n",
      "        [-0.0038, -0.0029,  0.0066,  ...,  0.0065, -0.0009, -0.0100],\n",
      "        ...,\n",
      "        [-0.0011,  0.0034, -0.0026,  ...,  0.0089,  0.0151, -0.0076],\n",
      "        [-0.0076, -0.0037, -0.0040,  ..., -0.0011,  0.0019, -0.0063],\n",
      "        [ 0.0013, -0.0032,  0.0057,  ..., -0.0028, -0.0085, -0.0127]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight tensor([[ 0.0003, -0.0005, -0.0030,  ...,  0.0001, -0.0022,  0.0080],\n",
      "        [-0.0007, -0.0002, -0.0003,  ..., -0.0002,  0.0009,  0.0012],\n",
      "        [ 0.0021, -0.0027, -0.0013,  ..., -0.0020,  0.0006, -0.0024],\n",
      "        ...,\n",
      "        [ 0.0008,  0.0009, -0.0038,  ..., -0.0023,  0.0026, -0.0013],\n",
      "        [-0.0107,  0.0099, -0.0140,  ..., -0.0068, -0.0074,  0.0077],\n",
      "        [ 0.0027, -0.0033,  0.0036,  ..., -0.0008, -0.0032, -0.0009]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight tensor([[-1.1557e-02, -2.4458e-04,  6.2397e-03,  ...,  6.0124e-03,\n",
      "          1.1655e-03, -8.0653e-03],\n",
      "        [ 8.4684e-03,  8.2188e-04, -9.0567e-03,  ...,  7.0299e-04,\n",
      "         -5.4655e-03, -7.0370e-03],\n",
      "        [ 1.1281e-03, -2.9434e-03,  1.2447e-03,  ..., -5.6774e-03,\n",
      "         -3.7474e-03, -1.8349e-03],\n",
      "        ...,\n",
      "        [-7.5640e-03, -5.4286e-04, -1.3200e-03,  ...,  7.7192e-05,\n",
      "         -8.1867e-03, -2.6344e-03],\n",
      "        [-7.8410e-03,  8.3057e-04, -3.0817e-03,  ..., -3.8604e-04,\n",
      "         -1.4972e-02, -3.7938e-03],\n",
      "        [ 5.0231e-03, -1.4555e-03,  1.2198e-03,  ..., -1.0173e-03,\n",
      "         -1.9022e-03, -3.6478e-03]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight tensor([[ 1.1246e-03,  2.7385e-04, -4.9815e-03,  ...,  9.3250e-04,\n",
      "         -2.7394e-03, -2.0524e-03],\n",
      "        [-1.6837e-04,  7.7364e-04,  1.7532e-05,  ...,  1.4925e-03,\n",
      "          5.2321e-04, -9.6640e-04],\n",
      "        [-9.0910e-03, -2.7261e-03, -6.9791e-04,  ...,  4.6894e-03,\n",
      "         -1.7966e-03, -1.2483e-03],\n",
      "        ...,\n",
      "        [-3.9552e-03, -9.0226e-04,  2.2616e-03,  ..., -3.9295e-03,\n",
      "         -2.1211e-03, -2.0148e-03],\n",
      "        [ 1.0710e-03,  1.3745e-02, -1.1219e-02,  ...,  1.1591e-02,\n",
      "         -1.9568e-02, -1.7500e-02],\n",
      "        [ 3.5184e-03, -3.8551e-03,  1.2742e-03,  ..., -1.9647e-03,\n",
      "         -5.5078e-03,  1.2294e-03]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight tensor([[ 0.0046, -0.0048, -0.0057,  ...,  0.0016,  0.0001,  0.0049],\n",
      "        [-0.0132,  0.0025, -0.0019,  ..., -0.0064, -0.0145,  0.0022],\n",
      "        [ 0.0080, -0.0011, -0.0078,  ...,  0.0027, -0.0025, -0.0014],\n",
      "        ...,\n",
      "        [-0.0067, -0.0016, -0.0033,  ...,  0.0020, -0.0092, -0.0022],\n",
      "        [ 0.0038,  0.0006,  0.0047,  ..., -0.0018, -0.0072, -0.0095],\n",
      "        [-0.0060,  0.0055, -0.0002,  ..., -0.0015, -0.0041, -0.0013]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight tensor([[ 1.7694e-04,  6.8836e-03,  3.0020e-04,  ...,  9.6397e-04,\n",
      "         -1.8266e-03,  2.1697e-03],\n",
      "        [-6.6683e-05, -3.8856e-05, -5.3022e-05,  ..., -6.4425e-04,\n",
      "          4.1142e-04, -4.3823e-05],\n",
      "        [-1.3926e-03, -1.4392e-03, -2.2548e-03,  ..., -1.0531e-03,\n",
      "          3.4560e-04,  2.6715e-04],\n",
      "        ...,\n",
      "        [ 1.4252e-03, -1.8125e-03, -6.9615e-04,  ..., -6.2599e-04,\n",
      "         -7.5762e-04,  1.8204e-03],\n",
      "        [-4.7005e-04, -4.1585e-03, -4.5273e-03,  ...,  6.8420e-03,\n",
      "         -1.4296e-03,  1.0086e-02],\n",
      "        [-2.0460e-04,  1.9859e-03,  1.5541e-03,  ...,  6.2785e-04,\n",
      "          2.7445e-03,  2.8477e-03]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight tensor([[-1.7575e-03, -4.4784e-06,  2.1006e-03,  ...,  3.0802e-03,\n",
      "          1.8284e-03,  2.9692e-04],\n",
      "        [ 7.9893e-04, -7.7100e-04, -1.4667e-03,  ..., -8.9140e-05,\n",
      "         -6.7035e-03, -9.5499e-05],\n",
      "        [-1.3976e-04, -2.6037e-04, -3.6158e-04,  ..., -3.9455e-04,\n",
      "         -2.5164e-03,  5.8218e-04],\n",
      "        ...,\n",
      "        [-2.0699e-03,  5.9289e-04, -1.0685e-03,  ...,  3.9244e-03,\n",
      "          4.2064e-03, -1.9593e-04],\n",
      "        [-9.0335e-05, -6.2064e-05,  2.1021e-04,  ...,  6.0757e-04,\n",
      "          5.1226e-03,  1.1050e-03],\n",
      "        [-8.3646e-04,  2.0334e-04,  5.7931e-04,  ..., -1.8223e-04,\n",
      "          3.1716e-03, -6.8399e-04]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight tensor([[-0.0045, -0.0004, -0.0027,  ..., -0.0051,  0.0026,  0.0004],\n",
      "        [ 0.0016, -0.0008,  0.0003,  ..., -0.0006, -0.0001, -0.0010],\n",
      "        [ 0.0004, -0.0029, -0.0020,  ...,  0.0032,  0.0019, -0.0013],\n",
      "        ...,\n",
      "        [ 0.0017, -0.0007, -0.0007,  ...,  0.0002,  0.0045, -0.0031],\n",
      "        [-0.0188, -0.0018, -0.0021,  ...,  0.0062,  0.0023, -0.0119],\n",
      "        [-0.0015,  0.0003, -0.0006,  ...,  0.0013, -0.0002,  0.0015]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight tensor([[ 2.1752e-03, -7.9214e-03,  4.7791e-03,  ..., -4.0684e-03,\n",
      "          1.0348e-03,  7.5728e-03],\n",
      "        [-4.0321e-04, -3.9072e-03,  4.6564e-03,  ...,  9.5840e-04,\n",
      "          5.5512e-03, -7.1345e-03],\n",
      "        [ 3.2646e-03,  6.8386e-03,  4.8497e-04,  ...,  1.0488e-04,\n",
      "         -1.0805e-02, -5.0220e-03],\n",
      "        ...,\n",
      "        [ 1.9840e-03, -6.5218e-03,  6.2445e-03,  ..., -1.6207e-03,\n",
      "          5.6459e-03,  1.3778e-04],\n",
      "        [ 3.2500e-03, -1.5539e-03, -3.5697e-03,  ...,  1.7312e-05,\n",
      "         -5.2062e-03,  1.9522e-03],\n",
      "        [ 9.5588e-04,  7.2712e-03, -5.1071e-03,  ...,  7.0116e-03,\n",
      "         -3.9726e-03, -7.2369e-05]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight tensor([[-2.3025e-04,  1.7957e-03,  3.9814e-03,  ...,  4.3822e-03,\n",
      "         -3.1218e-04, -9.4326e-04],\n",
      "        [-3.0267e-04, -5.6036e-04,  9.2661e-05,  ..., -7.5367e-05,\n",
      "          8.6332e-05,  2.6465e-04],\n",
      "        [ 1.3370e-03, -1.0416e-03,  8.2791e-04,  ...,  3.1225e-03,\n",
      "          1.5077e-03,  2.6743e-03],\n",
      "        ...,\n",
      "        [ 1.1714e-03, -2.2488e-03, -7.5539e-04,  ..., -3.3275e-03,\n",
      "         -3.7117e-03,  1.6743e-03],\n",
      "        [-6.7041e-03, -1.2067e-02, -8.3066e-03,  ..., -1.4965e-02,\n",
      "         -1.9544e-03,  1.8395e-02],\n",
      "        [-1.1889e-03,  1.8200e-03, -8.3002e-04,  ...,  1.4045e-03,\n",
      "         -2.0074e-03, -4.9726e-04]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight tensor([[ 5.8376e-04,  3.0567e-04,  4.1868e-05,  ...,  2.5749e-03,\n",
      "          4.5270e-03, -4.9470e-04],\n",
      "        [ 2.7747e-04,  2.4777e-04, -4.7209e-04,  ...,  4.8168e-03,\n",
      "          4.2723e-03,  1.4716e-03],\n",
      "        [-1.4863e-03,  5.1250e-04, -7.5735e-05,  ..., -8.5304e-04,\n",
      "          2.9341e-03, -8.2735e-04],\n",
      "        ...,\n",
      "        [-5.2400e-04, -2.1663e-04, -1.4448e-03,  ...,  1.2543e-03,\n",
      "          5.3284e-03,  1.0563e-04],\n",
      "        [ 7.7594e-04,  6.5091e-04, -1.0614e-03,  ...,  1.0898e-03,\n",
      "          4.4832e-03,  1.4890e-04],\n",
      "        [ 8.6387e-04, -8.7781e-05, -8.6689e-05,  ...,  2.0933e-03,\n",
      "         -4.0987e-03,  7.7422e-04]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight tensor([[-1.7312e-03, -3.4673e-05,  1.8854e-03,  ..., -3.8655e-04,\n",
      "         -3.5593e-04, -2.9551e-03],\n",
      "        [ 6.7936e-04,  1.4271e-04,  3.9320e-04,  ..., -2.4545e-04,\n",
      "         -1.1348e-04,  5.1640e-04],\n",
      "        [ 4.5185e-03, -1.5502e-03,  2.6683e-03,  ...,  3.5785e-03,\n",
      "          6.8011e-04, -3.6413e-04],\n",
      "        ...,\n",
      "        [-6.0365e-04, -1.5289e-03,  1.3619e-03,  ..., -2.0259e-03,\n",
      "         -2.2951e-03, -1.5769e-03],\n",
      "        [-5.6879e-03, -8.9894e-03, -2.2759e-03,  ..., -6.7132e-03,\n",
      "         -3.1363e-03, -6.5702e-03],\n",
      "        [-1.5043e-04,  3.9474e-04,  3.2785e-03,  ..., -4.2251e-04,\n",
      "          1.6803e-04,  1.3515e-03]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight tensor([[ 0.0017, -0.0062,  0.0057,  ...,  0.0042,  0.0009,  0.0007],\n",
      "        [ 0.0037, -0.0073, -0.0019,  ..., -0.0062,  0.0034, -0.0020],\n",
      "        [ 0.0024,  0.0049,  0.0043,  ...,  0.0014, -0.0009,  0.0007],\n",
      "        ...,\n",
      "        [ 0.0030,  0.0031, -0.0078,  ..., -0.0035, -0.0033,  0.0069],\n",
      "        [-0.0023,  0.0009, -0.0044,  ..., -0.0058,  0.0062,  0.0042],\n",
      "        [ 0.0004, -0.0031,  0.0083,  ...,  0.0142,  0.0025,  0.0045]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.down.weight tensor([[ 3.0748e-03, -6.3797e-03, -2.9276e-03,  ...,  5.5910e-03,\n",
      "          2.6605e-03,  8.3218e-03],\n",
      "        [-3.4510e-04, -7.6221e-04, -7.0846e-04,  ...,  4.1376e-05,\n",
      "          7.6228e-04, -3.9828e-04],\n",
      "        [ 5.2317e-04,  5.4110e-03, -3.8290e-03,  ...,  8.2068e-04,\n",
      "          2.8072e-03, -6.2828e-03],\n",
      "        ...,\n",
      "        [-6.7449e-04, -7.2711e-03,  1.6051e-04,  ...,  1.1309e-03,\n",
      "          1.0206e-03, -4.2895e-03],\n",
      "        [ 5.8124e-03,  2.7239e-02,  4.5281e-03,  ...,  3.0359e-03,\n",
      "          3.8086e-03, -1.8516e-03],\n",
      "        [-4.1616e-03, -3.2338e-03,  7.9155e-04,  ..., -6.3793e-03,\n",
      "          8.0513e-03,  3.5995e-04]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.up.weight tensor([[-3.5696e-03, -4.8468e-03, -5.5432e-03,  ...,  2.1405e-03,\n",
      "          5.9534e-03, -1.5966e-03],\n",
      "        [ 4.2452e-03, -4.0663e-04,  6.2075e-03,  ..., -1.9519e-03,\n",
      "          7.1689e-03, -8.1471e-04],\n",
      "        [-4.5975e-05,  1.0328e-02,  1.9704e-03,  ..., -9.2127e-03,\n",
      "         -1.1735e-03, -1.8981e-04],\n",
      "        ...,\n",
      "        [ 2.3327e-03,  1.1132e-04, -1.0760e-03,  ..., -1.8650e-03,\n",
      "          4.5439e-03,  2.4768e-03],\n",
      "        [ 4.7555e-04,  6.0349e-03, -5.8572e-04,  ...,  7.9443e-03,\n",
      "          2.8781e-03,  8.6264e-03],\n",
      "        [ 3.1386e-04, -2.8372e-03, -1.2759e-03,  ...,  1.2387e-03,\n",
      "         -7.2587e-04, -1.2292e-02]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.lora.down.weight tensor([[ 9.4117e-04,  2.3765e-03, -1.0735e-03,  ..., -6.1176e-04,\n",
      "          4.7703e-03, -1.2309e-03],\n",
      "        [ 1.5823e-03, -2.3154e-05, -1.6510e-03,  ...,  4.6861e-04,\n",
      "         -1.9670e-03, -4.6251e-04],\n",
      "        [-1.6168e-03,  2.8821e-03,  9.6073e-04,  ..., -2.4401e-03,\n",
      "         -1.9804e-03,  1.2850e-03],\n",
      "        ...,\n",
      "        [ 2.6159e-04, -3.1301e-03,  5.2313e-03,  ..., -3.4718e-03,\n",
      "         -3.7498e-03,  1.6093e-03],\n",
      "        [ 1.1787e-02,  9.7134e-03,  6.6415e-03,  ..., -1.4324e-02,\n",
      "          5.8045e-03, -2.8938e-03],\n",
      "        [-4.8793e-04,  8.2553e-04,  1.4492e-04,  ...,  2.0315e-03,\n",
      "         -3.7666e-03,  4.2638e-03]])\n",
      "unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.lora.up.weight tensor([[-8.3186e-03, -9.9069e-03, -3.0861e-03,  ..., -1.5557e-03,\n",
      "          4.4580e-03, -8.4889e-03],\n",
      "        [-5.2233e-03,  3.2908e-03,  3.1484e-03,  ...,  1.9869e-03,\n",
      "         -2.3225e-03, -2.2721e-03],\n",
      "        [-6.0582e-03, -3.2971e-03, -6.6939e-04,  ...,  1.8518e-03,\n",
      "          4.2475e-03,  5.7548e-03],\n",
      "        ...,\n",
      "        [ 1.7288e-02,  3.1407e-03,  9.3960e-03,  ..., -6.1078e-03,\n",
      "          7.1512e-03, -3.7009e-03],\n",
      "        [ 6.6772e-05,  5.1036e-04, -2.8626e-03,  ..., -4.3036e-03,\n",
      "         -2.2894e-02, -4.9003e-03],\n",
      "        [ 6.0351e-03, -5.2125e-05, -9.8887e-03,  ...,  2.0271e-03,\n",
      "          6.0621e-03,  8.6647e-03]])\n",
      "unet.up_blocks.3.attentions.0.proj_in.lora.down.weight tensor([[[[-2.9704e-04]],\n",
      "\n",
      "         [[-3.5825e-03]],\n",
      "\n",
      "         [[-7.4092e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.8989e-04]],\n",
      "\n",
      "         [[-1.4438e-03]],\n",
      "\n",
      "         [[ 5.1857e-04]]],\n",
      "\n",
      "\n",
      "        [[[-9.4286e-05]],\n",
      "\n",
      "         [[ 6.4682e-04]],\n",
      "\n",
      "         [[-1.3171e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.3897e-04]],\n",
      "\n",
      "         [[-1.0808e-04]],\n",
      "\n",
      "         [[-5.0275e-04]]],\n",
      "\n",
      "\n",
      "        [[[-4.7984e-04]],\n",
      "\n",
      "         [[ 3.0127e-03]],\n",
      "\n",
      "         [[ 4.0197e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.2398e-04]],\n",
      "\n",
      "         [[-2.6657e-03]],\n",
      "\n",
      "         [[-1.3750e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.2393e-04]],\n",
      "\n",
      "         [[-3.0927e-03]],\n",
      "\n",
      "         [[-2.1095e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3039e-03]],\n",
      "\n",
      "         [[-2.0808e-04]],\n",
      "\n",
      "         [[ 1.5445e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2140e-03]],\n",
      "\n",
      "         [[-6.7254e-03]],\n",
      "\n",
      "         [[ 7.8874e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.4230e-03]],\n",
      "\n",
      "         [[-2.6115e-03]],\n",
      "\n",
      "         [[ 1.0379e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.7176e-03]],\n",
      "\n",
      "         [[ 1.1554e-03]],\n",
      "\n",
      "         [[ 3.1776e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8925e-03]],\n",
      "\n",
      "         [[ 3.5800e-04]],\n",
      "\n",
      "         [[ 1.7715e-04]]]])\n",
      "unet.up_blocks.3.attentions.0.proj_in.lora.up.weight tensor([[[[-5.6429e-04]],\n",
      "\n",
      "         [[-2.9892e-03]],\n",
      "\n",
      "         [[-4.0488e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.9237e-03]],\n",
      "\n",
      "         [[ 6.2249e-03]],\n",
      "\n",
      "         [[ 1.4288e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.3319e-04]],\n",
      "\n",
      "         [[ 6.0361e-03]],\n",
      "\n",
      "         [[-9.5847e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.8592e-03]],\n",
      "\n",
      "         [[-2.9655e-03]],\n",
      "\n",
      "         [[-3.5364e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.5710e-03]],\n",
      "\n",
      "         [[-7.1399e-03]],\n",
      "\n",
      "         [[-2.6870e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.1916e-03]],\n",
      "\n",
      "         [[-7.0099e-03]],\n",
      "\n",
      "         [[ 2.6902e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 6.1882e-03]],\n",
      "\n",
      "         [[ 5.0317e-03]],\n",
      "\n",
      "         [[ 4.3596e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3539e-03]],\n",
      "\n",
      "         [[ 2.5780e-04]],\n",
      "\n",
      "         [[ 1.1621e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.9191e-03]],\n",
      "\n",
      "         [[ 1.1327e-03]],\n",
      "\n",
      "         [[ 1.9750e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.5162e-03]],\n",
      "\n",
      "         [[-2.6365e-03]],\n",
      "\n",
      "         [[-2.9534e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.3138e-03]],\n",
      "\n",
      "         [[-6.8288e-05]],\n",
      "\n",
      "         [[ 3.2774e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.4097e-03]],\n",
      "\n",
      "         [[ 6.0005e-03]],\n",
      "\n",
      "         [[-5.1414e-04]]]])\n",
      "unet.up_blocks.3.attentions.0.proj_out.lora.down.weight tensor([[[[-0.0036]],\n",
      "\n",
      "         [[ 0.0081]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0042]],\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         [[ 0.0005]]],\n",
      "\n",
      "\n",
      "        [[[-0.0013]],\n",
      "\n",
      "         [[ 0.0007]],\n",
      "\n",
      "         [[-0.0001]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[ 0.0001]],\n",
      "\n",
      "         [[-0.0001]]],\n",
      "\n",
      "\n",
      "        [[[-0.0030]],\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0042]],\n",
      "\n",
      "         [[-0.0048]],\n",
      "\n",
      "         [[ 0.0032]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0032]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         [[-0.0036]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0012]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         [[ 0.0002]]],\n",
      "\n",
      "\n",
      "        [[[-0.0054]],\n",
      "\n",
      "         [[ 0.0110]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0042]],\n",
      "\n",
      "         [[-0.0206]],\n",
      "\n",
      "         [[ 0.0057]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0028]],\n",
      "\n",
      "         [[-0.0036]],\n",
      "\n",
      "         [[-0.0030]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[-0.0004]]]])\n",
      "unet.up_blocks.3.attentions.0.proj_out.lora.up.weight tensor([[[[-2.6835e-03]],\n",
      "\n",
      "         [[ 2.7549e-03]],\n",
      "\n",
      "         [[ 5.8249e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.3005e-03]],\n",
      "\n",
      "         [[ 7.3834e-03]],\n",
      "\n",
      "         [[ 2.4851e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.3129e-06]],\n",
      "\n",
      "         [[ 1.7996e-03]],\n",
      "\n",
      "         [[-2.3285e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.0747e-04]],\n",
      "\n",
      "         [[-4.3283e-03]],\n",
      "\n",
      "         [[-1.9710e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.5116e-03]],\n",
      "\n",
      "         [[ 3.4045e-03]],\n",
      "\n",
      "         [[-2.6818e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0301e-02]],\n",
      "\n",
      "         [[-2.0932e-03]],\n",
      "\n",
      "         [[-5.4722e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 4.3803e-03]],\n",
      "\n",
      "         [[-2.8570e-03]],\n",
      "\n",
      "         [[ 3.2252e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.4722e-03]],\n",
      "\n",
      "         [[ 4.3426e-03]],\n",
      "\n",
      "         [[ 1.7769e-04]]],\n",
      "\n",
      "\n",
      "        [[[-6.6074e-03]],\n",
      "\n",
      "         [[ 4.3869e-03]],\n",
      "\n",
      "         [[ 4.7322e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.5524e-03]],\n",
      "\n",
      "         [[-1.8792e-03]],\n",
      "\n",
      "         [[-5.5921e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.1417e-03]],\n",
      "\n",
      "         [[ 3.3701e-04]],\n",
      "\n",
      "         [[ 5.4005e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.2160e-03]],\n",
      "\n",
      "         [[-3.6533e-03]],\n",
      "\n",
      "         [[ 4.0511e-03]]]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight tensor([[ 0.0022, -0.0043,  0.0016,  ..., -0.0008, -0.0011, -0.0023],\n",
      "        [ 0.0007, -0.0002, -0.0005,  ...,  0.0006,  0.0011,  0.0006],\n",
      "        [-0.0021, -0.0020, -0.0020,  ..., -0.0002, -0.0030,  0.0007],\n",
      "        ...,\n",
      "        [ 0.0028,  0.0003,  0.0023,  ...,  0.0007, -0.0049, -0.0002],\n",
      "        [-0.0020,  0.0071, -0.0046,  ..., -0.0023, -0.0035, -0.0084],\n",
      "        [-0.0020,  0.0015, -0.0003,  ..., -0.0012,  0.0049, -0.0006]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight tensor([[-1.1276e-02,  1.6035e-03,  1.0812e-03,  ...,  3.5353e-03,\n",
      "          3.9275e-03,  1.0715e-03],\n",
      "        [ 4.2143e-03,  5.1437e-04, -2.5374e-03,  ...,  1.2231e-03,\n",
      "         -1.8050e-03, -6.7935e-03],\n",
      "        [-2.7682e-03, -2.3786e-03,  3.7557e-03,  ..., -2.0901e-03,\n",
      "         -1.8634e-03, -1.1254e-03],\n",
      "        ...,\n",
      "        [ 4.2964e-03, -2.8813e-03,  1.3631e-02,  ...,  7.4981e-04,\n",
      "          2.9336e-03,  1.5291e-03],\n",
      "        [-3.0694e-03, -6.2017e-05,  5.6652e-03,  ..., -1.4842e-03,\n",
      "          7.5436e-03, -1.0033e-02],\n",
      "        [ 4.4620e-03,  9.3873e-04, -8.4388e-03,  ...,  1.5632e-03,\n",
      "         -5.5852e-04, -2.7825e-03]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight tensor([[ 2.3102e-04, -2.2432e-03,  6.2311e-05,  ...,  5.7497e-04,\n",
      "         -1.0527e-03, -3.0774e-03],\n",
      "        [-5.3681e-04, -4.1538e-04, -6.7521e-04,  ...,  1.3870e-03,\n",
      "          1.1009e-03,  3.7798e-04],\n",
      "        [ 4.3418e-03, -1.1447e-03, -1.0312e-03,  ...,  6.7915e-04,\n",
      "         -2.1063e-03, -1.7176e-03],\n",
      "        ...,\n",
      "        [ 2.7787e-03, -2.0336e-04,  2.8838e-04,  ...,  1.6800e-03,\n",
      "         -2.3579e-03, -1.3877e-03],\n",
      "        [ 5.0126e-03, -5.5155e-03, -1.2733e-02,  ..., -5.7784e-03,\n",
      "         -5.0109e-03,  1.8560e-03],\n",
      "        [-1.3771e-03, -3.6815e-03,  1.9470e-03,  ...,  1.8082e-03,\n",
      "          4.2844e-04,  1.6749e-03]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight tensor([[-2.2563e-03,  1.2333e-02,  5.1973e-03,  ..., -1.7154e-03,\n",
      "          5.6850e-03, -7.5853e-04],\n",
      "        [ 8.6221e-04, -5.3811e-03,  3.0199e-03,  ..., -3.1718e-04,\n",
      "          4.4985e-04,  2.6647e-03],\n",
      "        [ 1.7152e-03,  4.6177e-05,  1.1490e-03,  ..., -4.6242e-03,\n",
      "          5.2833e-03, -3.6440e-03],\n",
      "        ...,\n",
      "        [ 1.1839e-03,  5.0517e-03,  1.3417e-03,  ..., -2.0420e-03,\n",
      "          6.1980e-04,  6.3754e-03],\n",
      "        [ 2.3144e-04, -1.4828e-03,  7.3475e-03,  ..., -2.0994e-03,\n",
      "         -2.9184e-03,  1.0020e-03],\n",
      "        [-2.9606e-03, -1.0506e-04, -5.3398e-03,  ..., -1.3999e-04,\n",
      "          3.0206e-03, -3.4986e-03]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight tensor([[ 8.1459e-03, -5.2832e-04,  5.0281e-03,  ...,  2.2443e-03,\n",
      "         -3.3910e-03, -5.1866e-03],\n",
      "        [-9.4796e-04, -9.6993e-05, -1.2548e-03,  ..., -8.4200e-04,\n",
      "          1.0751e-03,  2.6869e-04],\n",
      "        [ 4.2566e-04, -3.2907e-03,  2.8595e-03,  ...,  1.7971e-04,\n",
      "         -1.8481e-03, -3.8358e-03],\n",
      "        ...,\n",
      "        [ 1.2823e-03,  1.8084e-03,  3.7885e-03,  ..., -2.2974e-03,\n",
      "         -3.5822e-03,  4.0978e-04],\n",
      "        [-1.6062e-02, -9.7742e-04, -7.3107e-03,  ..., -5.8293e-05,\n",
      "         -1.5004e-03,  3.7582e-03],\n",
      "        [-1.4246e-03,  7.9842e-04,  6.1187e-04,  ...,  2.0561e-03,\n",
      "         -1.9031e-03,  1.6622e-03]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight tensor([[ 4.5847e-03,  2.4728e-03, -7.7772e-04,  ...,  3.7608e-03,\n",
      "         -5.3792e-03, -2.8193e-03],\n",
      "        [ 1.1020e-02,  1.6800e-03,  3.8916e-03,  ..., -3.4073e-03,\n",
      "          9.4547e-03, -2.8370e-04],\n",
      "        [ 6.7839e-04, -6.1576e-03,  5.8055e-03,  ..., -8.2925e-05,\n",
      "          2.7118e-03, -1.9812e-04],\n",
      "        ...,\n",
      "        [ 1.3949e-03, -1.3218e-03, -3.7045e-03,  ..., -6.5058e-03,\n",
      "         -5.6040e-03, -3.3664e-04],\n",
      "        [-8.1410e-03,  1.5463e-03,  2.5744e-04,  ..., -3.3058e-03,\n",
      "         -6.3856e-03,  2.4124e-03],\n",
      "        [-7.3917e-04, -2.8164e-03, -3.7597e-03,  ...,  2.8952e-03,\n",
      "         -8.9977e-03, -3.5496e-03]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight tensor([[ 2.1498e-03, -8.8147e-04,  1.3185e-03,  ...,  3.4672e-03,\n",
      "          1.5335e-03, -2.9741e-03],\n",
      "        [-1.4451e-03, -1.3490e-03,  1.3352e-03,  ...,  4.1035e-04,\n",
      "         -6.2447e-04,  3.8997e-04],\n",
      "        [ 1.2310e-03,  2.0087e-03,  2.0837e-03,  ..., -8.4002e-05,\n",
      "          6.2126e-03, -3.8610e-03],\n",
      "        ...,\n",
      "        [ 1.1030e-03,  2.3806e-03, -3.6079e-03,  ..., -2.1341e-04,\n",
      "         -2.4461e-03,  5.2515e-04],\n",
      "        [-1.8140e-03,  6.0959e-03, -4.1122e-03,  ...,  6.6909e-04,\n",
      "          4.5203e-04,  8.5227e-03],\n",
      "        [ 1.2023e-03,  2.0172e-03, -7.4014e-04,  ...,  9.3953e-04,\n",
      "         -4.2375e-04,  2.9733e-03]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight tensor([[-1.8656e-03, -3.1967e-03, -1.5873e-03,  ..., -6.1294e-03,\n",
      "         -6.0870e-03, -5.1594e-03],\n",
      "        [ 2.5361e-04,  6.2308e-03,  5.2193e-03,  ..., -1.5751e-03,\n",
      "          5.0426e-03, -5.6764e-03],\n",
      "        [ 7.5297e-03, -3.5481e-04, -4.3194e-03,  ..., -2.7924e-04,\n",
      "          4.6226e-03,  4.6502e-03],\n",
      "        ...,\n",
      "        [ 1.4381e-03, -1.0810e-03,  5.2376e-03,  ..., -8.3206e-04,\n",
      "         -7.4554e-03, -3.1155e-03],\n",
      "        [ 4.6828e-03, -2.0892e-03,  3.7578e-03,  ...,  1.0978e-03,\n",
      "         -1.4317e-03,  3.2156e-03],\n",
      "        [ 1.4728e-03,  8.8110e-04,  9.9762e-04,  ..., -2.5399e-03,\n",
      "          6.6363e-05,  5.6085e-04]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight tensor([[-1.1537e-04,  2.6336e-03, -1.0082e-03,  ...,  1.5004e-03,\n",
      "          5.7602e-04, -2.1956e-03],\n",
      "        [ 3.2754e-04, -6.1305e-04,  1.3908e-04,  ...,  2.2580e-04,\n",
      "         -3.0526e-04,  3.9969e-04],\n",
      "        [ 1.9367e-03, -7.6680e-04, -8.6958e-04,  ...,  2.9890e-03,\n",
      "         -1.7227e-03, -1.5467e-03],\n",
      "        ...,\n",
      "        [-4.5706e-04,  2.3065e-03,  7.1503e-05,  ..., -1.3958e-03,\n",
      "         -1.4056e-03, -1.1383e-03],\n",
      "        [ 1.5035e-03, -3.9204e-03,  4.2258e-03,  ...,  1.4716e-03,\n",
      "         -2.2454e-03,  1.8828e-03],\n",
      "        [ 1.5817e-03,  5.7872e-04, -1.2042e-03,  ...,  6.2427e-04,\n",
      "          6.3132e-04,  3.0355e-03]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight tensor([[-6.3831e-04,  7.8847e-04, -2.1371e-04,  ...,  2.8133e-04,\n",
      "         -3.1756e-03,  7.7815e-05],\n",
      "        [ 8.0639e-04, -8.6731e-04, -4.2078e-05,  ..., -6.6790e-04,\n",
      "          1.7044e-03,  3.2337e-04],\n",
      "        [-6.5563e-04,  1.2286e-05,  4.2798e-04,  ...,  9.5113e-04,\n",
      "          1.4623e-04,  4.9603e-04],\n",
      "        ...,\n",
      "        [-1.4532e-03, -8.1081e-04, -2.1394e-04,  ...,  2.1083e-03,\n",
      "          8.5958e-05, -1.0524e-03],\n",
      "        [-1.8401e-03, -5.4771e-04,  7.0251e-05,  ...,  6.7541e-05,\n",
      "          2.4709e-04,  1.5287e-04],\n",
      "        [-4.3869e-04, -2.4813e-04, -9.2750e-04,  ...,  6.7394e-04,\n",
      "         -1.8145e-04, -2.4809e-04]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight tensor([[ 1.9654e-03, -2.1518e-03,  6.4606e-05,  ...,  4.0038e-03,\n",
      "          3.7903e-03, -1.5461e-03],\n",
      "        [ 9.3403e-04,  5.7468e-04, -4.0616e-04,  ...,  1.0909e-04,\n",
      "         -2.9344e-04, -8.1323e-05],\n",
      "        [-2.7808e-03, -2.4545e-03, -2.0857e-03,  ..., -1.9232e-03,\n",
      "          1.7728e-03,  2.4365e-03],\n",
      "        ...,\n",
      "        [ 3.7746e-03,  8.4994e-04, -1.3875e-04,  ...,  2.0775e-03,\n",
      "         -1.3806e-03,  2.4218e-03],\n",
      "        [ 3.7552e-03, -7.5251e-03, -8.5262e-04,  ..., -2.6681e-03,\n",
      "          3.9895e-03,  7.6042e-03],\n",
      "        [-1.7276e-03,  5.3161e-03, -2.9153e-03,  ..., -2.8713e-03,\n",
      "         -2.1773e-03, -2.5261e-04]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight tensor([[-0.0004,  0.0051, -0.0028,  ..., -0.0015,  0.0056,  0.0033],\n",
      "        [ 0.0004, -0.0014,  0.0081,  ..., -0.0024,  0.0007,  0.0008],\n",
      "        [ 0.0079, -0.0021, -0.0012,  ...,  0.0023,  0.0015,  0.0099],\n",
      "        ...,\n",
      "        [ 0.0021, -0.0033, -0.0003,  ..., -0.0029,  0.0048,  0.0007],\n",
      "        [ 0.0001,  0.0089, -0.0057,  ...,  0.0005, -0.0006, -0.0097],\n",
      "        [ 0.0041, -0.0034,  0.0036,  ..., -0.0002, -0.0025, -0.0036]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight tensor([[-0.0002, -0.0003, -0.0008,  ..., -0.0027, -0.0014, -0.0015],\n",
      "        [-0.0003,  0.0004,  0.0002,  ..., -0.0009,  0.0002,  0.0002],\n",
      "        [-0.0005, -0.0031, -0.0003,  ...,  0.0034,  0.0010,  0.0009],\n",
      "        ...,\n",
      "        [ 0.0013,  0.0007,  0.0033,  ...,  0.0019, -0.0012, -0.0026],\n",
      "        [-0.0045, -0.0045, -0.0020,  ...,  0.0040,  0.0019,  0.0066],\n",
      "        [-0.0007, -0.0011, -0.0033,  ...,  0.0032,  0.0008,  0.0006]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight tensor([[ 1.2564e-03,  1.1157e-03,  1.2586e-03,  ...,  7.5369e-04,\n",
      "          3.0338e-04, -1.7474e-03],\n",
      "        [ 4.9530e-04,  1.6739e-03,  2.4743e-04,  ..., -1.3010e-03,\n",
      "          4.7681e-03, -3.5627e-04],\n",
      "        [-1.7030e-05, -6.9106e-04,  6.0757e-05,  ..., -6.7341e-04,\n",
      "          2.3748e-04,  7.5927e-04],\n",
      "        ...,\n",
      "        [-2.3608e-03,  1.6009e-03, -5.0858e-03,  ..., -9.0432e-04,\n",
      "          4.2808e-05, -7.5953e-04],\n",
      "        [ 6.5688e-04, -1.4161e-03,  6.0979e-04,  ..., -1.7816e-04,\n",
      "         -3.8688e-03,  1.9228e-03],\n",
      "        [-2.2410e-04,  3.4909e-05,  1.8431e-04,  ...,  6.4959e-04,\n",
      "         -1.7002e-03, -3.7893e-04]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight tensor([[ 1.7898e-03, -4.3800e-03,  3.5846e-04,  ...,  4.3301e-04,\n",
      "         -2.0789e-03,  3.8734e-03],\n",
      "        [ 5.1089e-05,  9.8977e-04, -4.0455e-06,  ..., -5.2055e-04,\n",
      "         -9.7416e-05, -7.9682e-04],\n",
      "        [-1.5761e-04,  2.7937e-04, -7.1192e-04,  ...,  3.9471e-05,\n",
      "          1.6353e-03, -7.3468e-05],\n",
      "        ...,\n",
      "        [-3.7739e-03, -2.2809e-03,  4.7904e-04,  ...,  8.0671e-04,\n",
      "         -9.3660e-04,  2.4084e-03],\n",
      "        [-1.8824e-03, -4.7976e-03,  8.0203e-03,  ...,  3.3823e-03,\n",
      "          2.6935e-03, -3.7110e-03],\n",
      "        [-2.2054e-03, -1.9877e-03, -1.8178e-03,  ..., -2.9735e-03,\n",
      "          1.5836e-03,  2.6683e-03]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight tensor([[-4.1299e-04, -3.5676e-03,  1.9420e-03,  ...,  2.7209e-03,\n",
      "          2.5038e-03, -5.2719e-03],\n",
      "        [-1.7656e-03,  9.1162e-04, -4.4482e-03,  ..., -3.3707e-05,\n",
      "         -2.7838e-03,  8.0920e-04],\n",
      "        [ 1.1780e-03,  5.8582e-03, -1.0446e-03,  ..., -2.7263e-03,\n",
      "         -7.8998e-04,  1.1976e-03],\n",
      "        ...,\n",
      "        [-2.6152e-03,  6.2094e-04,  3.4687e-04,  ..., -1.8001e-03,\n",
      "          2.5109e-03, -2.6528e-04],\n",
      "        [-1.7289e-03, -1.4470e-03,  1.0634e-03,  ...,  9.0555e-05,\n",
      "          2.7702e-03, -6.4789e-03],\n",
      "        [-3.0795e-03, -3.8861e-03,  4.8317e-03,  ...,  2.3729e-04,\n",
      "         -2.5187e-03,  2.6274e-03]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight tensor([[-2.2638e-03, -2.0048e-03,  4.4885e-03,  ..., -4.7226e-03,\n",
      "          4.2139e-03,  9.4597e-04],\n",
      "        [-8.9108e-04, -4.6457e-04,  5.8494e-04,  ..., -5.9507e-04,\n",
      "          2.1590e-04, -2.9408e-04],\n",
      "        [ 2.6603e-03,  8.9581e-04, -1.0579e-03,  ...,  5.6774e-04,\n",
      "          2.3499e-05,  4.7092e-04],\n",
      "        ...,\n",
      "        [-5.6375e-04,  1.1398e-03, -1.4701e-03,  ..., -3.8915e-04,\n",
      "         -2.2383e-03, -4.1111e-04],\n",
      "        [-2.9380e-03,  1.2298e-02, -9.7376e-03,  ...,  1.1597e-02,\n",
      "          2.3438e-03,  3.0860e-03],\n",
      "        [-2.1625e-04, -6.4511e-04,  3.6997e-03,  ..., -2.4954e-04,\n",
      "          2.2534e-03, -3.0009e-03]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight tensor([[-6.1369e-04, -3.7667e-03, -4.8214e-05,  ..., -5.6442e-03,\n",
      "          2.8240e-03, -2.5931e-03],\n",
      "        [ 3.6614e-03, -1.1420e-03, -2.7714e-03,  ...,  3.4012e-03,\n",
      "         -5.6403e-03,  6.9750e-03],\n",
      "        [-3.4962e-03,  8.1487e-04,  4.1332e-03,  ..., -1.9186e-03,\n",
      "         -4.5566e-03, -3.1392e-03],\n",
      "        ...,\n",
      "        [-3.8514e-03,  1.7801e-03, -8.7222e-03,  ..., -1.9567e-03,\n",
      "          1.3388e-04,  3.2685e-03],\n",
      "        [-4.3632e-03, -6.2643e-03,  1.6409e-03,  ..., -4.2099e-03,\n",
      "          4.8566e-03, -4.9573e-03],\n",
      "        [-4.6354e-04,  4.4612e-03,  4.8955e-03,  ...,  4.7319e-03,\n",
      "          8.1519e-04,  6.0817e-03]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight tensor([[ 4.6420e-03, -2.0961e-04,  5.4953e-03,  ...,  5.2026e-03,\n",
      "          2.0606e-03, -1.2065e-03],\n",
      "        [ 4.1283e-04,  4.5186e-04, -6.9809e-04,  ...,  6.7944e-04,\n",
      "          6.4012e-04, -3.9501e-05],\n",
      "        [ 2.4212e-03, -1.1140e-04, -5.0879e-03,  ..., -1.0852e-03,\n",
      "         -8.7702e-04,  1.9172e-03],\n",
      "        ...,\n",
      "        [-4.1167e-03, -8.4471e-04,  3.3122e-03,  ..., -1.7597e-03,\n",
      "          1.6319e-03, -3.7738e-03],\n",
      "        [ 2.6035e-03, -1.0360e-02, -5.6641e-03,  ...,  3.4865e-04,\n",
      "          1.2382e-02, -1.7323e-02],\n",
      "        [ 1.9708e-03,  1.4563e-03, -7.8612e-04,  ...,  2.1152e-03,\n",
      "          2.6397e-04,  1.9600e-03]])\n",
      "unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight tensor([[-4.7403e-04,  6.3556e-03, -3.8314e-03,  ..., -9.0258e-05,\n",
      "          2.4794e-03, -4.8615e-04],\n",
      "        [-3.2807e-03,  1.9187e-03, -3.6326e-03,  ..., -2.2321e-03,\n",
      "          7.2083e-04, -2.4889e-03],\n",
      "        [-8.3363e-04,  1.1149e-03,  6.2296e-04,  ...,  4.4608e-03,\n",
      "          2.0525e-02,  3.4584e-03],\n",
      "        ...,\n",
      "        [-8.4809e-03,  4.1741e-03, -4.0184e-03,  ...,  1.4187e-03,\n",
      "          7.2167e-03,  3.0536e-03],\n",
      "        [ 1.8066e-03, -1.3576e-03, -1.1424e-03,  ...,  3.7510e-04,\n",
      "         -7.2134e-03, -5.4681e-03],\n",
      "        [-4.4148e-03,  1.5693e-03, -6.2276e-03,  ..., -5.4358e-04,\n",
      "         -4.9586e-03, -3.1162e-03]])\n",
      "unet.up_blocks.3.attentions.1.proj_in.lora.down.weight tensor([[[[-5.1528e-03]],\n",
      "\n",
      "         [[-5.2304e-03]],\n",
      "\n",
      "         [[-5.2193e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.7694e-03]],\n",
      "\n",
      "         [[-5.8853e-03]],\n",
      "\n",
      "         [[-6.9481e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1527e-04]],\n",
      "\n",
      "         [[ 2.7260e-04]],\n",
      "\n",
      "         [[-6.5416e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.0926e-04]],\n",
      "\n",
      "         [[-3.4242e-05]],\n",
      "\n",
      "         [[ 1.0886e-04]]],\n",
      "\n",
      "\n",
      "        [[[-3.9658e-03]],\n",
      "\n",
      "         [[ 5.6541e-04]],\n",
      "\n",
      "         [[-2.3596e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.4473e-04]],\n",
      "\n",
      "         [[ 1.0005e-03]],\n",
      "\n",
      "         [[-2.6236e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 7.5723e-04]],\n",
      "\n",
      "         [[ 6.5975e-04]],\n",
      "\n",
      "         [[ 1.2860e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7709e-03]],\n",
      "\n",
      "         [[ 2.7487e-03]],\n",
      "\n",
      "         [[-4.3694e-05]]],\n",
      "\n",
      "\n",
      "        [[[-9.3419e-04]],\n",
      "\n",
      "         [[-8.7892e-04]],\n",
      "\n",
      "         [[ 5.4191e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2801e-02]],\n",
      "\n",
      "         [[ 5.6602e-03]],\n",
      "\n",
      "         [[ 8.5081e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.7505e-03]],\n",
      "\n",
      "         [[ 4.6586e-04]],\n",
      "\n",
      "         [[ 1.5358e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.0586e-04]],\n",
      "\n",
      "         [[-2.0185e-03]],\n",
      "\n",
      "         [[-3.0230e-03]]]])\n",
      "unet.up_blocks.3.attentions.1.proj_in.lora.up.weight tensor([[[[ 5.1399e-03]],\n",
      "\n",
      "         [[ 2.4077e-03]],\n",
      "\n",
      "         [[-3.4974e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6579e-04]],\n",
      "\n",
      "         [[-3.3721e-03]],\n",
      "\n",
      "         [[-2.8031e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4424e-03]],\n",
      "\n",
      "         [[-3.7650e-03]],\n",
      "\n",
      "         [[-2.3823e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.6757e-03]],\n",
      "\n",
      "         [[ 7.8545e-03]],\n",
      "\n",
      "         [[ 3.4132e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.8610e-03]],\n",
      "\n",
      "         [[-2.9338e-03]],\n",
      "\n",
      "         [[ 3.3981e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.1807e-03]],\n",
      "\n",
      "         [[-6.9151e-03]],\n",
      "\n",
      "         [[-2.5346e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-8.2644e-03]],\n",
      "\n",
      "         [[ 3.9971e-04]],\n",
      "\n",
      "         [[ 9.3992e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.0283e-04]],\n",
      "\n",
      "         [[-6.9171e-03]],\n",
      "\n",
      "         [[-8.6669e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 9.4572e-03]],\n",
      "\n",
      "         [[ 1.3000e-05]],\n",
      "\n",
      "         [[ 1.9385e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.5967e-03]],\n",
      "\n",
      "         [[-2.4504e-04]],\n",
      "\n",
      "         [[-3.6831e-03]]],\n",
      "\n",
      "\n",
      "        [[[-5.8838e-03]],\n",
      "\n",
      "         [[ 4.5251e-03]],\n",
      "\n",
      "         [[ 5.0425e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.0776e-04]],\n",
      "\n",
      "         [[ 6.4779e-04]],\n",
      "\n",
      "         [[-5.1285e-03]]]])\n",
      "unet.up_blocks.3.attentions.1.proj_out.lora.down.weight tensor([[[[-0.0021]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0028]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[ 0.0082]]],\n",
      "\n",
      "\n",
      "        [[[-0.0003]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[-0.0015]]],\n",
      "\n",
      "\n",
      "        [[[-0.0030]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0032]],\n",
      "\n",
      "         [[ 0.0024]],\n",
      "\n",
      "         [[ 0.0043]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0028]],\n",
      "\n",
      "         [[ 0.0033]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[ 0.0022]],\n",
      "\n",
      "         [[-0.0005]]],\n",
      "\n",
      "\n",
      "        [[[-0.0124]],\n",
      "\n",
      "         [[-0.0029]],\n",
      "\n",
      "         [[-0.0136]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0102]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[ 0.0141]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0010]],\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         [[-0.0001]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[ 0.0007]],\n",
      "\n",
      "         [[ 0.0008]]]])\n",
      "unet.up_blocks.3.attentions.1.proj_out.lora.up.weight tensor([[[[-0.0047]],\n",
      "\n",
      "         [[ 0.0055]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         [[-0.0068]],\n",
      "\n",
      "         [[ 0.0019]]],\n",
      "\n",
      "\n",
      "        [[[-0.0196]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0033]],\n",
      "\n",
      "         [[-0.0115]],\n",
      "\n",
      "         [[-0.0030]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0051]],\n",
      "\n",
      "         [[ 0.0030]],\n",
      "\n",
      "         [[-0.0037]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0005]],\n",
      "\n",
      "         [[ 0.0062]],\n",
      "\n",
      "         [[ 0.0042]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0035]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[ 0.0030]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0007]],\n",
      "\n",
      "         [[ 0.0089]],\n",
      "\n",
      "         [[-0.0036]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0058]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[-0.0069]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0002]],\n",
      "\n",
      "         [[-0.0060]],\n",
      "\n",
      "         [[ 0.0080]]],\n",
      "\n",
      "\n",
      "        [[[-0.0094]],\n",
      "\n",
      "         [[ 0.0051]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[-0.0047]],\n",
      "\n",
      "         [[ 0.0008]]]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight tensor([[ 0.0066, -0.0029, -0.0026,  ...,  0.0019, -0.0050,  0.0031],\n",
      "        [ 0.0005,  0.0001,  0.0014,  ...,  0.0005, -0.0003,  0.0007],\n",
      "        [ 0.0002,  0.0009, -0.0022,  ...,  0.0024,  0.0013,  0.0003],\n",
      "        ...,\n",
      "        [ 0.0005,  0.0054, -0.0024,  ..., -0.0014, -0.0005, -0.0018],\n",
      "        [-0.0121,  0.0254, -0.0016,  ...,  0.0086,  0.0024, -0.0049],\n",
      "        [ 0.0012,  0.0020, -0.0007,  ..., -0.0026,  0.0010, -0.0001]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight tensor([[-0.0023,  0.0012,  0.0002,  ..., -0.0044, -0.0036,  0.0027],\n",
      "        [ 0.0020,  0.0025,  0.0024,  ...,  0.0003, -0.0015,  0.0033],\n",
      "        [ 0.0034,  0.0002, -0.0009,  ...,  0.0012, -0.0025,  0.0015],\n",
      "        ...,\n",
      "        [ 0.0018,  0.0019,  0.0061,  ...,  0.0054, -0.0101,  0.0025],\n",
      "        [ 0.0050, -0.0036, -0.0029,  ..., -0.0004, -0.0070,  0.0036],\n",
      "        [ 0.0023, -0.0007,  0.0026,  ...,  0.0002, -0.0073,  0.0042]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight tensor([[ 3.4857e-03,  2.1106e-03,  3.7642e-03,  ..., -3.8145e-03,\n",
      "          6.9847e-03, -2.2807e-03],\n",
      "        [ 3.4146e-04, -5.4942e-04,  9.7398e-04,  ...,  3.0755e-04,\n",
      "          5.5588e-04, -3.3453e-05],\n",
      "        [-7.4383e-04,  1.0496e-03,  2.7641e-03,  ...,  3.5885e-03,\n",
      "         -6.2996e-04,  4.0820e-03],\n",
      "        ...,\n",
      "        [-4.5905e-04, -3.6594e-04, -1.4784e-03,  ...,  1.7759e-04,\n",
      "         -1.8980e-03,  9.3110e-04],\n",
      "        [ 2.2837e-02, -6.5843e-03,  1.0192e-02,  ..., -3.2511e-02,\n",
      "         -2.7776e-04,  2.7431e-03],\n",
      "        [ 4.8654e-04,  1.6032e-03,  8.2590e-04,  ...,  1.2978e-03,\n",
      "         -1.4289e-03, -3.2989e-03]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight tensor([[-5.2638e-03,  3.6994e-03,  3.6665e-03,  ...,  9.5543e-05,\n",
      "          1.2235e-03,  1.4311e-03],\n",
      "        [-3.8527e-04, -4.0845e-04, -1.7732e-03,  ..., -7.5885e-03,\n",
      "          2.2956e-03,  2.6038e-03],\n",
      "        [ 3.9479e-03,  4.5187e-03, -2.8549e-03,  ..., -8.4055e-04,\n",
      "          2.3713e-03, -7.5745e-04],\n",
      "        ...,\n",
      "        [ 1.5468e-03, -6.9543e-04,  5.4631e-03,  ..., -8.0216e-04,\n",
      "          1.9622e-03,  6.0144e-03],\n",
      "        [-2.8071e-03,  4.3344e-04,  1.7981e-04,  ..., -1.0765e-03,\n",
      "          7.6533e-03,  1.5395e-03],\n",
      "        [-7.0214e-03,  2.9132e-03, -3.7309e-03,  ...,  1.3787e-03,\n",
      "         -2.2104e-03, -3.4164e-03]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight tensor([[-2.1224e-03, -3.4912e-03,  3.0857e-03,  ..., -2.5339e-03,\n",
      "         -1.7382e-03, -1.4390e-03],\n",
      "        [ 1.3016e-03, -1.0357e-03, -7.2282e-04,  ...,  4.7589e-04,\n",
      "         -6.4009e-05,  6.7544e-04],\n",
      "        [ 9.6895e-04, -1.7378e-03, -1.2414e-03,  ...,  2.2579e-04,\n",
      "          7.5280e-03, -2.5558e-03],\n",
      "        ...,\n",
      "        [-4.4620e-03,  3.6653e-03,  1.1192e-03,  ...,  5.2607e-04,\n",
      "         -1.7219e-03, -1.1386e-03],\n",
      "        [-3.6874e-03, -1.1150e-02, -1.8465e-02,  ..., -6.3916e-03,\n",
      "          3.2301e-04,  2.9095e-03],\n",
      "        [-6.4636e-04,  1.1474e-03,  3.2988e-03,  ..., -8.6561e-04,\n",
      "         -3.1702e-03, -1.2570e-03]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight tensor([[-4.3409e-03, -3.0389e-05,  1.5112e-03,  ..., -1.5354e-04,\n",
      "         -4.9145e-03, -2.4306e-03],\n",
      "        [-1.0336e-02,  2.5644e-03,  1.5217e-03,  ..., -6.1570e-04,\n",
      "         -9.5366e-03,  8.6694e-04],\n",
      "        [-2.0826e-03, -6.5876e-04,  6.7205e-03,  ..., -2.5589e-03,\n",
      "          1.0117e-02,  5.1669e-04],\n",
      "        ...,\n",
      "        [-1.5002e-02, -1.2177e-03, -3.7135e-03,  ..., -7.4603e-03,\n",
      "         -1.5857e-02,  1.9314e-03],\n",
      "        [-9.2850e-03, -4.1163e-04, -3.1451e-03,  ...,  2.7990e-03,\n",
      "         -7.2020e-03,  8.4723e-03],\n",
      "        [-6.2092e-03,  3.2170e-03, -2.7299e-03,  ..., -9.4012e-04,\n",
      "         -1.0716e-02,  2.4437e-03]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight tensor([[-0.0033,  0.0006,  0.0025,  ..., -0.0015,  0.0007,  0.0016],\n",
      "        [ 0.0002,  0.0004,  0.0002,  ..., -0.0007, -0.0009, -0.0003],\n",
      "        [ 0.0024,  0.0002, -0.0027,  ...,  0.0018,  0.0016,  0.0002],\n",
      "        ...,\n",
      "        [ 0.0023,  0.0016,  0.0016,  ..., -0.0017,  0.0002,  0.0004],\n",
      "        [-0.0019, -0.0060, -0.0045,  ...,  0.0058, -0.0062, -0.0043],\n",
      "        [ 0.0008,  0.0007,  0.0017,  ...,  0.0015,  0.0026,  0.0019]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight tensor([[ 0.0011, -0.0018, -0.0021,  ..., -0.0027,  0.0047,  0.0022],\n",
      "        [-0.0045, -0.0016,  0.0002,  ...,  0.0015,  0.0076, -0.0008],\n",
      "        [-0.0022, -0.0076, -0.0015,  ..., -0.0025, -0.0016, -0.0030],\n",
      "        ...,\n",
      "        [ 0.0055, -0.0005,  0.0016,  ...,  0.0042, -0.0083, -0.0006],\n",
      "        [-0.0049, -0.0003,  0.0028,  ..., -0.0029, -0.0062, -0.0010],\n",
      "        [ 0.0015,  0.0004, -0.0002,  ..., -0.0004,  0.0002, -0.0060]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight tensor([[-0.0018, -0.0034, -0.0049,  ..., -0.0001, -0.0018,  0.0007],\n",
      "        [-0.0004,  0.0006,  0.0005,  ...,  0.0005, -0.0005, -0.0005],\n",
      "        [ 0.0016,  0.0009, -0.0024,  ...,  0.0007, -0.0014, -0.0018],\n",
      "        ...,\n",
      "        [ 0.0023, -0.0003,  0.0011,  ...,  0.0008, -0.0020,  0.0011],\n",
      "        [ 0.0038,  0.0043,  0.0020,  ..., -0.0004, -0.0025,  0.0047],\n",
      "        [-0.0008, -0.0017,  0.0002,  ..., -0.0021,  0.0013, -0.0005]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight tensor([[-9.2924e-04, -4.7624e-04, -9.1675e-05,  ...,  2.6221e-04,\n",
      "         -2.0138e-03, -1.1605e-03],\n",
      "        [-1.5810e-03,  4.8497e-04, -1.5734e-03,  ..., -9.0087e-05,\n",
      "          4.0053e-03,  2.0080e-04],\n",
      "        [-1.5040e-03,  4.7774e-04,  4.0792e-05,  ..., -4.2483e-04,\n",
      "          3.7126e-03, -3.4772e-04],\n",
      "        ...,\n",
      "        [ 8.5789e-04, -1.1538e-03, -8.5204e-04,  ..., -3.9569e-04,\n",
      "         -4.1461e-04, -4.1799e-04],\n",
      "        [ 1.2658e-03,  1.6058e-04, -5.5890e-04,  ..., -4.4753e-04,\n",
      "          1.0847e-03, -1.3451e-04],\n",
      "        [ 1.7686e-04,  7.6614e-04, -1.3864e-03,  ..., -7.1321e-04,\n",
      "          1.2075e-03,  1.0361e-04]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight tensor([[ 1.0174e-03,  4.0354e-04,  1.7956e-03,  ...,  1.2304e-03,\n",
      "          2.5407e-03,  7.5865e-03],\n",
      "        [ 3.9230e-04,  8.9978e-04,  4.6971e-04,  ..., -9.0220e-05,\n",
      "         -1.1994e-03,  2.9311e-04],\n",
      "        [-3.3353e-04, -1.9720e-04,  2.8896e-03,  ..., -6.3557e-04,\n",
      "         -2.1812e-03,  4.1861e-04],\n",
      "        ...,\n",
      "        [-3.0211e-04,  1.0136e-03, -4.9593e-03,  ..., -1.7200e-03,\n",
      "          2.0231e-03,  1.5857e-03],\n",
      "        [-1.2100e-02, -2.7121e-03, -2.1933e-03,  ...,  1.0559e-02,\n",
      "          7.7413e-03,  4.1486e-03],\n",
      "        [-2.9681e-03,  1.0436e-03, -6.9188e-04,  ..., -2.7550e-04,\n",
      "         -1.3672e-03,  1.0651e-03]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight tensor([[ 6.7366e-03,  4.1118e-03,  3.6173e-03,  ...,  5.1087e-03,\n",
      "         -4.3686e-03,  3.1460e-03],\n",
      "        [-5.0291e-03,  4.8655e-03, -2.1330e-03,  ...,  2.9788e-03,\n",
      "         -2.5478e-03, -1.3362e-03],\n",
      "        [ 1.3502e-03, -4.2610e-03,  4.8576e-03,  ...,  8.1459e-04,\n",
      "         -5.4294e-03, -4.0791e-03],\n",
      "        ...,\n",
      "        [-6.1154e-04, -1.0100e-02,  4.2272e-03,  ..., -4.9187e-03,\n",
      "          5.8961e-03, -5.0605e-03],\n",
      "        [-1.5117e-03,  1.7369e-03,  5.5195e-03,  ...,  4.5946e-03,\n",
      "         -9.9883e-04,  1.4818e-03],\n",
      "        [ 5.4698e-03, -5.1258e-06, -5.6070e-03,  ...,  1.6153e-03,\n",
      "          1.3034e-03, -7.6934e-03]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight tensor([[ 4.0913e-04, -2.7102e-04,  5.8832e-04,  ...,  3.0200e-04,\n",
      "         -2.9448e-03,  1.4800e-03],\n",
      "        [ 7.5106e-04,  2.3098e-04,  1.1028e-03,  ..., -1.3518e-05,\n",
      "          5.9185e-04, -6.9609e-04],\n",
      "        [-3.4508e-03, -1.4967e-03, -2.7354e-03,  ...,  1.1550e-04,\n",
      "          2.4126e-03, -3.4440e-03],\n",
      "        ...,\n",
      "        [-5.1487e-04, -1.2037e-03, -5.6136e-04,  ...,  1.2664e-03,\n",
      "         -4.4211e-03,  1.9340e-03],\n",
      "        [ 1.3113e-02,  5.3407e-03,  1.5222e-02,  ...,  1.6367e-02,\n",
      "          4.0894e-04,  1.2040e-03],\n",
      "        [ 1.2612e-03,  9.1166e-04, -1.0557e-03,  ..., -6.9799e-04,\n",
      "         -1.6429e-03, -1.3485e-03]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight tensor([[ 6.0672e-04,  2.3879e-04,  1.0082e-03,  ..., -3.4428e-04,\n",
      "         -9.3877e-04,  3.3480e-04],\n",
      "        [ 3.1881e-05, -2.2481e-03,  1.7021e-03,  ..., -1.3845e-04,\n",
      "         -9.1142e-03,  4.2286e-04],\n",
      "        [-9.6408e-04, -1.7645e-05,  1.4747e-03,  ...,  2.0597e-03,\n",
      "         -1.0405e-03, -1.1916e-04],\n",
      "        ...,\n",
      "        [-1.6189e-05, -1.6078e-03, -3.5291e-04,  ..., -1.4677e-03,\n",
      "          5.7324e-03,  1.9789e-04],\n",
      "        [-1.2922e-03,  1.0113e-03, -1.1712e-03,  ...,  1.5743e-03,\n",
      "         -6.8299e-03, -4.3844e-04],\n",
      "        [ 1.9759e-03,  1.8421e-04,  1.5102e-03,  ..., -1.4284e-03,\n",
      "         -1.3689e-03, -1.4439e-03]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight tensor([[ 5.7125e-03,  3.1311e-03,  3.2851e-03,  ..., -5.2477e-03,\n",
      "          4.3600e-05, -3.3689e-03],\n",
      "        [ 2.3534e-05, -6.2346e-04, -5.7837e-04,  ..., -2.5452e-04,\n",
      "         -3.6822e-04,  3.3720e-04],\n",
      "        [ 1.8347e-03, -3.1193e-04, -4.1416e-03,  ..., -1.0768e-03,\n",
      "         -1.3937e-03,  7.0128e-04],\n",
      "        ...,\n",
      "        [ 1.1333e-03, -4.9877e-05, -4.7429e-04,  ..., -3.4255e-03,\n",
      "         -1.5662e-03,  1.3381e-04],\n",
      "        [-4.5276e-04,  9.2783e-03, -4.5921e-03,  ..., -9.1684e-03,\n",
      "          8.9954e-03, -1.3553e-03],\n",
      "        [-3.8143e-04, -3.0651e-03, -2.9200e-03,  ...,  3.1746e-04,\n",
      "         -2.8920e-03,  2.1878e-03]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight tensor([[-1.0884e-04,  2.4101e-03, -1.5702e-03,  ...,  2.5948e-03,\n",
      "          3.3731e-04,  6.3120e-03],\n",
      "        [ 3.3286e-03, -5.9573e-03, -2.1557e-03,  ..., -2.2378e-04,\n",
      "         -1.9266e-03, -9.7018e-03],\n",
      "        [-3.1776e-03, -5.6968e-03,  7.7391e-04,  ..., -3.9411e-03,\n",
      "         -3.2928e-03, -1.6454e-03],\n",
      "        ...,\n",
      "        [ 3.5491e-03,  1.4715e-03,  1.4322e-03,  ..., -2.8620e-03,\n",
      "          1.5852e-03, -8.2123e-03],\n",
      "        [-1.5065e-03, -7.4344e-04, -9.5900e-04,  ...,  2.2171e-03,\n",
      "          2.7402e-03,  8.1941e-03],\n",
      "        [ 4.4927e-03, -3.6292e-03,  3.2234e-03,  ..., -5.7700e-04,\n",
      "         -4.8392e-05, -8.4044e-03]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight tensor([[ 1.7595e-03, -1.1804e-03, -1.8305e-03,  ..., -6.9622e-04,\n",
      "          4.7880e-03,  3.5268e-03],\n",
      "        [ 3.6046e-04,  5.0572e-04, -7.3622e-04,  ...,  6.6484e-04,\n",
      "          9.5193e-05, -7.9236e-05],\n",
      "        [-1.0356e-03,  6.2507e-04, -3.3263e-03,  ...,  6.0529e-04,\n",
      "          2.9577e-03, -1.0685e-03],\n",
      "        ...,\n",
      "        [ 1.9111e-03, -2.6473e-03,  3.8595e-03,  ...,  4.1971e-03,\n",
      "         -1.9880e-03,  9.3910e-04],\n",
      "        [-8.6893e-05,  1.1628e-02,  2.2701e-02,  ...,  1.2777e-02,\n",
      "         -1.5690e-02, -1.5792e-02],\n",
      "        [ 1.3934e-03,  3.5208e-04, -2.4282e-03,  ...,  2.7156e-03,\n",
      "          8.6573e-05,  2.2559e-03]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight tensor([[-0.0055,  0.0026,  0.0082,  ..., -0.0030, -0.0038, -0.0002],\n",
      "        [ 0.0022,  0.0034,  0.0008,  ...,  0.0060,  0.0021, -0.0115],\n",
      "        [-0.0117,  0.0006, -0.0010,  ..., -0.0032, -0.0020,  0.0061],\n",
      "        ...,\n",
      "        [-0.0096, -0.0018,  0.0129,  ..., -0.0042,  0.0029, -0.0008],\n",
      "        [-0.0026,  0.0088,  0.0044,  ..., -0.0016,  0.0070,  0.0046],\n",
      "        [ 0.0069, -0.0023, -0.0020,  ..., -0.0037,  0.0077,  0.0014]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight tensor([[-2.9400e-03, -6.0356e-03, -4.6017e-03,  ...,  2.2629e-03,\n",
      "         -2.0701e-03, -7.0056e-05],\n",
      "        [ 1.1162e-03, -2.8979e-04, -1.0974e-03,  ..., -2.1701e-04,\n",
      "         -2.9486e-04,  7.9183e-04],\n",
      "        [-1.4436e-03,  3.4124e-03, -2.6174e-03,  ..., -3.3974e-03,\n",
      "          5.5342e-03, -2.2920e-03],\n",
      "        ...,\n",
      "        [-1.7572e-03, -1.9318e-03, -1.9384e-03,  ...,  3.6696e-03,\n",
      "         -4.8821e-04, -8.4265e-04],\n",
      "        [-6.7984e-03,  1.0996e-04,  1.1128e-02,  ...,  1.2689e-02,\n",
      "          1.4054e-02, -2.1016e-02],\n",
      "        [-7.9106e-04,  9.8784e-04,  6.1017e-04,  ...,  2.9955e-03,\n",
      "         -7.1999e-04,  2.9704e-03]])\n",
      "unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight tensor([[-3.4883e-03,  2.9606e-03, -2.3537e-03,  ..., -7.5070e-04,\n",
      "          6.0038e-03,  4.7903e-03],\n",
      "        [ 8.8052e-03, -3.6535e-03, -4.6155e-03,  ...,  5.3479e-04,\n",
      "         -2.8817e-03,  2.5259e-03],\n",
      "        [-1.4022e-02,  2.0488e-03,  5.3988e-03,  ..., -3.7074e-03,\n",
      "          5.8015e-03, -1.6217e-03],\n",
      "        ...,\n",
      "        [-1.0033e-03, -1.1392e-03,  6.0815e-03,  ..., -2.0814e-03,\n",
      "         -7.5129e-04,  2.0306e-03],\n",
      "        [-3.4079e-03, -7.7488e-04, -4.1569e-03,  ...,  1.5920e-03,\n",
      "         -2.3740e-03, -3.8097e-03],\n",
      "        [ 7.2690e-03,  4.8144e-03,  4.5555e-03,  ..., -3.0494e-03,\n",
      "         -2.9228e-05, -1.3353e-04]])\n",
      "unet.up_blocks.3.attentions.2.proj_in.lora.down.weight tensor([[[[ 0.0015]],\n",
      "\n",
      "         [[ 0.0001]],\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[-0.0069]],\n",
      "\n",
      "         [[ 0.0058]]],\n",
      "\n",
      "\n",
      "        [[[-0.0004]],\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         [[ 0.0012]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[ 0.0004]]],\n",
      "\n",
      "\n",
      "        [[[-0.0059]],\n",
      "\n",
      "         [[ 0.0010]],\n",
      "\n",
      "         [[-0.0030]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[-0.0036]],\n",
      "\n",
      "         [[ 0.0015]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0028]],\n",
      "\n",
      "         [[ 0.0024]],\n",
      "\n",
      "         [[ 0.0007]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0057]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         [[ 0.0003]]],\n",
      "\n",
      "\n",
      "        [[[-0.0019]],\n",
      "\n",
      "         [[-0.0125]],\n",
      "\n",
      "         [[-0.0152]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[ 0.0230]],\n",
      "\n",
      "         [[ 0.0069]]],\n",
      "\n",
      "\n",
      "        [[[-0.0013]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0036]],\n",
      "\n",
      "         [[ 0.0035]],\n",
      "\n",
      "         [[-0.0030]]]])\n",
      "unet.up_blocks.3.attentions.2.proj_in.lora.up.weight tensor([[[[-0.0030]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         [[-0.0035]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0021]],\n",
      "\n",
      "         [[-0.0027]],\n",
      "\n",
      "         [[-0.0138]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0035]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[-0.0042]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0061]],\n",
      "\n",
      "         [[-0.0054]],\n",
      "\n",
      "         [[-0.0021]]],\n",
      "\n",
      "\n",
      "        [[[-0.0147]],\n",
      "\n",
      "         [[-0.0047]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0014]],\n",
      "\n",
      "         [[ 0.0086]],\n",
      "\n",
      "         [[-0.0030]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0027]],\n",
      "\n",
      "         [[-0.0005]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[-0.0068]],\n",
      "\n",
      "         [[-0.0006]]],\n",
      "\n",
      "\n",
      "        [[[-0.0014]],\n",
      "\n",
      "         [[ 0.0061]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0033]],\n",
      "\n",
      "         [[ 0.0001]],\n",
      "\n",
      "         [[ 0.0064]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0045]],\n",
      "\n",
      "         [[ 0.0001]],\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0045]],\n",
      "\n",
      "         [[ 0.0061]],\n",
      "\n",
      "         [[-0.0026]]]])\n",
      "unet.up_blocks.3.attentions.2.proj_out.lora.down.weight tensor([[[[ 0.0006]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         [[ 0.0024]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0083]],\n",
      "\n",
      "         [[-0.0003]],\n",
      "\n",
      "         [[-0.0010]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0007]],\n",
      "\n",
      "         [[ 0.0010]],\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[-0.0011]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0067]],\n",
      "\n",
      "         [[-0.0078]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0070]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         [[ 0.0056]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0048]],\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0017]],\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         [[ 0.0001]]],\n",
      "\n",
      "\n",
      "        [[[-0.0162]],\n",
      "\n",
      "         [[ 0.0103]],\n",
      "\n",
      "         [[ 0.0037]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0083]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         [[ 0.0060]]],\n",
      "\n",
      "\n",
      "        [[[-0.0028]],\n",
      "\n",
      "         [[-0.0048]],\n",
      "\n",
      "         [[ 0.0050]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[ 0.0008]],\n",
      "\n",
      "         [[ 0.0024]]]])\n",
      "unet.up_blocks.3.attentions.2.proj_out.lora.up.weight tensor([[[[ 0.0030]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[ 0.0008]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0020]],\n",
      "\n",
      "         [[ 0.0016]],\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0007]],\n",
      "\n",
      "         [[-0.0084]],\n",
      "\n",
      "         [[-0.0025]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0011]],\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         [[ 0.0078]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0069]],\n",
      "\n",
      "         [[-0.0052]],\n",
      "\n",
      "         [[-0.0127]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0102]],\n",
      "\n",
      "         [[-0.0034]],\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0025]],\n",
      "\n",
      "         [[ 0.0059]],\n",
      "\n",
      "         [[-0.0048]]],\n",
      "\n",
      "\n",
      "        [[[-0.0087]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         [[ 0.0021]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         [[ 0.0033]],\n",
      "\n",
      "         [[-0.0033]]],\n",
      "\n",
      "\n",
      "        [[[-0.0015]],\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         [[ 0.0074]]]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight tensor([[-7.5480e-03,  3.0583e-03, -5.5583e-03,  ..., -5.2944e-03,\n",
      "          1.7118e-03,  6.8491e-03],\n",
      "        [-3.2392e-04, -7.6107e-05, -4.5511e-04,  ...,  8.9222e-04,\n",
      "          2.8119e-06,  4.8860e-04],\n",
      "        [ 8.2562e-04,  2.7518e-03, -2.8617e-04,  ..., -6.5919e-04,\n",
      "         -3.1900e-03, -4.1118e-04],\n",
      "        ...,\n",
      "        [-2.4685e-03, -1.5704e-03, -2.0771e-03,  ..., -1.3430e-03,\n",
      "         -2.1977e-03, -1.0037e-03],\n",
      "        [ 2.5639e-02,  6.6539e-03,  3.3124e-02,  ...,  1.4907e-03,\n",
      "          8.4889e-03, -1.0340e-02],\n",
      "        [ 1.6890e-03, -8.5644e-04, -1.4218e-03,  ...,  1.1959e-03,\n",
      "          3.4695e-04, -1.0159e-03]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight tensor([[-0.0032, -0.0052, -0.0044,  ..., -0.0025,  0.0086,  0.0030],\n",
      "        [-0.0057,  0.0009, -0.0024,  ...,  0.0039,  0.0066,  0.0031],\n",
      "        [ 0.0038,  0.0033,  0.0017,  ...,  0.0036, -0.0023, -0.0011],\n",
      "        ...,\n",
      "        [ 0.0081, -0.0023,  0.0094,  ..., -0.0013, -0.0123, -0.0005],\n",
      "        [ 0.0161, -0.0018, -0.0054,  ...,  0.0011, -0.0102, -0.0003],\n",
      "        [ 0.0102,  0.0020, -0.0033,  ...,  0.0001, -0.0020, -0.0041]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight tensor([[-6.6606e-04, -3.9327e-05, -3.3543e-03,  ...,  2.2164e-03,\n",
      "          2.2907e-03, -2.5863e-03],\n",
      "        [ 3.6201e-04, -6.2446e-04,  2.7633e-05,  ...,  3.3258e-04,\n",
      "          5.0128e-04,  9.2541e-04],\n",
      "        [-3.2226e-03, -2.5899e-03,  1.9886e-04,  ...,  2.2184e-04,\n",
      "         -1.7957e-03,  2.6420e-03],\n",
      "        ...,\n",
      "        [-2.2799e-03, -5.8154e-04,  2.0926e-03,  ..., -2.3452e-03,\n",
      "         -3.3462e-03, -9.4439e-04],\n",
      "        [-4.6194e-03, -2.8973e-03, -5.6364e-03,  ...,  1.3244e-02,\n",
      "         -1.0623e-02, -1.7548e-02],\n",
      "        [ 2.4469e-03,  5.2842e-03, -1.0162e-03,  ...,  4.8214e-03,\n",
      "          2.8115e-03, -2.4574e-03]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight tensor([[-2.0823e-03, -1.3469e-03, -3.9183e-03,  ...,  3.6854e-03,\n",
      "         -2.9839e-03,  5.7737e-04],\n",
      "        [-5.4694e-03,  2.1175e-03,  2.8917e-03,  ..., -3.1564e-03,\n",
      "          7.4614e-03, -3.0183e-04],\n",
      "        [-7.0550e-03,  2.9351e-03, -1.0188e-02,  ..., -2.1452e-03,\n",
      "          6.2066e-03, -3.9257e-03],\n",
      "        ...,\n",
      "        [ 4.2001e-03, -3.6018e-03, -1.4432e-03,  ...,  3.9741e-03,\n",
      "         -1.1546e-02, -9.7750e-05],\n",
      "        [-2.2302e-03,  1.0548e-03,  1.4855e-03,  ...,  3.9419e-03,\n",
      "         -6.0635e-04,  3.3684e-03],\n",
      "        [ 1.0337e-03, -5.2451e-04,  5.3304e-04,  ..., -1.9084e-03,\n",
      "         -9.3352e-05, -1.8332e-03]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight tensor([[-1.0303e-05,  8.2138e-03,  8.9950e-04,  ...,  1.8746e-03,\n",
      "         -1.7141e-03, -7.7422e-03],\n",
      "        [-9.6839e-05,  2.0341e-04, -3.2258e-05,  ...,  3.6681e-04,\n",
      "         -6.9689e-04,  9.8793e-04],\n",
      "        [ 1.0439e-03, -7.9377e-06, -2.1116e-03,  ...,  8.8844e-04,\n",
      "         -1.2418e-03, -2.8536e-03],\n",
      "        ...,\n",
      "        [-5.7302e-04,  3.7509e-07,  4.9199e-04,  ...,  2.6213e-03,\n",
      "          2.0005e-03, -1.3554e-04],\n",
      "        [-9.1135e-03, -1.1893e-02, -2.7936e-02,  ...,  1.8572e-03,\n",
      "         -5.8404e-03,  8.7981e-03],\n",
      "        [ 7.2359e-04, -1.0592e-03, -5.3264e-04,  ..., -5.8881e-04,\n",
      "          1.4777e-03, -7.4209e-04]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight tensor([[-2.8475e-03,  2.7320e-03,  2.6660e-03,  ..., -1.3747e-03,\n",
      "         -7.5752e-03,  7.4744e-04],\n",
      "        [-1.6268e-03,  1.2677e-03, -8.1228e-03,  ..., -3.8688e-03,\n",
      "          2.6836e-03, -1.6119e-03],\n",
      "        [ 5.3843e-03, -3.7567e-03,  2.9540e-03,  ..., -1.9147e-03,\n",
      "          1.9677e-03, -2.9405e-05],\n",
      "        ...,\n",
      "        [ 2.1459e-02,  3.4110e-03, -4.3535e-06,  ..., -1.9035e-03,\n",
      "         -1.2691e-02,  8.2545e-04],\n",
      "        [ 1.3575e-02, -1.0921e-03, -2.0408e-04,  ..., -9.9137e-05,\n",
      "         -3.1912e-03,  8.4966e-04],\n",
      "        [-3.4545e-03,  1.2714e-03, -3.5189e-03,  ...,  9.3561e-04,\n",
      "          2.1700e-03, -3.1191e-03]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight tensor([[ 2.3637e-03, -3.4971e-03, -2.3760e-03,  ..., -6.6656e-03,\n",
      "          2.7614e-03,  6.3524e-04],\n",
      "        [-1.9390e-04, -1.5317e-05,  1.3571e-04,  ..., -5.1857e-04,\n",
      "         -4.4950e-04, -2.2002e-04],\n",
      "        [-2.6316e-04, -1.5020e-03,  1.0851e-03,  ..., -1.5359e-03,\n",
      "          5.6039e-04,  5.0069e-05],\n",
      "        ...,\n",
      "        [-2.4692e-03, -9.9882e-04,  2.9148e-03,  ...,  1.3876e-04,\n",
      "         -3.1785e-03,  1.4460e-03],\n",
      "        [-9.4622e-03, -2.1878e-03,  1.7876e-03,  ...,  1.1751e-02,\n",
      "          1.5610e-02, -1.9971e-03],\n",
      "        [ 2.3269e-03, -1.8072e-05,  3.0148e-04,  ..., -2.1410e-03,\n",
      "          1.4774e-03, -4.0988e-03]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight tensor([[-0.0020, -0.0044,  0.0010,  ..., -0.0025, -0.0040,  0.0039],\n",
      "        [-0.0026,  0.0023,  0.0030,  ..., -0.0015,  0.0065,  0.0012],\n",
      "        [-0.0033,  0.0022, -0.0020,  ..., -0.0020,  0.0062, -0.0008],\n",
      "        ...,\n",
      "        [-0.0022,  0.0062, -0.0011,  ...,  0.0068,  0.0016,  0.0030],\n",
      "        [ 0.0012,  0.0007, -0.0022,  ..., -0.0003, -0.0012,  0.0035],\n",
      "        [ 0.0028, -0.0032, -0.0016,  ..., -0.0041,  0.0072, -0.0002]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight tensor([[ 1.9973e-03, -2.5521e-04, -1.2598e-03,  ...,  1.7515e-03,\n",
      "          6.8653e-04, -1.8621e-03],\n",
      "        [-1.2382e-03, -1.7344e-05, -4.1911e-04,  ...,  9.5704e-04,\n",
      "          3.2715e-04, -4.4680e-04],\n",
      "        [-1.5022e-03,  6.9219e-04,  7.6611e-04,  ..., -2.1917e-03,\n",
      "         -1.9037e-03, -1.1215e-03],\n",
      "        ...,\n",
      "        [ 8.8514e-04,  6.7621e-04, -8.9783e-05,  ..., -2.3204e-03,\n",
      "         -1.1596e-03,  2.5228e-03],\n",
      "        [-7.3431e-03, -6.5880e-03,  2.3027e-03,  ..., -3.3528e-03,\n",
      "          3.8085e-04,  2.1781e-03],\n",
      "        [-4.3049e-04, -4.9132e-04, -5.1618e-04,  ..., -9.1330e-04,\n",
      "          9.7254e-04,  1.9293e-04]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight tensor([[-6.6858e-04, -5.9906e-04, -4.6822e-04,  ...,  4.8747e-04,\n",
      "          3.1383e-03, -7.7088e-04],\n",
      "        [ 1.0782e-03, -1.1580e-03,  4.8541e-04,  ...,  1.0990e-03,\n",
      "         -4.1008e-03,  1.3431e-04],\n",
      "        [-5.4197e-04,  1.5514e-03,  1.9855e-03,  ...,  4.2114e-04,\n",
      "         -1.8363e-03, -6.3496e-05],\n",
      "        ...,\n",
      "        [ 2.7525e-04, -9.4317e-04, -1.7010e-04,  ...,  5.3184e-04,\n",
      "          1.5178e-03,  5.1968e-04],\n",
      "        [-3.4207e-04, -4.8054e-04, -4.3797e-04,  ...,  8.2605e-04,\n",
      "          3.0459e-04, -6.9493e-05],\n",
      "        [ 6.5314e-04,  1.2091e-04,  2.0173e-04,  ...,  1.2874e-04,\n",
      "         -2.3765e-03,  4.0653e-04]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight tensor([[ 6.9380e-06,  2.6706e-03,  1.5595e-03,  ..., -1.7586e-03,\n",
      "         -3.6022e-04, -1.8577e-03],\n",
      "        [ 2.0065e-04, -1.0074e-03,  1.8931e-03,  ...,  5.4072e-04,\n",
      "         -5.2897e-04, -3.5838e-05],\n",
      "        [ 5.8131e-03, -7.9312e-03,  4.5118e-03,  ..., -5.9176e-04,\n",
      "         -3.6814e-03, -2.8842e-05],\n",
      "        ...,\n",
      "        [-1.3423e-03,  2.1341e-03,  1.7222e-03,  ..., -2.8767e-03,\n",
      "         -2.3424e-03, -3.8957e-03],\n",
      "        [ 5.1805e-04,  6.5051e-03,  6.7554e-03,  ..., -8.7896e-03,\n",
      "          1.0592e-03,  1.1264e-02],\n",
      "        [ 1.4577e-03,  6.2483e-04,  1.7635e-03,  ..., -2.9229e-03,\n",
      "         -3.1726e-03,  1.1237e-03]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight tensor([[-8.2817e-03,  6.3013e-03,  1.3496e-03,  ..., -4.6082e-03,\n",
      "         -3.6658e-05, -5.9055e-03],\n",
      "        [ 2.6281e-03,  4.1239e-03, -3.6741e-03,  ..., -6.2006e-04,\n",
      "         -5.1132e-03,  5.6468e-03],\n",
      "        [-3.3407e-03, -8.6874e-04,  1.1570e-03,  ..., -2.4565e-03,\n",
      "         -4.4443e-03, -4.4789e-03],\n",
      "        ...,\n",
      "        [ 1.8367e-03, -1.4928e-03, -4.2063e-03,  ...,  4.0127e-03,\n",
      "          5.1069e-03,  2.6316e-03],\n",
      "        [-4.5386e-03, -1.3300e-03,  1.8540e-03,  ...,  5.8770e-03,\n",
      "          1.5449e-03,  6.5215e-03],\n",
      "        [ 4.1346e-03, -8.2532e-03, -3.5481e-03,  ...,  4.4830e-03,\n",
      "          3.2338e-03,  2.9775e-03]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight tensor([[-3.7472e-03,  4.5453e-03,  2.5932e-03,  ..., -9.9257e-04,\n",
      "          1.3946e-03,  3.5636e-03],\n",
      "        [-7.4590e-06, -1.1376e-03, -5.1088e-04,  ..., -3.6270e-04,\n",
      "         -7.1231e-04, -1.2093e-04],\n",
      "        [-1.0302e-03, -3.1172e-03, -1.8244e-03,  ..., -7.3308e-04,\n",
      "          8.4707e-04, -2.2682e-03],\n",
      "        ...,\n",
      "        [ 3.5176e-03,  8.6023e-04, -2.7965e-04,  ..., -3.8639e-04,\n",
      "          2.6733e-03, -3.4158e-03],\n",
      "        [ 1.4505e-02,  9.2185e-03, -4.6104e-03,  ..., -4.1322e-03,\n",
      "         -3.4445e-03,  1.2192e-02],\n",
      "        [-2.3423e-03, -2.7704e-04, -1.8864e-03,  ...,  2.3769e-03,\n",
      "         -4.5243e-03, -1.2480e-03]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight tensor([[-3.8951e-03,  1.9812e-03, -6.6149e-04,  ...,  8.3020e-04,\n",
      "          1.4112e-02, -2.0884e-03],\n",
      "        [-5.8379e-03, -8.9394e-04, -2.1316e-03,  ..., -3.6614e-03,\n",
      "         -2.4909e-03, -4.3346e-04],\n",
      "        [-3.4127e-03,  4.1679e-04,  1.1463e-03,  ..., -3.4756e-03,\n",
      "          9.6016e-03, -4.1235e-03],\n",
      "        ...,\n",
      "        [ 6.1124e-05,  6.2410e-04,  1.9523e-03,  ...,  4.9439e-04,\n",
      "          4.5770e-03,  5.4631e-04],\n",
      "        [-2.2576e-03, -1.7433e-04,  1.4069e-05,  ..., -2.6381e-05,\n",
      "          7.3341e-03, -1.7844e-05],\n",
      "        [-1.9051e-03, -9.4476e-04, -1.2819e-03,  ...,  7.8864e-04,\n",
      "         -1.5534e-03,  2.0843e-04]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight tensor([[ 2.6846e-03, -4.0921e-03, -3.1132e-04,  ..., -1.9770e-04,\n",
      "          4.7046e-04, -3.9600e-03],\n",
      "        [-2.4399e-04,  5.8420e-05, -1.4427e-04,  ..., -3.5305e-04,\n",
      "         -9.4541e-04, -4.5155e-05],\n",
      "        [-1.3410e-03,  7.5376e-04,  1.1468e-03,  ..., -1.1501e-03,\n",
      "          3.7812e-04,  3.2385e-03],\n",
      "        ...,\n",
      "        [ 1.8120e-03,  1.4376e-03,  9.4463e-05,  ..., -3.5097e-03,\n",
      "         -1.9646e-03,  7.8579e-04],\n",
      "        [-7.8745e-04,  7.6329e-03, -5.7062e-03,  ...,  1.2302e-03,\n",
      "          7.3860e-03,  6.3254e-04],\n",
      "        [ 8.2263e-04, -1.0130e-03,  2.0422e-03,  ...,  1.2494e-03,\n",
      "         -2.5675e-03, -7.3575e-04]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight tensor([[ 3.6488e-04,  1.6495e-04,  3.8142e-03,  ..., -3.0276e-03,\n",
      "          5.0063e-03,  4.3674e-03],\n",
      "        [-7.9133e-04,  4.2631e-03,  1.7498e-03,  ..., -2.2300e-03,\n",
      "         -5.1919e-05, -1.6176e-03],\n",
      "        [-1.9980e-03,  7.2928e-03, -1.1664e-03,  ...,  3.2440e-03,\n",
      "          5.8883e-04, -2.2276e-03],\n",
      "        ...,\n",
      "        [ 1.2151e-03, -8.4818e-04, -2.4992e-05,  ...,  2.9154e-03,\n",
      "          4.7276e-03,  1.8741e-03],\n",
      "        [ 2.5483e-03,  1.6617e-03, -1.9590e-03,  ..., -2.0253e-03,\n",
      "         -2.1329e-04,  1.2711e-04],\n",
      "        [ 2.1770e-03,  9.2128e-04, -3.9039e-03,  ...,  7.6781e-04,\n",
      "          4.0384e-03, -4.3836e-03]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.down.weight tensor([[-6.7826e-03,  2.6871e-03, -1.1795e-02,  ...,  5.3709e-03,\n",
      "         -8.5426e-03, -3.4073e-03],\n",
      "        [ 5.7029e-04,  2.3576e-04, -7.4914e-05,  ...,  1.0994e-03,\n",
      "          2.3689e-04,  3.0671e-04],\n",
      "        [-3.3821e-03, -1.9326e-03, -1.3767e-03,  ...,  2.2391e-03,\n",
      "          1.9871e-03, -5.0849e-04],\n",
      "        ...,\n",
      "        [ 7.4697e-04, -1.8609e-03, -4.2155e-03,  ..., -1.7332e-03,\n",
      "          3.5277e-03, -2.0060e-03],\n",
      "        [ 5.0602e-03, -1.8622e-03,  8.6445e-03,  ..., -7.1783e-04,\n",
      "          1.3452e-03, -6.6567e-04],\n",
      "        [ 4.9596e-03, -1.1780e-03, -1.7853e-03,  ..., -3.1279e-03,\n",
      "          1.0537e-03, -4.2729e-04]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.up.weight tensor([[ 6.1275e-03,  5.9229e-03,  4.3427e-03,  ..., -3.8681e-03,\n",
      "          6.4429e-03,  3.5410e-03],\n",
      "        [-9.7203e-03,  2.3281e-03,  9.2267e-03,  ..., -3.1186e-03,\n",
      "         -9.2287e-03, -4.1475e-03],\n",
      "        [ 1.9363e-03, -2.3163e-03, -3.7716e-03,  ...,  3.2883e-03,\n",
      "         -9.2036e-03,  8.8139e-05],\n",
      "        ...,\n",
      "        [ 2.1025e-03,  7.9710e-04, -4.5928e-03,  ...,  3.3409e-03,\n",
      "          2.2871e-03,  3.3960e-03],\n",
      "        [-1.6520e-03, -5.8969e-04,  2.0853e-03,  ...,  3.3946e-03,\n",
      "         -1.3715e-03,  9.2843e-04],\n",
      "        [-3.1170e-03,  3.2259e-03,  3.6250e-03,  ..., -4.1552e-03,\n",
      "          2.0688e-03,  1.7544e-03]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.lora.down.weight tensor([[-0.0026,  0.0021,  0.0002,  ...,  0.0040, -0.0020, -0.0014],\n",
      "        [-0.0010,  0.0002, -0.0016,  ...,  0.0005,  0.0002, -0.0006],\n",
      "        [-0.0046, -0.0026, -0.0039,  ...,  0.0018, -0.0013, -0.0058],\n",
      "        ...,\n",
      "        [ 0.0021, -0.0016,  0.0025,  ...,  0.0049,  0.0032,  0.0027],\n",
      "        [-0.0046,  0.0077, -0.0030,  ..., -0.0051, -0.0135,  0.0117],\n",
      "        [-0.0023,  0.0029,  0.0038,  ...,  0.0010,  0.0048, -0.0020]])\n",
      "unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.lora.up.weight tensor([[-3.1945e-03, -6.6349e-03,  1.4938e-03,  ...,  9.6298e-05,\n",
      "         -3.5294e-03, -4.4837e-03],\n",
      "        [ 1.2052e-02, -5.1925e-03, -4.3547e-04,  ..., -4.4873e-03,\n",
      "         -8.0122e-03,  3.2901e-03],\n",
      "        [ 6.0072e-04, -5.0264e-03, -2.4041e-03,  ..., -1.4643e-03,\n",
      "         -7.4649e-03, -2.6245e-03],\n",
      "        ...,\n",
      "        [ 1.2617e-02, -2.9224e-03,  3.6027e-03,  ...,  4.0932e-04,\n",
      "          1.5494e-03, -3.9216e-03],\n",
      "        [-1.7848e-03, -7.1769e-04, -4.1688e-04,  ..., -1.5166e-03,\n",
      "          6.6868e-03,  3.9039e-04],\n",
      "        [-2.8879e-03, -6.1370e-03,  1.2060e-03,  ..., -1.6431e-03,\n",
      "          8.7489e-03, -8.6338e-04]])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T14:07:50.796544Z",
     "start_time": "2024-10-22T14:07:50.274651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 这个是没有合并水印的lora模型的结构\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "filename = \"./checkpoints/pytorch_lora_weights_origin.safetensors\"\n",
    "model = load_file(filename)\n",
    "no_watermarked_state_dict = model\n",
    "for key, value in model.items():\n",
    "    print(\"\\nkey:\", key, \"\\nvalue:\", value)"
   ],
   "id": "b49b31523eabbfcb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "key: unet.down_blocks.0.attentions.0.proj_in.lora.down.weight \n",
      "value: tensor([[[[ 0.0027]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[ 0.0050]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0012]],\n",
      "\n",
      "         [[ 0.0035]],\n",
      "\n",
      "         [[-0.0007]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0006]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[-0.0080]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0025]],\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         [[ 0.0020]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0020]],\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[ 0.0053]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0016]],\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[-0.0011]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0004]],\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         [[ 0.0034]],\n",
      "\n",
      "         [[-0.0046]]],\n",
      "\n",
      "\n",
      "        [[[-0.0013]],\n",
      "\n",
      "         [[-0.0042]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0045]],\n",
      "\n",
      "         [[ 0.0083]],\n",
      "\n",
      "         [[ 0.0035]]],\n",
      "\n",
      "\n",
      "        [[[-0.0041]],\n",
      "\n",
      "         [[ 0.0048]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0025]],\n",
      "\n",
      "         [[ 0.0017]],\n",
      "\n",
      "         [[ 0.0021]]]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.proj_in.lora.up.weight \n",
      "value: tensor([[[[-0.0024]],\n",
      "\n",
      "         [[ 0.0023]],\n",
      "\n",
      "         [[-0.0101]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0042]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[-0.0020]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0026]],\n",
      "\n",
      "         [[ 0.0030]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0003]],\n",
      "\n",
      "         [[-0.0043]],\n",
      "\n",
      "         [[ 0.0073]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0023]],\n",
      "\n",
      "         [[ 0.0069]],\n",
      "\n",
      "         [[-0.0008]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         [[-0.0035]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0036]],\n",
      "\n",
      "         [[-0.0058]],\n",
      "\n",
      "         [[ 0.0074]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         [[-0.0106]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0066]],\n",
      "\n",
      "         [[ 0.0055]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0010]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[ 0.0014]]],\n",
      "\n",
      "\n",
      "        [[[-0.0044]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0074]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[ 0.0086]]]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.proj_out.lora.down.weight \n",
      "value: tensor([[[[ 5.9322e-03]],\n",
      "\n",
      "         [[ 2.7839e-05]],\n",
      "\n",
      "         [[ 8.7541e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.3379e-03]],\n",
      "\n",
      "         [[ 6.7226e-03]],\n",
      "\n",
      "         [[-7.1026e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.0057e-02]],\n",
      "\n",
      "         [[-4.4044e-03]],\n",
      "\n",
      "         [[-4.7351e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7700e-03]],\n",
      "\n",
      "         [[-6.6695e-03]],\n",
      "\n",
      "         [[ 1.3956e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.1463e-03]],\n",
      "\n",
      "         [[-3.1117e-03]],\n",
      "\n",
      "         [[-1.2953e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.5144e-03]],\n",
      "\n",
      "         [[ 5.7373e-03]],\n",
      "\n",
      "         [[ 4.2112e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.4619e-03]],\n",
      "\n",
      "         [[-1.0702e-02]],\n",
      "\n",
      "         [[ 1.9129e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2017e-02]],\n",
      "\n",
      "         [[-9.6443e-03]],\n",
      "\n",
      "         [[ 1.8830e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4054e-04]],\n",
      "\n",
      "         [[-1.1403e-02]],\n",
      "\n",
      "         [[ 2.3060e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2555e-02]],\n",
      "\n",
      "         [[ 9.1934e-04]],\n",
      "\n",
      "         [[-1.1398e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.8004e-03]],\n",
      "\n",
      "         [[ 2.3799e-03]],\n",
      "\n",
      "         [[ 8.1020e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3252e-03]],\n",
      "\n",
      "         [[-1.4128e-03]],\n",
      "\n",
      "         [[-4.9243e-03]]]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.proj_out.lora.up.weight \n",
      "value: tensor([[[[-0.0030]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         [[ 0.0052]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[-0.0084]],\n",
      "\n",
      "         [[-0.0029]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0028]],\n",
      "\n",
      "         [[ 0.0012]],\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[-0.0015]]],\n",
      "\n",
      "\n",
      "        [[[-0.0090]],\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         [[-0.0041]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0050]],\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         [[ 0.0044]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0103]],\n",
      "\n",
      "         [[ 0.0112]],\n",
      "\n",
      "         [[ 0.0024]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0010]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         [[ 0.0101]]],\n",
      "\n",
      "\n",
      "        [[[-0.0028]],\n",
      "\n",
      "         [[-0.0110]],\n",
      "\n",
      "         [[-0.0057]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0036]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[-0.0020]]],\n",
      "\n",
      "\n",
      "        [[[-0.0024]],\n",
      "\n",
      "         [[ 0.0072]],\n",
      "\n",
      "         [[ 0.0051]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         [[ 0.0001]],\n",
      "\n",
      "         [[ 0.0014]]]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight \n",
      "value: tensor([[ 0.0050, -0.0010,  0.0016,  ...,  0.0077,  0.0045,  0.0024],\n",
      "        [-0.0074, -0.0022, -0.0014,  ..., -0.0033,  0.0079,  0.0016],\n",
      "        [ 0.0054, -0.0054,  0.0030,  ..., -0.0024, -0.0052, -0.0098],\n",
      "        ...,\n",
      "        [-0.0083, -0.0079,  0.0006,  ...,  0.0005, -0.0032,  0.0044],\n",
      "        [ 0.0008,  0.0104,  0.0004,  ..., -0.0018, -0.0008,  0.0030],\n",
      "        [-0.0035, -0.0035,  0.0034,  ..., -0.0020, -0.0071, -0.0071]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight \n",
      "value: tensor([[ 2.9920e-03, -1.0507e-03, -1.0934e-03,  ...,  5.6809e-03,\n",
      "          6.2737e-03, -2.9567e-03],\n",
      "        [ 2.5939e-03,  9.3724e-04, -7.7603e-03,  ..., -1.9042e-03,\n",
      "          5.9859e-03, -6.2347e-03],\n",
      "        [-1.4211e-02,  3.4130e-03, -1.2161e-03,  ...,  2.0769e-03,\n",
      "          1.0663e-02, -3.1851e-03],\n",
      "        ...,\n",
      "        [ 4.6098e-03, -2.7930e-03, -1.7797e-03,  ...,  5.5384e-04,\n",
      "         -3.5034e-03, -1.9354e-03],\n",
      "        [-1.2643e-03, -1.8099e-03, -2.2602e-03,  ...,  1.7232e-03,\n",
      "          1.2067e-05,  3.1870e-03],\n",
      "        [-1.1149e-02, -5.9707e-03, -4.7565e-03,  ...,  3.7991e-03,\n",
      "          9.1035e-03,  7.2870e-03]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight \n",
      "value: tensor([[ 0.0092,  0.0108, -0.0037,  ...,  0.0029, -0.0037,  0.0052],\n",
      "        [ 0.0010, -0.0032, -0.0018,  ..., -0.0005,  0.0061, -0.0051],\n",
      "        [-0.0005, -0.0048,  0.0052,  ...,  0.0039, -0.0023,  0.0079],\n",
      "        ...,\n",
      "        [-0.0012,  0.0008, -0.0057,  ..., -0.0022,  0.0010, -0.0102],\n",
      "        [ 0.0015, -0.0004, -0.0034,  ...,  0.0060,  0.0022,  0.0057],\n",
      "        [-0.0006,  0.0008, -0.0018,  ..., -0.0066, -0.0063,  0.0015]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight \n",
      "value: tensor([[ 0.0002, -0.0040,  0.0037,  ...,  0.0028,  0.0104, -0.0052],\n",
      "        [-0.0034,  0.0007,  0.0070,  ...,  0.0020, -0.0045, -0.0031],\n",
      "        [-0.0025,  0.0072,  0.0061,  ...,  0.0039, -0.0028, -0.0009],\n",
      "        ...,\n",
      "        [-0.0019,  0.0013,  0.0034,  ...,  0.0040, -0.0055, -0.0089],\n",
      "        [-0.0061,  0.0046,  0.0046,  ...,  0.0036, -0.0014, -0.0005],\n",
      "        [ 0.0017,  0.0131,  0.0041,  ...,  0.0070, -0.0035, -0.0100]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight \n",
      "value: tensor([[ 0.0030,  0.0045, -0.0034,  ..., -0.0013,  0.0028,  0.0043],\n",
      "        [ 0.0015, -0.0060,  0.0015,  ..., -0.0082,  0.0011,  0.0048],\n",
      "        [ 0.0053,  0.0128, -0.0035,  ..., -0.0005,  0.0041,  0.0018],\n",
      "        ...,\n",
      "        [-0.0115, -0.0071, -0.0024,  ...,  0.0076, -0.0008, -0.0026],\n",
      "        [-0.0020,  0.0014,  0.0038,  ...,  0.0062,  0.0090,  0.0047],\n",
      "        [-0.0068,  0.0063,  0.0092,  ..., -0.0025, -0.0001, -0.0066]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight \n",
      "value: tensor([[-4.0427e-03,  1.4557e-03,  1.8350e-04,  ...,  2.5958e-03,\n",
      "          4.0318e-03, -6.5469e-03],\n",
      "        [ 3.2521e-03, -2.5751e-03, -4.6226e-04,  ...,  5.3004e-03,\n",
      "          7.1366e-03,  9.2916e-03],\n",
      "        [-1.5103e-03,  2.8812e-03,  6.8930e-03,  ..., -2.5148e-03,\n",
      "         -2.8178e-03,  9.9661e-03],\n",
      "        ...,\n",
      "        [ 3.3109e-03,  4.6051e-03, -7.5339e-03,  ..., -6.3745e-03,\n",
      "          1.1121e-03,  1.7892e-03],\n",
      "        [ 5.2172e-04, -1.9417e-03,  6.3243e-04,  ...,  1.4785e-03,\n",
      "         -4.6879e-04, -6.4951e-04],\n",
      "        [-1.4512e-03,  1.7483e-03, -4.9664e-03,  ...,  3.0707e-03,\n",
      "          6.4800e-03,  8.3908e-05]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight \n",
      "value: tensor([[-3.3236e-03,  1.9881e-03,  5.3666e-04,  ..., -4.2593e-03,\n",
      "          4.5107e-03,  9.0027e-05],\n",
      "        [-6.1211e-03, -1.0259e-02, -5.2624e-03,  ..., -9.3907e-03,\n",
      "          7.9614e-03, -9.2278e-03],\n",
      "        [-8.8298e-03,  2.7799e-04, -9.2037e-05,  ..., -4.9546e-03,\n",
      "          1.1812e-03, -2.0412e-03],\n",
      "        ...,\n",
      "        [ 1.1526e-03,  7.7808e-03, -4.1356e-03,  ..., -5.1665e-03,\n",
      "          5.5147e-03, -2.3296e-03],\n",
      "        [ 4.0923e-03,  3.7886e-03,  4.8159e-03,  ...,  6.1479e-03,\n",
      "         -8.7077e-03, -7.0194e-03],\n",
      "        [ 6.3192e-03, -9.4812e-03,  1.7558e-03,  ...,  5.0859e-03,\n",
      "         -3.0734e-03,  1.3150e-02]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight \n",
      "value: tensor([[ 0.0039, -0.0112, -0.0075,  ...,  0.0066,  0.0034,  0.0091],\n",
      "        [ 0.0019, -0.0013,  0.0047,  ..., -0.0041, -0.0011, -0.0072],\n",
      "        [ 0.0045, -0.0136,  0.0012,  ..., -0.0037,  0.0028, -0.0038],\n",
      "        ...,\n",
      "        [ 0.0034, -0.0093, -0.0054,  ...,  0.0029,  0.0049, -0.0046],\n",
      "        [ 0.0017, -0.0034,  0.0015,  ..., -0.0135, -0.0041,  0.0004],\n",
      "        [ 0.0089,  0.0132, -0.0047,  ..., -0.0045,  0.0030,  0.0009]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight \n",
      "value: tensor([[ 1.2964e-04,  2.3937e-03, -1.5815e-03,  ...,  1.5542e-03,\n",
      "         -3.3865e-03, -7.0151e-04],\n",
      "        [ 1.0007e-03, -2.1818e-03,  7.3658e-04,  ...,  6.8511e-03,\n",
      "          2.0083e-05,  4.9222e-03],\n",
      "        [-8.0876e-04, -4.7836e-04, -4.2303e-04,  ..., -1.8219e-04,\n",
      "         -1.4299e-03, -2.5016e-04],\n",
      "        ...,\n",
      "        [ 5.1277e-03,  1.2296e-03,  5.3625e-03,  ...,  1.1600e-03,\n",
      "          2.2998e-03, -8.9035e-03],\n",
      "        [ 4.7842e-03, -3.1044e-03, -9.6071e-04,  ..., -3.1435e-03,\n",
      "         -4.7536e-03,  7.9139e-03],\n",
      "        [ 1.4083e-03, -4.4005e-03, -4.7623e-04,  ...,  4.2887e-03,\n",
      "         -1.3857e-03, -2.5003e-03]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight \n",
      "value: tensor([[ 0.0033,  0.0042, -0.0025,  ...,  0.0056,  0.0010,  0.0008],\n",
      "        [ 0.0081,  0.0039, -0.0041,  ..., -0.0068,  0.0014,  0.0044],\n",
      "        [ 0.0025, -0.0021,  0.0059,  ..., -0.0010,  0.0012,  0.0029],\n",
      "        ...,\n",
      "        [ 0.0028, -0.0034,  0.0042,  ...,  0.0024, -0.0032, -0.0033],\n",
      "        [ 0.0018, -0.0001,  0.0037,  ..., -0.0018, -0.0032,  0.0061],\n",
      "        [-0.0026, -0.0016,  0.0011,  ...,  0.0025,  0.0045, -0.0033]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight \n",
      "value: tensor([[-0.0044,  0.0004, -0.0066,  ..., -0.0031,  0.0030,  0.0059],\n",
      "        [-0.0067,  0.0031,  0.0034,  ..., -0.0023, -0.0081, -0.0011],\n",
      "        [-0.0024, -0.0032, -0.0098,  ..., -0.0044,  0.0022, -0.0069],\n",
      "        ...,\n",
      "        [-0.0055,  0.0093, -0.0009,  ..., -0.0060, -0.0097,  0.0005],\n",
      "        [ 0.0016,  0.0028,  0.0007,  ...,  0.0021,  0.0045,  0.0028],\n",
      "        [-0.0184,  0.0109, -0.0035,  ...,  0.0034, -0.0042,  0.0076]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight \n",
      "value: tensor([[-0.0026, -0.0014, -0.0056,  ...,  0.0103, -0.0028,  0.0090],\n",
      "        [-0.0106,  0.0062, -0.0019,  ...,  0.0191, -0.0063, -0.0040],\n",
      "        [ 0.0102, -0.0003,  0.0059,  ..., -0.0104,  0.0010,  0.0126],\n",
      "        ...,\n",
      "        [ 0.0035, -0.0025, -0.0007,  ..., -0.0008, -0.0060,  0.0090],\n",
      "        [ 0.0001,  0.0047, -0.0070,  ...,  0.0042, -0.0081,  0.0171],\n",
      "        [ 0.0042,  0.0067, -0.0056,  ...,  0.0074,  0.0097,  0.0178]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight \n",
      "value: tensor([[ 0.0030, -0.0070, -0.0029,  ...,  0.0037,  0.0002,  0.0003],\n",
      "        [ 0.0030, -0.0017, -0.0097,  ...,  0.0031,  0.0058,  0.0013],\n",
      "        [-0.0112, -0.0077,  0.0014,  ...,  0.0032, -0.0098,  0.0107],\n",
      "        ...,\n",
      "        [-0.0045, -0.0105,  0.0021,  ..., -0.0190,  0.0015, -0.0064],\n",
      "        [-0.0045,  0.0003, -0.0027,  ..., -0.0039, -0.0086, -0.0045],\n",
      "        [-0.0101,  0.0063,  0.0003,  ...,  0.0016, -0.0017, -0.0017]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight \n",
      "value: tensor([[-1.7793e-03,  6.2000e-03, -3.5821e-03,  ...,  6.0751e-03,\n",
      "         -4.7896e-03, -6.4086e-03],\n",
      "        [ 3.1086e-03,  3.5364e-04, -1.8037e-03,  ...,  1.3282e-03,\n",
      "         -2.7917e-03, -1.1467e-03],\n",
      "        [ 1.2521e-03, -1.2064e-03, -1.5969e-03,  ...,  2.4116e-03,\n",
      "         -1.1351e-03,  3.3437e-03],\n",
      "        ...,\n",
      "        [ 8.1110e-04,  1.4884e-03,  3.7755e-03,  ...,  4.4522e-04,\n",
      "          1.1743e-03, -9.5418e-05],\n",
      "        [-5.5103e-04,  1.9580e-03, -3.5026e-03,  ..., -8.0001e-04,\n",
      "          1.5134e-03, -1.5724e-04],\n",
      "        [-1.5746e-04,  3.8379e-04,  1.9362e-03,  ...,  3.5432e-03,\n",
      "          2.8307e-03,  2.2154e-03]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight \n",
      "value: tensor([[-4.6507e-03, -4.8200e-03,  1.8917e-03,  ..., -1.0422e-03,\n",
      "          2.0265e-03,  4.3793e-03],\n",
      "        [-8.3802e-03, -4.1996e-03,  3.1747e-03,  ..., -6.8230e-03,\n",
      "         -3.4513e-03, -4.3134e-03],\n",
      "        [ 1.4576e-03,  1.0955e-03, -6.1980e-03,  ...,  1.5392e-03,\n",
      "          3.0761e-05, -1.3328e-03],\n",
      "        ...,\n",
      "        [-5.3559e-05,  2.9744e-03, -4.3764e-03,  ...,  4.4163e-03,\n",
      "          3.4615e-03,  4.1609e-04],\n",
      "        [ 4.0405e-03, -1.2050e-02,  4.5838e-03,  ...,  2.0567e-03,\n",
      "          4.2630e-03,  8.0747e-03],\n",
      "        [ 1.8386e-03,  2.3792e-03, -5.0998e-05,  ..., -5.1359e-03,\n",
      "          2.6253e-03, -5.0685e-03]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight \n",
      "value: tensor([[-5.5407e-03,  6.2238e-03, -1.8213e-03,  ...,  4.9749e-03,\n",
      "          1.2074e-03, -6.0521e-03],\n",
      "        [-4.3724e-03,  1.4415e-03, -1.7427e-03,  ...,  4.2122e-03,\n",
      "          3.6350e-05,  1.0687e-03],\n",
      "        [-6.7283e-03,  1.5840e-03,  7.2794e-04,  ...,  5.4378e-03,\n",
      "         -4.4928e-03,  3.6730e-03],\n",
      "        ...,\n",
      "        [-1.8607e-03,  2.7997e-03,  1.2020e-03,  ...,  2.5962e-03,\n",
      "         -1.2432e-03, -4.6829e-03],\n",
      "        [ 1.0867e-03,  3.7787e-03, -2.5664e-03,  ..., -8.7857e-03,\n",
      "          2.2770e-03, -1.7731e-04],\n",
      "        [-2.8134e-03,  3.3380e-03, -9.3260e-03,  ..., -7.5001e-03,\n",
      "         -6.8584e-03, -3.9627e-03]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight \n",
      "value: tensor([[-0.0094, -0.0028, -0.0003,  ...,  0.0033, -0.0022,  0.0011],\n",
      "        [ 0.0080,  0.0009, -0.0034,  ...,  0.0120, -0.0041,  0.0051],\n",
      "        [-0.0007, -0.0057,  0.0002,  ..., -0.0053,  0.0029, -0.0060],\n",
      "        ...,\n",
      "        [ 0.0059, -0.0099, -0.0051,  ...,  0.0074,  0.0015,  0.0021],\n",
      "        [-0.0047,  0.0076, -0.0063,  ...,  0.0155, -0.0039, -0.0043],\n",
      "        [-0.0016,  0.0082, -0.0030,  ...,  0.0058,  0.0075, -0.0133]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight \n",
      "value: tensor([[-0.0011,  0.0042, -0.0091,  ..., -0.0006, -0.0017, -0.0022],\n",
      "        [-0.0111, -0.0201, -0.0093,  ..., -0.0071,  0.0108, -0.0003],\n",
      "        [ 0.0016, -0.0035,  0.0009,  ..., -0.0062, -0.0083, -0.0095],\n",
      "        ...,\n",
      "        [ 0.0046, -0.0050,  0.0017,  ..., -0.0123,  0.0028, -0.0011],\n",
      "        [-0.0050, -0.0074,  0.0073,  ...,  0.0048,  0.0049, -0.0060],\n",
      "        [-0.0013, -0.0121,  0.0113,  ...,  0.0061,  0.0047, -0.0011]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight \n",
      "value: tensor([[ 8.1193e-05, -4.8048e-03, -3.2071e-03,  ..., -5.5943e-03,\n",
      "          5.4197e-03, -7.9903e-03],\n",
      "        [ 1.2053e-02, -1.4033e-02,  4.1100e-04,  ..., -7.2533e-03,\n",
      "          3.6193e-04,  1.2301e-02],\n",
      "        [ 8.9766e-03,  2.1999e-02, -4.9389e-03,  ...,  2.9479e-03,\n",
      "         -1.5727e-03,  1.1088e-02],\n",
      "        ...,\n",
      "        [-4.7083e-03,  1.9031e-02,  7.9877e-04,  ...,  3.4651e-03,\n",
      "         -8.6825e-03,  6.2303e-03],\n",
      "        [-6.4348e-03, -5.1756e-03, -2.9700e-03,  ..., -1.3911e-03,\n",
      "          1.2091e-02, -1.3465e-03],\n",
      "        [-1.0359e-02,  1.7886e-02, -1.0678e-03,  ...,  3.3858e-03,\n",
      "         -1.5102e-02,  1.1391e-02]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight \n",
      "value: tensor([[-0.0029,  0.0002, -0.0069,  ..., -0.0060, -0.0037,  0.0003],\n",
      "        [-0.0012,  0.0020,  0.0052,  ...,  0.0040,  0.0113, -0.0017],\n",
      "        [-0.0016, -0.0038,  0.0080,  ..., -0.0050, -0.0003,  0.0014],\n",
      "        ...,\n",
      "        [-0.0063,  0.0051, -0.0060,  ..., -0.0083,  0.0003, -0.0009],\n",
      "        [ 0.0090,  0.0136, -0.0036,  ..., -0.0066,  0.0027, -0.0119],\n",
      "        [ 0.0081, -0.0033,  0.0005,  ...,  0.0054,  0.0137, -0.0008]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.proj_in.lora.down.weight \n",
      "value: tensor([[[[-0.0001]],\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         [[ 0.0070]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[ 0.0017]],\n",
      "\n",
      "         [[ 0.0100]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0018]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         [[ 0.0067]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0002]],\n",
      "\n",
      "         [[-0.0069]],\n",
      "\n",
      "         [[ 0.0085]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0020]],\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         [[-0.0049]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         [[ 0.0035]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0072]],\n",
      "\n",
      "         [[ 0.0025]],\n",
      "\n",
      "         [[-0.0059]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0014]],\n",
      "\n",
      "         [[ 0.0055]],\n",
      "\n",
      "         [[-0.0038]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0029]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0026]],\n",
      "\n",
      "         [[-0.0053]],\n",
      "\n",
      "         [[ 0.0090]]],\n",
      "\n",
      "\n",
      "        [[[-0.0015]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[ 0.0035]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0093]],\n",
      "\n",
      "         [[ 0.0010]],\n",
      "\n",
      "         [[-0.0001]]]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.proj_in.lora.up.weight \n",
      "value: tensor([[[[-6.1631e-03]],\n",
      "\n",
      "         [[-4.3038e-03]],\n",
      "\n",
      "         [[-2.0067e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7261e-03]],\n",
      "\n",
      "         [[ 3.2422e-03]],\n",
      "\n",
      "         [[ 4.6332e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 5.1652e-03]],\n",
      "\n",
      "         [[ 2.6195e-04]],\n",
      "\n",
      "         [[-3.6464e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.4257e-04]],\n",
      "\n",
      "         [[ 2.9475e-03]],\n",
      "\n",
      "         [[ 2.9216e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 7.8778e-03]],\n",
      "\n",
      "         [[ 2.2323e-03]],\n",
      "\n",
      "         [[-3.3371e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.9058e-03]],\n",
      "\n",
      "         [[ 4.3023e-03]],\n",
      "\n",
      "         [[-3.0750e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.7576e-05]],\n",
      "\n",
      "         [[ 2.7960e-04]],\n",
      "\n",
      "         [[-9.7108e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.7732e-03]],\n",
      "\n",
      "         [[-4.6864e-03]],\n",
      "\n",
      "         [[-1.0647e-02]]],\n",
      "\n",
      "\n",
      "        [[[-6.6749e-03]],\n",
      "\n",
      "         [[ 2.9037e-03]],\n",
      "\n",
      "         [[ 7.0872e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.1451e-03]],\n",
      "\n",
      "         [[ 4.9782e-04]],\n",
      "\n",
      "         [[-2.7548e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.0139e-02]],\n",
      "\n",
      "         [[-3.5889e-04]],\n",
      "\n",
      "         [[-9.2820e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3019e-03]],\n",
      "\n",
      "         [[ 5.0623e-03]],\n",
      "\n",
      "         [[ 4.9857e-03]]]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.proj_out.lora.down.weight \n",
      "value: tensor([[[[ 0.0029]],\n",
      "\n",
      "         [[-0.0047]],\n",
      "\n",
      "         [[-0.0069]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0028]],\n",
      "\n",
      "         [[ 0.0115]],\n",
      "\n",
      "         [[-0.0017]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0095]],\n",
      "\n",
      "         [[ 0.0034]],\n",
      "\n",
      "         [[-0.0052]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0174]],\n",
      "\n",
      "         [[-0.0082]],\n",
      "\n",
      "         [[-0.0046]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0116]],\n",
      "\n",
      "         [[-0.0123]],\n",
      "\n",
      "         [[ 0.0039]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0148]],\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[-0.0045]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0019]],\n",
      "\n",
      "         [[-0.0058]],\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0060]],\n",
      "\n",
      "         [[ 0.0088]],\n",
      "\n",
      "         [[-0.0066]]],\n",
      "\n",
      "\n",
      "        [[[-0.0004]],\n",
      "\n",
      "         [[ 0.0082]],\n",
      "\n",
      "         [[-0.0045]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0102]],\n",
      "\n",
      "         [[-0.0076]],\n",
      "\n",
      "         [[ 0.0019]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0009]],\n",
      "\n",
      "         [[ 0.0047]],\n",
      "\n",
      "         [[ 0.0049]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0139]],\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         [[ 0.0087]]]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.proj_out.lora.up.weight \n",
      "value: tensor([[[[ 0.0017]],\n",
      "\n",
      "         [[-0.0066]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         [[-0.0015]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0061]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[-0.0053]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         [[-0.0038]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0040]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         [[-0.0058]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[ 0.0083]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0032]],\n",
      "\n",
      "         [[ 0.0037]],\n",
      "\n",
      "         [[ 0.0047]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         [[ 0.0096]],\n",
      "\n",
      "         [[ 0.0029]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0018]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[ 0.0083]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[ 0.0016]],\n",
      "\n",
      "         [[-0.0038]]],\n",
      "\n",
      "\n",
      "        [[[-0.0036]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[ 0.0047]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0092]],\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         [[ 0.0027]]]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight \n",
      "value: tensor([[ 0.0018, -0.0102,  0.0077,  ...,  0.0022, -0.0046, -0.0090],\n",
      "        [ 0.0057, -0.0133, -0.0030,  ..., -0.0017, -0.0058, -0.0068],\n",
      "        [ 0.0151, -0.0021, -0.0014,  ...,  0.0033,  0.0006,  0.0003],\n",
      "        ...,\n",
      "        [-0.0074,  0.0008,  0.0017,  ..., -0.0063,  0.0129, -0.0055],\n",
      "        [-0.0024, -0.0053,  0.0004,  ...,  0.0053, -0.0027, -0.0101],\n",
      "        [ 0.0043, -0.0020,  0.0008,  ...,  0.0023, -0.0085, -0.0002]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight \n",
      "value: tensor([[ 0.0025, -0.0095, -0.0048,  ...,  0.0073, -0.0067, -0.0017],\n",
      "        [ 0.0068,  0.0061,  0.0023,  ..., -0.0018,  0.0066,  0.0065],\n",
      "        [-0.0032,  0.0031,  0.0003,  ..., -0.0035,  0.0079, -0.0071],\n",
      "        ...,\n",
      "        [-0.0003, -0.0047, -0.0030,  ..., -0.0018,  0.0007,  0.0015],\n",
      "        [ 0.0041, -0.0041, -0.0039,  ..., -0.0012,  0.0029, -0.0027],\n",
      "        [ 0.0017,  0.0015,  0.0063,  ..., -0.0050, -0.0068, -0.0039]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight \n",
      "value: tensor([[-3.3181e-03,  1.8279e-03,  2.0067e-03,  ..., -1.1390e-03,\n",
      "         -1.2556e-03,  1.2488e-03],\n",
      "        [ 5.4835e-03,  2.0408e-03,  1.9222e-03,  ..., -5.2445e-03,\n",
      "          2.0866e-03, -4.1212e-04],\n",
      "        [-6.8205e-03,  2.4207e-03,  4.8515e-04,  ..., -4.9475e-03,\n",
      "         -5.7562e-04, -3.0676e-03],\n",
      "        ...,\n",
      "        [ 1.8783e-03,  5.3908e-03, -4.1344e-03,  ...,  2.3232e-04,\n",
      "          2.3470e-03, -5.8394e-03],\n",
      "        [-6.4812e-04,  5.5019e-03,  2.0754e-03,  ..., -1.3919e-03,\n",
      "          8.8074e-04, -1.4507e-05],\n",
      "        [ 8.5051e-03, -5.1909e-03,  4.7459e-03,  ..., -6.2310e-03,\n",
      "          8.8889e-04, -1.3426e-03]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight \n",
      "value: tensor([[ 0.0034,  0.0061, -0.0032,  ...,  0.0123,  0.0002, -0.0059],\n",
      "        [ 0.0093, -0.0072, -0.0004,  ..., -0.0041,  0.0004,  0.0019],\n",
      "        [-0.0039,  0.0021,  0.0030,  ..., -0.0011,  0.0055,  0.0035],\n",
      "        ...,\n",
      "        [ 0.0007, -0.0017,  0.0063,  ...,  0.0012,  0.0001,  0.0064],\n",
      "        [ 0.0018,  0.0026,  0.0054,  ...,  0.0019,  0.0017, -0.0051],\n",
      "        [-0.0003, -0.0006, -0.0011,  ...,  0.0024,  0.0033, -0.0069]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight \n",
      "value: tensor([[ 0.0017, -0.0014, -0.0045,  ...,  0.0016, -0.0017, -0.0015],\n",
      "        [-0.0101, -0.0025,  0.0056,  ...,  0.0070, -0.0041,  0.0007],\n",
      "        [ 0.0047,  0.0039,  0.0091,  ...,  0.0060, -0.0047,  0.0096],\n",
      "        ...,\n",
      "        [ 0.0033,  0.0046,  0.0011,  ...,  0.0055,  0.0022,  0.0019],\n",
      "        [ 0.0068, -0.0041,  0.0065,  ..., -0.0004, -0.0057, -0.0046],\n",
      "        [-0.0099, -0.0130,  0.0012,  ...,  0.0014, -0.0071, -0.0041]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight \n",
      "value: tensor([[ 1.3673e-02,  8.3444e-05,  6.1254e-03,  ..., -5.3502e-03,\n",
      "          5.2428e-03,  5.6199e-03],\n",
      "        [-1.8646e-03,  9.0148e-03,  5.9363e-04,  ..., -7.7789e-04,\n",
      "         -2.7132e-03,  1.8054e-03],\n",
      "        [-3.5516e-03,  1.7669e-03, -7.6207e-03,  ..., -2.2957e-03,\n",
      "          4.8451e-03,  1.6658e-03],\n",
      "        ...,\n",
      "        [-1.1446e-02, -9.0545e-04, -2.9412e-03,  ...,  3.4637e-03,\n",
      "         -4.4871e-04, -2.8654e-03],\n",
      "        [-8.4570e-04, -1.2236e-03, -6.0150e-04,  ..., -3.9168e-05,\n",
      "          1.8983e-03, -4.4649e-04],\n",
      "        [ 5.1194e-03, -2.3173e-03, -2.5792e-03,  ..., -3.2334e-03,\n",
      "          1.7231e-03,  1.9041e-03]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight \n",
      "value: tensor([[ 2.2531e-03, -6.2245e-03,  4.0689e-03,  ...,  2.0901e-04,\n",
      "          2.3675e-03,  5.7177e-03],\n",
      "        [-1.5579e-03, -4.8576e-03,  3.2677e-03,  ..., -1.4275e-03,\n",
      "         -5.3325e-03,  7.1872e-04],\n",
      "        [ 4.8636e-03,  5.6637e-04, -1.7729e-03,  ...,  4.5912e-03,\n",
      "         -1.1830e-02, -1.1433e-03],\n",
      "        ...,\n",
      "        [-1.5141e-03,  3.8947e-03, -3.9392e-03,  ..., -2.6605e-03,\n",
      "         -6.3533e-04,  9.5869e-05],\n",
      "        [ 1.8454e-03, -2.9933e-04, -9.1844e-03,  ...,  9.1121e-03,\n",
      "          1.3972e-03, -8.9101e-03],\n",
      "        [-1.9394e-03, -1.5146e-03, -1.2201e-02,  ..., -2.6357e-03,\n",
      "          1.4457e-02,  2.9746e-03]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight \n",
      "value: tensor([[ 0.0068,  0.0036, -0.0003,  ...,  0.0006,  0.0075, -0.0028],\n",
      "        [-0.0025, -0.0012, -0.0014,  ..., -0.0042,  0.0009,  0.0071],\n",
      "        [ 0.0010, -0.0052, -0.0064,  ..., -0.0077,  0.0031, -0.0036],\n",
      "        ...,\n",
      "        [ 0.0033, -0.0011,  0.0053,  ...,  0.0032, -0.0051,  0.0013],\n",
      "        [ 0.0031, -0.0089, -0.0029,  ..., -0.0026,  0.0014,  0.0002],\n",
      "        [-0.0005, -0.0040,  0.0012,  ..., -0.0012,  0.0037,  0.0007]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight \n",
      "value: tensor([[-0.0041,  0.0016, -0.0018,  ..., -0.0033, -0.0023,  0.0030],\n",
      "        [ 0.0040,  0.0027, -0.0042,  ...,  0.0018,  0.0015, -0.0025],\n",
      "        [-0.0040,  0.0003,  0.0029,  ..., -0.0008, -0.0053,  0.0002],\n",
      "        ...,\n",
      "        [ 0.0004,  0.0039, -0.0066,  ...,  0.0076,  0.0004,  0.0030],\n",
      "        [-0.0044, -0.0059, -0.0026,  ..., -0.0011, -0.0037,  0.0016],\n",
      "        [ 0.0053, -0.0004,  0.0026,  ...,  0.0009, -0.0040,  0.0009]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight \n",
      "value: tensor([[ 0.0008,  0.0030,  0.0023,  ..., -0.0066, -0.0004, -0.0013],\n",
      "        [-0.0011,  0.0014,  0.0013,  ...,  0.0005,  0.0013, -0.0041],\n",
      "        [ 0.0039, -0.0024, -0.0034,  ..., -0.0070,  0.0011,  0.0056],\n",
      "        ...,\n",
      "        [ 0.0036,  0.0002,  0.0006,  ...,  0.0043, -0.0006, -0.0011],\n",
      "        [ 0.0009,  0.0002,  0.0027,  ...,  0.0010,  0.0022,  0.0018],\n",
      "        [-0.0016, -0.0026, -0.0022,  ..., -0.0014, -0.0024,  0.0020]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight \n",
      "value: tensor([[ 4.2752e-03,  1.0317e-02,  8.7365e-03,  ..., -1.3314e-03,\n",
      "          1.0111e-02,  4.5797e-04],\n",
      "        [-3.7246e-03, -9.4161e-03, -5.3410e-03,  ...,  1.9380e-03,\n",
      "         -4.7431e-04,  1.2469e-03],\n",
      "        [ 2.9027e-03, -1.6310e-02,  9.2787e-05,  ..., -1.4379e-02,\n",
      "          9.9324e-03, -3.9900e-03],\n",
      "        ...,\n",
      "        [ 7.4940e-03,  1.5627e-03,  1.0449e-02,  ..., -5.3973e-03,\n",
      "         -4.3934e-03,  7.4669e-04],\n",
      "        [ 1.3879e-03,  3.9179e-03,  2.7605e-03,  ...,  1.5343e-04,\n",
      "         -3.7228e-03,  1.0472e-02],\n",
      "        [ 1.4008e-02,  1.1126e-03, -1.1786e-02,  ...,  2.3959e-04,\n",
      "         -8.1378e-03, -5.6206e-03]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight \n",
      "value: tensor([[ 0.0017, -0.0163,  0.0118,  ..., -0.0096, -0.0004, -0.0057],\n",
      "        [ 0.0053, -0.0043,  0.0003,  ...,  0.0099, -0.0059, -0.0037],\n",
      "        [-0.0035,  0.0028,  0.0023,  ..., -0.0006,  0.0020, -0.0004],\n",
      "        ...,\n",
      "        [-0.0090, -0.0017, -0.0052,  ...,  0.0018,  0.0030,  0.0018],\n",
      "        [-0.0064,  0.0053, -0.0039,  ..., -0.0002,  0.0054, -0.0031],\n",
      "        [ 0.0002, -0.0048, -0.0007,  ...,  0.0060, -0.0067, -0.0003]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight \n",
      "value: tensor([[-2.1806e-03, -5.5966e-03,  1.8439e-03,  ...,  2.1269e-03,\n",
      "         -3.1028e-03, -4.3589e-03],\n",
      "        [ 3.8821e-03,  4.4734e-03,  2.3216e-03,  ...,  5.5493e-03,\n",
      "         -3.4661e-03,  2.0743e-03],\n",
      "        [ 1.1384e-03, -1.9779e-03,  6.7159e-05,  ...,  3.3906e-03,\n",
      "          2.1869e-03, -3.5586e-04],\n",
      "        ...,\n",
      "        [ 8.6614e-04,  1.9771e-03, -1.9805e-03,  ..., -4.7402e-03,\n",
      "          7.5322e-03, -1.6119e-03],\n",
      "        [ 1.8523e-03,  1.7537e-03, -7.7703e-03,  ..., -4.9159e-03,\n",
      "          2.3865e-03,  3.4377e-03],\n",
      "        [-3.0520e-03,  6.2551e-04,  3.3276e-03,  ..., -5.4418e-03,\n",
      "         -3.7125e-03, -1.4400e-02]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight \n",
      "value: tensor([[ 0.0012,  0.0024,  0.0030,  ..., -0.0010, -0.0050,  0.0009],\n",
      "        [ 0.0023,  0.0042, -0.0011,  ..., -0.0039, -0.0039, -0.0011],\n",
      "        [ 0.0008, -0.0064, -0.0011,  ...,  0.0012, -0.0005, -0.0011],\n",
      "        ...,\n",
      "        [ 0.0050,  0.0013,  0.0010,  ..., -0.0020, -0.0022,  0.0010],\n",
      "        [ 0.0015,  0.0046,  0.0045,  ...,  0.0009, -0.0043,  0.0036],\n",
      "        [-0.0008, -0.0036,  0.0008,  ...,  0.0003, -0.0028,  0.0008]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight \n",
      "value: tensor([[ 3.1545e-03, -7.3954e-04, -6.2696e-03,  ..., -3.4570e-03,\n",
      "         -1.3597e-03, -2.1353e-03],\n",
      "        [ 5.6019e-03,  1.9379e-03, -1.6368e-03,  ...,  1.8091e-03,\n",
      "         -5.9038e-03,  5.2159e-03],\n",
      "        [ 1.3160e-03,  8.9245e-03,  2.2592e-03,  ...,  1.7021e-03,\n",
      "         -3.9934e-03,  4.3779e-05],\n",
      "        ...,\n",
      "        [ 2.6930e-03,  6.2837e-03, -4.9175e-03,  ...,  7.2433e-03,\n",
      "         -4.5436e-03,  1.3889e-03],\n",
      "        [-6.2320e-04,  1.8431e-03, -2.9475e-03,  ...,  1.2279e-03,\n",
      "         -3.7650e-03, -8.4581e-04],\n",
      "        [ 1.7451e-03, -6.9430e-03,  4.4637e-03,  ...,  3.8604e-03,\n",
      "         -2.0249e-03, -3.4913e-03]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight \n",
      "value: tensor([[ 0.0007,  0.0083,  0.0016,  ...,  0.0081,  0.0003, -0.0035],\n",
      "        [ 0.0009, -0.0087,  0.0039,  ..., -0.0175,  0.0008,  0.0022],\n",
      "        [ 0.0043, -0.0081,  0.0021,  ...,  0.0062,  0.0001, -0.0004],\n",
      "        ...,\n",
      "        [ 0.0031, -0.0027,  0.0110,  ...,  0.0024,  0.0008, -0.0031],\n",
      "        [ 0.0013, -0.0072,  0.0044,  ...,  0.0077, -0.0027, -0.0045],\n",
      "        [-0.0026,  0.0010,  0.0004,  ...,  0.0028,  0.0047, -0.0059]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight \n",
      "value: tensor([[-4.9897e-03, -1.5701e-03, -1.0015e-02,  ..., -1.3680e-02,\n",
      "          5.9470e-03,  1.9374e-04],\n",
      "        [-1.3475e-02,  4.8580e-03,  4.3782e-03,  ...,  5.5252e-05,\n",
      "         -4.9917e-04, -1.3078e-02],\n",
      "        [ 1.6953e-03,  3.9904e-03, -5.0209e-03,  ..., -3.5669e-03,\n",
      "          1.2503e-03,  8.6146e-03],\n",
      "        ...,\n",
      "        [-7.2048e-03, -2.6026e-03, -5.4624e-03,  ...,  1.0953e-02,\n",
      "         -5.6988e-03,  4.9009e-03],\n",
      "        [-4.2576e-04, -4.3730e-03,  9.2522e-03,  ...,  5.9217e-03,\n",
      "         -4.6763e-03, -8.3733e-03],\n",
      "        [-3.8539e-03,  1.7153e-03,  1.2021e-03,  ..., -5.3203e-03,\n",
      "          8.1005e-03,  9.0283e-04]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight \n",
      "value: tensor([[ 0.0069,  0.0012,  0.0017,  ..., -0.0002, -0.0003, -0.0012],\n",
      "        [ 0.0105, -0.0048, -0.0009,  ...,  0.0012, -0.0028, -0.0018],\n",
      "        [-0.0023, -0.0056,  0.0082,  ...,  0.0022, -0.0010,  0.0060],\n",
      "        ...,\n",
      "        [ 0.0087,  0.0030,  0.0006,  ...,  0.0078, -0.0020,  0.0023],\n",
      "        [-0.0037,  0.0028, -0.0015,  ...,  0.0029, -0.0026,  0.0055],\n",
      "        [-0.0049,  0.0107,  0.0052,  ...,  0.0091, -0.0059,  0.0012]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight \n",
      "value: tensor([[-0.0002, -0.0047,  0.0008,  ...,  0.0013, -0.0005, -0.0010],\n",
      "        [-0.0020,  0.0042, -0.0076,  ...,  0.0033,  0.0027, -0.0135],\n",
      "        [-0.0011,  0.0017, -0.0069,  ...,  0.0029,  0.0046, -0.0108],\n",
      "        ...,\n",
      "        [ 0.0054, -0.0030, -0.0028,  ...,  0.0013, -0.0018, -0.0076],\n",
      "        [ 0.0041, -0.0020, -0.0012,  ..., -0.0070,  0.0065, -0.0059],\n",
      "        [ 0.0002, -0.0149,  0.0055,  ...,  0.0057,  0.0028, -0.0030]])\n",
      "\n",
      "key: unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight \n",
      "value: tensor([[-1.2040e-02, -5.1758e-03,  9.0024e-03,  ...,  8.5680e-04,\n",
      "          6.5408e-03, -9.3874e-04],\n",
      "        [ 1.1619e-02, -1.8354e-03, -7.0634e-03,  ..., -9.3538e-03,\n",
      "          1.1821e-02,  1.7443e-03],\n",
      "        [-4.5047e-03,  5.8278e-03,  6.9402e-03,  ...,  5.4031e-03,\n",
      "          7.3122e-03,  5.6026e-03],\n",
      "        ...,\n",
      "        [-4.9391e-03,  5.7758e-03,  7.8517e-03,  ...,  1.8527e-03,\n",
      "         -1.0487e-04, -5.9512e-03],\n",
      "        [ 1.0292e-03, -4.3078e-03,  2.4700e-03,  ..., -2.2654e-03,\n",
      "         -5.5193e-03,  1.4962e-03],\n",
      "        [-3.5807e-03,  9.4748e-03,  4.0883e-03,  ..., -1.6071e-03,\n",
      "         -5.9458e-03,  3.0600e-05]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.proj_in.lora.down.weight \n",
      "value: tensor([[[[-0.0012]],\n",
      "\n",
      "         [[-0.0075]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0071]],\n",
      "\n",
      "         [[-0.0037]],\n",
      "\n",
      "         [[ 0.0095]]],\n",
      "\n",
      "\n",
      "        [[[-0.0019]],\n",
      "\n",
      "         [[ 0.0069]],\n",
      "\n",
      "         [[ 0.0121]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0066]],\n",
      "\n",
      "         [[-0.0094]],\n",
      "\n",
      "         [[ 0.0085]]],\n",
      "\n",
      "\n",
      "        [[[-0.0027]],\n",
      "\n",
      "         [[-0.0038]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0024]],\n",
      "\n",
      "         [[-0.0028]],\n",
      "\n",
      "         [[ 0.0070]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0012]],\n",
      "\n",
      "         [[ 0.0005]],\n",
      "\n",
      "         [[ 0.0037]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0021]],\n",
      "\n",
      "         [[-0.0003]],\n",
      "\n",
      "         [[ 0.0021]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0041]],\n",
      "\n",
      "         [[ 0.0086]],\n",
      "\n",
      "         [[-0.0159]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0088]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[-0.0010]]],\n",
      "\n",
      "\n",
      "        [[[-0.0039]],\n",
      "\n",
      "         [[ 0.0183]],\n",
      "\n",
      "         [[ 0.0010]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0075]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         [[-0.0005]]]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.proj_in.lora.up.weight \n",
      "value: tensor([[[[ 8.4569e-03]],\n",
      "\n",
      "         [[ 1.0186e-02]],\n",
      "\n",
      "         [[ 1.3658e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.4320e-03]],\n",
      "\n",
      "         [[-5.7109e-05]],\n",
      "\n",
      "         [[ 6.6165e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.4754e-04]],\n",
      "\n",
      "         [[-1.7158e-02]],\n",
      "\n",
      "         [[-1.0872e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.9898e-04]],\n",
      "\n",
      "         [[-5.2980e-03]],\n",
      "\n",
      "         [[-3.4143e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.0321e-03]],\n",
      "\n",
      "         [[ 3.2596e-03]],\n",
      "\n",
      "         [[-5.8144e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.7128e-03]],\n",
      "\n",
      "         [[ 2.0031e-03]],\n",
      "\n",
      "         [[-3.8670e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 6.9170e-03]],\n",
      "\n",
      "         [[-1.0749e-02]],\n",
      "\n",
      "         [[ 1.6417e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.5010e-03]],\n",
      "\n",
      "         [[-2.2052e-03]],\n",
      "\n",
      "         [[-7.2340e-03]]],\n",
      "\n",
      "\n",
      "        [[[-9.1710e-03]],\n",
      "\n",
      "         [[ 1.3595e-02]],\n",
      "\n",
      "         [[ 3.2209e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.8644e-05]],\n",
      "\n",
      "         [[ 8.4089e-04]],\n",
      "\n",
      "         [[-3.3927e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.3521e-02]],\n",
      "\n",
      "         [[ 9.7877e-04]],\n",
      "\n",
      "         [[ 9.9528e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.3706e-03]],\n",
      "\n",
      "         [[-1.6146e-03]],\n",
      "\n",
      "         [[-3.4164e-03]]]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.proj_out.lora.down.weight \n",
      "value: tensor([[[[ 0.0028]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0073]],\n",
      "\n",
      "         [[-0.0073]],\n",
      "\n",
      "         [[ 0.0041]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0053]],\n",
      "\n",
      "         [[-0.0043]],\n",
      "\n",
      "         [[-0.0152]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0047]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[-0.0003]]],\n",
      "\n",
      "\n",
      "        [[[-0.0019]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[ 0.0051]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0187]],\n",
      "\n",
      "         [[ 0.0041]],\n",
      "\n",
      "         [[ 0.0007]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0044]],\n",
      "\n",
      "         [[-0.0110]],\n",
      "\n",
      "         [[ 0.0074]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0165]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[-0.0078]]],\n",
      "\n",
      "\n",
      "        [[[-0.0033]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[ 0.0068]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         [[-0.0084]],\n",
      "\n",
      "         [[-0.0015]]],\n",
      "\n",
      "\n",
      "        [[[-0.0035]],\n",
      "\n",
      "         [[ 0.0012]],\n",
      "\n",
      "         [[ 0.0071]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0039]],\n",
      "\n",
      "         [[-0.0095]],\n",
      "\n",
      "         [[ 0.0036]]]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.proj_out.lora.up.weight \n",
      "value: tensor([[[[ 0.0011]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[ 0.0096]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0039]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         [[ 0.0044]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0059]],\n",
      "\n",
      "         [[ 0.0030]],\n",
      "\n",
      "         [[-0.0040]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0039]],\n",
      "\n",
      "         [[-0.0141]],\n",
      "\n",
      "         [[ 0.0065]]],\n",
      "\n",
      "\n",
      "        [[[-0.0094]],\n",
      "\n",
      "         [[ 0.0021]],\n",
      "\n",
      "         [[ 0.0022]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0095]],\n",
      "\n",
      "         [[-0.0052]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0061]],\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         [[ 0.0059]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0048]],\n",
      "\n",
      "         [[-0.0089]],\n",
      "\n",
      "         [[ 0.0060]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0008]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         [[ 0.0055]]],\n",
      "\n",
      "\n",
      "        [[[-0.0002]],\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         [[ 0.0037]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0062]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[-0.0060]]]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight \n",
      "value: tensor([[-0.0009, -0.0028,  0.0093,  ...,  0.0047,  0.0093, -0.0015],\n",
      "        [ 0.0022,  0.0029, -0.0019,  ..., -0.0119,  0.0050,  0.0101],\n",
      "        [-0.0052,  0.0009,  0.0043,  ...,  0.0039, -0.0121, -0.0085],\n",
      "        ...,\n",
      "        [-0.0036,  0.0070,  0.0054,  ..., -0.0026, -0.0035, -0.0049],\n",
      "        [ 0.0004, -0.0087,  0.0092,  ...,  0.0051,  0.0127,  0.0038],\n",
      "        [-0.0013,  0.0019, -0.0078,  ...,  0.0036,  0.0002,  0.0007]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight \n",
      "value: tensor([[-2.0159e-03, -7.2402e-03,  4.9374e-03,  ..., -4.0789e-03,\n",
      "          2.3183e-03,  8.8404e-03],\n",
      "        [ 2.8012e-03, -1.1668e-02,  6.9704e-04,  ...,  6.7810e-06,\n",
      "         -1.4308e-03,  5.3503e-04],\n",
      "        [ 3.8184e-03, -5.8460e-03,  3.5123e-03,  ..., -3.1739e-03,\n",
      "          9.5962e-03, -3.7068e-03],\n",
      "        ...,\n",
      "        [ 1.8202e-03,  1.0904e-03, -2.5850e-03,  ..., -7.6704e-03,\n",
      "         -9.3698e-03,  3.0076e-03],\n",
      "        [-3.5548e-03, -8.9006e-03, -3.2220e-03,  ...,  1.3774e-02,\n",
      "          4.4834e-03, -5.3068e-04],\n",
      "        [ 3.5594e-04,  4.7847e-03, -3.8957e-03,  ..., -3.1899e-03,\n",
      "          7.1835e-05,  1.1107e-03]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight \n",
      "value: tensor([[ 7.9293e-04, -8.8521e-05, -3.4777e-03,  ...,  1.9804e-03,\n",
      "         -3.2971e-03, -2.9932e-04],\n",
      "        [ 8.8692e-03,  1.8253e-03,  5.0787e-03,  ..., -3.0371e-03,\n",
      "          6.0518e-03,  7.2049e-03],\n",
      "        [-3.8811e-03, -3.1342e-03, -1.2842e-02,  ...,  1.2120e-03,\n",
      "          1.0849e-02,  7.7450e-04],\n",
      "        ...,\n",
      "        [-2.4195e-03,  6.4498e-03,  4.8836e-03,  ...,  8.0435e-03,\n",
      "          4.7995e-03,  7.4388e-03],\n",
      "        [ 5.6026e-03,  7.8559e-03,  3.4593e-03,  ...,  9.9481e-04,\n",
      "          4.0195e-04, -4.4606e-03],\n",
      "        [ 6.7979e-03, -4.6076e-03, -2.3418e-03,  ..., -3.1905e-03,\n",
      "          3.2698e-03,  3.1247e-03]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight \n",
      "value: tensor([[ 3.2828e-03, -3.1922e-03, -1.2702e-02,  ...,  7.6906e-03,\n",
      "          4.8098e-03, -8.3844e-03],\n",
      "        [-1.0954e-03, -1.0261e-02,  5.9013e-03,  ..., -1.2640e-02,\n",
      "         -8.4783e-04,  1.2323e-02],\n",
      "        [ 1.2961e-03,  1.2489e-02,  8.7113e-03,  ...,  8.8744e-03,\n",
      "          4.6156e-03, -5.9173e-03],\n",
      "        ...,\n",
      "        [ 5.3031e-04,  2.3040e-03, -8.6313e-03,  ...,  1.1616e-02,\n",
      "         -5.2119e-03, -1.5650e-02],\n",
      "        [ 2.5580e-03,  6.6959e-03, -8.4386e-05,  ..., -8.0236e-03,\n",
      "         -4.7706e-03,  4.1019e-03],\n",
      "        [ 6.3765e-03,  5.3596e-03,  2.5761e-03,  ...,  8.6824e-04,\n",
      "          1.7299e-03,  5.6491e-04]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight \n",
      "value: tensor([[ 0.0046,  0.0082, -0.0125,  ..., -0.0091, -0.0019,  0.0053],\n",
      "        [ 0.0123,  0.0151, -0.0038,  ...,  0.0128,  0.0144,  0.0082],\n",
      "        [ 0.0075, -0.0003,  0.0009,  ..., -0.0147,  0.0027, -0.0041],\n",
      "        ...,\n",
      "        [-0.0009, -0.0026,  0.0074,  ...,  0.0080, -0.0023,  0.0020],\n",
      "        [ 0.0066, -0.0009, -0.0105,  ...,  0.0015, -0.0023, -0.0033],\n",
      "        [ 0.0073, -0.0042, -0.0059,  ...,  0.0097, -0.0066, -0.0021]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight \n",
      "value: tensor([[ 3.4833e-03,  1.2356e-02,  2.3502e-03,  ..., -2.8917e-03,\n",
      "         -9.9104e-03, -1.8911e-03],\n",
      "        [-2.4668e-03,  7.3605e-03,  7.1214e-04,  ...,  7.7329e-04,\n",
      "         -4.3336e-03,  1.1803e-02],\n",
      "        [-6.7555e-03, -5.5505e-04,  9.1494e-04,  ..., -2.5288e-03,\n",
      "         -1.1307e-02,  4.4193e-05],\n",
      "        ...,\n",
      "        [-1.2794e-03, -9.9864e-04,  6.2031e-04,  ...,  1.0529e-04,\n",
      "         -4.1212e-03,  4.7195e-03],\n",
      "        [-4.0556e-03, -1.3092e-03, -4.2004e-03,  ...,  1.0046e-02,\n",
      "         -7.8408e-04, -2.9416e-03],\n",
      "        [ 7.2650e-03,  2.2121e-03,  5.4016e-03,  ...,  1.2386e-02,\n",
      "         -9.7292e-03,  1.0018e-02]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight \n",
      "value: tensor([[-0.0050, -0.0077, -0.0075,  ...,  0.0003, -0.0064,  0.0002],\n",
      "        [-0.0002, -0.0009,  0.0109,  ..., -0.0125, -0.0053,  0.0085],\n",
      "        [ 0.0077, -0.0066,  0.0036,  ..., -0.0028,  0.0018,  0.0058],\n",
      "        ...,\n",
      "        [ 0.0021,  0.0026,  0.0103,  ..., -0.0088, -0.0017,  0.0019],\n",
      "        [ 0.0019, -0.0003,  0.0042,  ...,  0.0050,  0.0038,  0.0016],\n",
      "        [ 0.0059,  0.0082, -0.0077,  ..., -0.0056,  0.0020,  0.0084]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight \n",
      "value: tensor([[-0.0032, -0.0044,  0.0037,  ...,  0.0040, -0.0049, -0.0092],\n",
      "        [-0.0047,  0.0098, -0.0059,  ...,  0.0039,  0.0003, -0.0038],\n",
      "        [ 0.0034,  0.0016, -0.0032,  ..., -0.0010, -0.0028,  0.0071],\n",
      "        ...,\n",
      "        [ 0.0045,  0.0014,  0.0013,  ..., -0.0118, -0.0006,  0.0035],\n",
      "        [ 0.0019, -0.0071,  0.0056,  ..., -0.0049,  0.0041, -0.0032],\n",
      "        [-0.0035,  0.0051, -0.0065,  ...,  0.0025, -0.0009, -0.0025]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight \n",
      "value: tensor([[-0.0027, -0.0017, -0.0024,  ..., -0.0012, -0.0041,  0.0015],\n",
      "        [-0.0114,  0.0016, -0.0030,  ..., -0.0021, -0.0035,  0.0006],\n",
      "        [-0.0095,  0.0010, -0.0014,  ...,  0.0011,  0.0068,  0.0069],\n",
      "        ...,\n",
      "        [-0.0066,  0.0080, -0.0004,  ..., -0.0110, -0.0119, -0.0027],\n",
      "        [-0.0004, -0.0009,  0.0004,  ..., -0.0008, -0.0068,  0.0070],\n",
      "        [ 0.0025, -0.0036, -0.0072,  ..., -0.0051, -0.0005, -0.0031]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight \n",
      "value: tensor([[-0.0003, -0.0007,  0.0041,  ...,  0.0003,  0.0026,  0.0033],\n",
      "        [-0.0021,  0.0036, -0.0007,  ...,  0.0034, -0.0071,  0.0009],\n",
      "        [-0.0007,  0.0041,  0.0009,  ...,  0.0002,  0.0016, -0.0062],\n",
      "        ...,\n",
      "        [-0.0007,  0.0017,  0.0012,  ...,  0.0050,  0.0058,  0.0006],\n",
      "        [-0.0048,  0.0030, -0.0072,  ...,  0.0033, -0.0016,  0.0030],\n",
      "        [ 0.0046, -0.0015,  0.0007,  ..., -0.0075,  0.0044, -0.0037]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight \n",
      "value: tensor([[-0.0076, -0.0005, -0.0095,  ..., -0.0028, -0.0015, -0.0009],\n",
      "        [-0.0007, -0.0035,  0.0007,  ..., -0.0045,  0.0024,  0.0117],\n",
      "        [-0.0028, -0.0064, -0.0108,  ...,  0.0027, -0.0090, -0.0104],\n",
      "        ...,\n",
      "        [ 0.0062, -0.0011,  0.0032,  ...,  0.0024,  0.0014,  0.0023],\n",
      "        [-0.0043, -0.0082,  0.0026,  ...,  0.0023,  0.0058, -0.0003],\n",
      "        [-0.0061,  0.0073,  0.0025,  ...,  0.0009, -0.0051,  0.0075]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight \n",
      "value: tensor([[-1.0481e-02, -4.8184e-03,  5.9048e-03,  ...,  1.1235e-02,\n",
      "         -3.2339e-03,  8.7874e-04],\n",
      "        [ 3.1537e-03, -9.5585e-03, -1.5552e-02,  ...,  4.9316e-03,\n",
      "         -1.3392e-02, -7.4391e-03],\n",
      "        [ 4.6410e-03, -1.3533e-02,  2.4817e-03,  ...,  7.3495e-03,\n",
      "         -4.4229e-03,  1.0257e-03],\n",
      "        ...,\n",
      "        [ 6.9888e-05, -6.4306e-03,  4.3036e-03,  ..., -1.1537e-02,\n",
      "         -1.4069e-03,  1.6537e-03],\n",
      "        [-1.8952e-03,  9.9521e-03, -1.0921e-02,  ...,  7.4379e-03,\n",
      "          1.4365e-03,  1.6162e-03],\n",
      "        [-8.3775e-03, -5.0192e-03, -1.2660e-03,  ...,  2.7200e-03,\n",
      "          4.0661e-03,  4.5798e-03]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight \n",
      "value: tensor([[ 0.0009, -0.0054,  0.0023,  ..., -0.0004, -0.0113,  0.0025],\n",
      "        [ 0.0050,  0.0020, -0.0145,  ...,  0.0116,  0.0043,  0.0031],\n",
      "        [ 0.0067, -0.0008, -0.0093,  ..., -0.0005, -0.0023,  0.0013],\n",
      "        ...,\n",
      "        [ 0.0013,  0.0039, -0.0043,  ..., -0.0041,  0.0090,  0.0066],\n",
      "        [ 0.0025, -0.0069,  0.0061,  ..., -0.0054,  0.0062,  0.0078],\n",
      "        [-0.0026,  0.0015, -0.0039,  ...,  0.0022,  0.0035,  0.0054]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight \n",
      "value: tensor([[ 0.0012,  0.0005, -0.0005,  ..., -0.0016,  0.0031, -0.0003],\n",
      "        [ 0.0032,  0.0036, -0.0057,  ..., -0.0046, -0.0073,  0.0005],\n",
      "        [ 0.0007,  0.0034,  0.0017,  ...,  0.0029,  0.0033, -0.0008],\n",
      "        ...,\n",
      "        [ 0.0054,  0.0039, -0.0109,  ...,  0.0003, -0.0044, -0.0020],\n",
      "        [ 0.0077,  0.0042,  0.0044,  ..., -0.0020, -0.0031, -0.0022],\n",
      "        [ 0.0036, -0.0032, -0.0039,  ..., -0.0003,  0.0002,  0.0078]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight \n",
      "value: tensor([[-1.4617e-03, -8.9423e-03, -6.9261e-04,  ..., -1.6687e-03,\n",
      "          5.9747e-04, -8.5146e-03],\n",
      "        [ 3.5968e-03,  4.3927e-03, -2.0122e-04,  ..., -4.4711e-03,\n",
      "          1.0295e-03,  1.0700e-02],\n",
      "        [ 2.0453e-03, -4.8486e-03,  1.5256e-03,  ...,  2.7736e-03,\n",
      "          2.8832e-05,  9.5205e-03],\n",
      "        ...,\n",
      "        [ 3.1988e-04, -7.9069e-03, -5.8416e-04,  ..., -3.8412e-03,\n",
      "         -3.0886e-03,  1.0384e-02],\n",
      "        [ 4.9056e-03, -2.7811e-03,  1.2412e-02,  ..., -5.4913e-03,\n",
      "         -4.5999e-03, -3.8729e-03],\n",
      "        [-3.6534e-03, -3.5665e-03,  2.1239e-03,  ...,  3.8042e-04,\n",
      "          2.8573e-03,  8.7991e-03]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight \n",
      "value: tensor([[ 0.0005, -0.0087,  0.0154,  ...,  0.0062, -0.0006, -0.0013],\n",
      "        [ 0.0066, -0.0006, -0.0030,  ..., -0.0022, -0.0064,  0.0015],\n",
      "        [-0.0052, -0.0007,  0.0081,  ..., -0.0076,  0.0051, -0.0006],\n",
      "        ...,\n",
      "        [ 0.0042, -0.0065,  0.0089,  ..., -0.0030,  0.0008, -0.0069],\n",
      "        [-0.0021,  0.0012, -0.0027,  ..., -0.0024,  0.0017,  0.0169],\n",
      "        [ 0.0088, -0.0059, -0.0067,  ..., -0.0032, -0.0060, -0.0067]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight \n",
      "value: tensor([[-6.9128e-03, -7.0547e-03, -9.9907e-03,  ...,  7.4741e-03,\n",
      "          2.5508e-03, -8.6331e-03],\n",
      "        [-1.4748e-03, -1.3113e-02, -5.7137e-03,  ..., -7.9462e-03,\n",
      "         -7.0874e-03,  8.8873e-04],\n",
      "        [ 5.9877e-04, -4.9106e-03,  9.0045e-04,  ..., -5.5152e-03,\n",
      "         -4.4907e-03, -7.1786e-03],\n",
      "        ...,\n",
      "        [-3.4146e-03,  8.3722e-03, -7.6208e-04,  ...,  1.1731e-04,\n",
      "          1.9482e-02,  1.0122e-03],\n",
      "        [-5.5991e-03,  7.4743e-03, -4.5767e-03,  ..., -1.2404e-02,\n",
      "          4.0430e-03, -4.6188e-03],\n",
      "        [ 1.7842e-05,  4.5241e-03,  2.9087e-04,  ...,  1.2557e-03,\n",
      "          2.2180e-04,  1.5467e-02]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight \n",
      "value: tensor([[ 3.5590e-03,  7.8786e-03, -1.1748e-02,  ...,  5.0340e-03,\n",
      "          2.8332e-04, -8.0651e-03],\n",
      "        [-1.3892e-02,  8.6803e-03,  3.8162e-03,  ...,  3.5267e-03,\n",
      "         -4.7087e-03, -4.2456e-03],\n",
      "        [-4.6763e-03, -1.7205e-03, -5.9470e-03,  ..., -2.3954e-03,\n",
      "          3.6627e-03, -5.6813e-03],\n",
      "        ...,\n",
      "        [-3.0318e-03, -5.5972e-03, -2.6777e-04,  ..., -1.3040e-02,\n",
      "         -5.1576e-03,  4.9787e-04],\n",
      "        [ 1.9807e-03, -6.2940e-03, -1.2436e-02,  ...,  1.1397e-02,\n",
      "         -6.9882e-03,  3.6724e-03],\n",
      "        [-3.2503e-05,  8.7418e-03, -1.0835e-03,  ...,  3.4590e-03,\n",
      "          1.9509e-04,  1.1331e-02]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight \n",
      "value: tensor([[-0.0009, -0.0130,  0.0023,  ...,  0.0028,  0.0010,  0.0018],\n",
      "        [-0.0013, -0.0009,  0.0003,  ...,  0.0007,  0.0048,  0.0008],\n",
      "        [ 0.0032,  0.0030,  0.0089,  ..., -0.0063, -0.0020,  0.0040],\n",
      "        ...,\n",
      "        [-0.0011,  0.0046, -0.0074,  ...,  0.0010, -0.0024,  0.0076],\n",
      "        [ 0.0084,  0.0021, -0.0021,  ..., -0.0006, -0.0020, -0.0080],\n",
      "        [ 0.0040,  0.0036,  0.0008,  ...,  0.0050, -0.0104,  0.0043]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight \n",
      "value: tensor([[ 8.3174e-03,  5.0514e-03,  5.2893e-03,  ..., -8.2238e-03,\n",
      "          5.3189e-03,  3.5051e-03],\n",
      "        [ 2.0782e-03,  1.2983e-02, -4.3975e-03,  ..., -2.7791e-03,\n",
      "          3.8999e-03,  8.0714e-03],\n",
      "        [-4.2426e-03,  2.1264e-04, -5.2360e-03,  ...,  1.1442e-02,\n",
      "         -1.0160e-02,  6.2080e-03],\n",
      "        ...,\n",
      "        [-6.6893e-04, -7.2201e-04, -2.9062e-03,  ..., -7.0081e-03,\n",
      "          5.2971e-04, -1.7699e-03],\n",
      "        [-5.3336e-04, -1.0974e-03, -9.4765e-05,  ...,  4.9656e-04,\n",
      "          3.2896e-03, -3.0045e-03],\n",
      "        [ 1.8852e-03, -5.7403e-03,  7.6314e-03,  ...,  1.8431e-03,\n",
      "          9.9669e-03, -3.7649e-03]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.proj_in.lora.down.weight \n",
      "value: tensor([[[[-0.0032]],\n",
      "\n",
      "         [[-0.0075]],\n",
      "\n",
      "         [[-0.0053]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0145]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[ 0.0053]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0008]],\n",
      "\n",
      "         [[-0.0098]],\n",
      "\n",
      "         [[ 0.0021]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0047]],\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[ 0.0006]]],\n",
      "\n",
      "\n",
      "        [[[-0.0056]],\n",
      "\n",
      "         [[ 0.0143]],\n",
      "\n",
      "         [[ 0.0068]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0216]],\n",
      "\n",
      "         [[-0.0027]],\n",
      "\n",
      "         [[-0.0007]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0025]],\n",
      "\n",
      "         [[ 0.0174]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0068]],\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         [[-0.0036]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0067]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0035]],\n",
      "\n",
      "         [[-0.0086]],\n",
      "\n",
      "         [[ 0.0065]]],\n",
      "\n",
      "\n",
      "        [[[-0.0036]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0123]],\n",
      "\n",
      "         [[-0.0151]],\n",
      "\n",
      "         [[-0.0025]]]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.proj_in.lora.up.weight \n",
      "value: tensor([[[[-0.0049]],\n",
      "\n",
      "         [[ 0.0061]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[-0.0133]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0057]],\n",
      "\n",
      "         [[-0.0101]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0055]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         [[-0.0049]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0018]],\n",
      "\n",
      "         [[ 0.0101]],\n",
      "\n",
      "         [[ 0.0068]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         [[-0.0045]],\n",
      "\n",
      "         [[ 0.0017]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0099]],\n",
      "\n",
      "         [[-0.0143]],\n",
      "\n",
      "         [[ 0.0061]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0046]],\n",
      "\n",
      "         [[-0.0081]],\n",
      "\n",
      "         [[-0.0040]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0027]],\n",
      "\n",
      "         [[-0.0081]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[-0.0095]],\n",
      "\n",
      "         [[ 0.0017]]],\n",
      "\n",
      "\n",
      "        [[[-0.0088]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[-0.0037]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0097]],\n",
      "\n",
      "         [[-0.0069]],\n",
      "\n",
      "         [[-0.0007]]]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.proj_out.lora.down.weight \n",
      "value: tensor([[[[ 6.5130e-03]],\n",
      "\n",
      "         [[-6.1718e-03]],\n",
      "\n",
      "         [[-4.2481e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5796e-04]],\n",
      "\n",
      "         [[-2.9537e-03]],\n",
      "\n",
      "         [[ 2.6842e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0245e-02]],\n",
      "\n",
      "         [[-4.5713e-03]],\n",
      "\n",
      "         [[-3.5845e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1968e-02]],\n",
      "\n",
      "         [[ 7.1204e-05]],\n",
      "\n",
      "         [[ 4.0579e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.3028e-03]],\n",
      "\n",
      "         [[ 4.8489e-03]],\n",
      "\n",
      "         [[ 9.9462e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5665e-02]],\n",
      "\n",
      "         [[ 7.0044e-03]],\n",
      "\n",
      "         [[-4.2475e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.1360e-03]],\n",
      "\n",
      "         [[ 1.9893e-03]],\n",
      "\n",
      "         [[ 3.4546e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.9980e-03]],\n",
      "\n",
      "         [[ 2.0328e-04]],\n",
      "\n",
      "         [[-3.8150e-03]]],\n",
      "\n",
      "\n",
      "        [[[-6.1710e-03]],\n",
      "\n",
      "         [[-7.5746e-03]],\n",
      "\n",
      "         [[-2.1413e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.2571e-04]],\n",
      "\n",
      "         [[ 8.0741e-03]],\n",
      "\n",
      "         [[ 1.9350e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.2602e-03]],\n",
      "\n",
      "         [[ 1.1437e-02]],\n",
      "\n",
      "         [[-3.8114e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9990e-03]],\n",
      "\n",
      "         [[ 4.4724e-03]],\n",
      "\n",
      "         [[ 6.3319e-03]]]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.proj_out.lora.up.weight \n",
      "value: tensor([[[[-0.0101]],\n",
      "\n",
      "         [[-0.0069]],\n",
      "\n",
      "         [[-0.0063]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         [[ 0.0041]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0051]],\n",
      "\n",
      "         [[-0.0150]],\n",
      "\n",
      "         [[-0.0256]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0066]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[ 0.0039]]],\n",
      "\n",
      "\n",
      "        [[[-0.0076]],\n",
      "\n",
      "         [[-0.0030]],\n",
      "\n",
      "         [[ 0.0062]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0063]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[ 0.0034]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0002]],\n",
      "\n",
      "         [[-0.0054]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0072]],\n",
      "\n",
      "         [[-0.0027]],\n",
      "\n",
      "         [[ 0.0110]]],\n",
      "\n",
      "\n",
      "        [[[-0.0026]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[-0.0038]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[-0.0092]],\n",
      "\n",
      "         [[ 0.0014]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0012]],\n",
      "\n",
      "         [[ 0.0149]],\n",
      "\n",
      "         [[ 0.0089]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[-0.0055]],\n",
      "\n",
      "         [[ 0.0035]]]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight \n",
      "value: tensor([[ 3.6634e-03, -6.1321e-04, -5.5533e-03,  ..., -4.3742e-03,\n",
      "          6.1518e-03, -4.9966e-03],\n",
      "        [ 3.1056e-03, -7.6511e-03, -2.8859e-04,  ...,  5.0170e-03,\n",
      "          4.1297e-03, -1.2913e-02],\n",
      "        [ 7.2862e-04, -2.9045e-03,  8.8446e-04,  ...,  7.0871e-03,\n",
      "         -6.4174e-05,  1.0887e-02],\n",
      "        ...,\n",
      "        [-3.5965e-03,  1.5348e-02, -4.9852e-03,  ..., -3.5263e-03,\n",
      "         -4.0254e-03,  3.7746e-03],\n",
      "        [-3.9455e-03, -3.3174e-03, -2.1478e-03,  ...,  3.3516e-03,\n",
      "         -1.1200e-02, -4.7878e-03],\n",
      "        [-2.0705e-03, -6.4009e-03,  1.5246e-03,  ..., -6.6004e-03,\n",
      "          8.2860e-03, -3.9117e-03]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight \n",
      "value: tensor([[ 0.0002, -0.0010, -0.0024,  ...,  0.0009,  0.0050, -0.0027],\n",
      "        [ 0.0093, -0.0035,  0.0026,  ..., -0.0077, -0.0019, -0.0002],\n",
      "        [-0.0011,  0.0002, -0.0036,  ...,  0.0029,  0.0046, -0.0064],\n",
      "        ...,\n",
      "        [ 0.0047, -0.0056,  0.0026,  ..., -0.0015,  0.0022,  0.0018],\n",
      "        [ 0.0045,  0.0026, -0.0097,  ...,  0.0082,  0.0052,  0.0003],\n",
      "        [ 0.0075, -0.0015, -0.0075,  ..., -0.0016, -0.0006, -0.0118]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight \n",
      "value: tensor([[ 1.4529e-03, -7.4410e-04,  7.7202e-03,  ..., -2.2061e-03,\n",
      "         -3.1630e-03,  3.6627e-03],\n",
      "        [-2.8126e-03, -1.1316e-03,  1.8680e-03,  ...,  6.6810e-03,\n",
      "         -2.0414e-03, -1.1771e-02],\n",
      "        [ 7.8735e-04, -6.8569e-03, -8.5292e-04,  ...,  5.0132e-03,\n",
      "          2.7712e-04,  4.9853e-03],\n",
      "        ...,\n",
      "        [-1.0089e-02,  1.6354e-05, -1.4690e-03,  ..., -5.5784e-03,\n",
      "          3.1648e-04, -8.2373e-03],\n",
      "        [ 3.6654e-03, -4.2205e-03,  7.3594e-03,  ..., -1.6951e-03,\n",
      "          4.8202e-04, -3.1278e-03],\n",
      "        [ 8.5407e-03, -9.3064e-03, -2.6082e-04,  ..., -5.0720e-03,\n",
      "          8.6665e-03,  2.1257e-03]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight \n",
      "value: tensor([[ 0.0043,  0.0041,  0.0008,  ...,  0.0034, -0.0038, -0.0005],\n",
      "        [ 0.0032, -0.0098,  0.0002,  ..., -0.0007,  0.0016, -0.0056],\n",
      "        [-0.0080, -0.0012, -0.0022,  ..., -0.0019,  0.0013,  0.0099],\n",
      "        ...,\n",
      "        [-0.0152,  0.0030, -0.0080,  ...,  0.0078,  0.0050, -0.0015],\n",
      "        [-0.0039, -0.0123,  0.0062,  ...,  0.0041, -0.0019, -0.0002],\n",
      "        [-0.0042,  0.0008, -0.0037,  ...,  0.0068,  0.0066, -0.0007]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight \n",
      "value: tensor([[-0.0015, -0.0170,  0.0025,  ..., -0.0052,  0.0001, -0.0058],\n",
      "        [ 0.0094,  0.0086, -0.0083,  ...,  0.0012,  0.0056,  0.0103],\n",
      "        [-0.0062, -0.0011, -0.0022,  ...,  0.0051, -0.0058, -0.0160],\n",
      "        ...,\n",
      "        [ 0.0199,  0.0099, -0.0155,  ..., -0.0043, -0.0015,  0.0077],\n",
      "        [ 0.0057,  0.0107, -0.0099,  ...,  0.0071, -0.0036, -0.0042],\n",
      "        [ 0.0055, -0.0088,  0.0078,  ..., -0.0031,  0.0060, -0.0060]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight \n",
      "value: tensor([[ 0.0051,  0.0045,  0.0022,  ...,  0.0031,  0.0003,  0.0049],\n",
      "        [-0.0094, -0.0006, -0.0074,  ...,  0.0012,  0.0015,  0.0090],\n",
      "        [-0.0030,  0.0056,  0.0085,  ..., -0.0015, -0.0017,  0.0060],\n",
      "        ...,\n",
      "        [ 0.0016, -0.0037,  0.0020,  ...,  0.0114, -0.0071,  0.0002],\n",
      "        [ 0.0085,  0.0008,  0.0157,  ..., -0.0086,  0.0104,  0.0031],\n",
      "        [ 0.0043,  0.0026,  0.0130,  ...,  0.0015, -0.0048, -0.0138]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight \n",
      "value: tensor([[-2.1046e-03,  3.6028e-03, -6.1972e-03,  ...,  9.1209e-04,\n",
      "         -5.0181e-03,  9.5079e-04],\n",
      "        [-8.7624e-03, -2.6195e-03,  1.0898e-02,  ...,  5.7911e-03,\n",
      "          1.7428e-03, -1.0260e-03],\n",
      "        [ 1.8320e-03,  4.5578e-03,  4.6885e-03,  ..., -8.3904e-03,\n",
      "          1.1734e-02, -7.4898e-03],\n",
      "        ...,\n",
      "        [ 3.8499e-03, -8.8142e-03, -8.2316e-05,  ..., -1.4614e-02,\n",
      "         -1.0905e-02,  1.4661e-02],\n",
      "        [ 6.8865e-03,  2.4228e-03, -7.4692e-03,  ...,  7.1807e-03,\n",
      "          8.3905e-03, -2.3703e-03],\n",
      "        [ 1.1907e-02,  6.7464e-03, -1.1771e-02,  ..., -7.3833e-03,\n",
      "          5.7632e-03,  4.0204e-03]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight \n",
      "value: tensor([[-1.1660e-03,  7.1420e-03,  2.1186e-03,  ..., -3.9428e-03,\n",
      "         -7.6446e-03,  6.7767e-03],\n",
      "        [ 5.0878e-03,  6.1241e-03, -4.7122e-03,  ...,  6.6480e-04,\n",
      "         -6.3313e-04, -9.2397e-03],\n",
      "        [ 8.7304e-04,  2.1263e-03,  3.7448e-03,  ..., -1.0852e-03,\n",
      "         -1.5775e-04, -8.1591e-03],\n",
      "        ...,\n",
      "        [ 4.1656e-03, -5.1941e-03, -1.7310e-03,  ...,  3.8552e-03,\n",
      "          1.1964e-05,  2.8504e-03],\n",
      "        [ 1.0534e-03, -2.2199e-03, -1.0368e-03,  ..., -9.6384e-03,\n",
      "         -8.5575e-04, -7.0140e-03],\n",
      "        [ 2.1439e-03, -6.6007e-03,  3.5758e-03,  ...,  4.8581e-03,\n",
      "          8.5601e-04,  2.6160e-03]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight \n",
      "value: tensor([[-3.0275e-03,  1.7396e-03, -3.1905e-03,  ..., -1.2511e-03,\n",
      "         -3.3523e-03, -1.7250e-03],\n",
      "        [ 1.6796e-03, -1.8521e-04,  1.6915e-03,  ..., -2.4062e-03,\n",
      "          3.6212e-03, -5.3489e-03],\n",
      "        [ 6.0628e-03,  1.7841e-03, -5.0865e-03,  ...,  2.4675e-03,\n",
      "         -5.6857e-03, -2.2529e-03],\n",
      "        ...,\n",
      "        [-2.9976e-03, -1.0113e-03,  6.3409e-03,  ...,  6.4131e-04,\n",
      "          6.1404e-03, -1.4253e-03],\n",
      "        [-1.4256e-03, -1.5077e-03, -2.4617e-04,  ...,  1.6219e-03,\n",
      "          5.3374e-04,  3.8771e-03],\n",
      "        [ 4.1224e-05,  2.7007e-03,  1.2694e-03,  ...,  6.0505e-04,\n",
      "          1.1924e-03, -1.3799e-03]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight \n",
      "value: tensor([[-0.0014, -0.0032,  0.0033,  ..., -0.0002,  0.0034,  0.0053],\n",
      "        [ 0.0008,  0.0002, -0.0053,  ...,  0.0078,  0.0015, -0.0006],\n",
      "        [ 0.0012,  0.0038, -0.0016,  ...,  0.0020,  0.0023, -0.0001],\n",
      "        ...,\n",
      "        [-0.0022, -0.0013,  0.0067,  ...,  0.0057,  0.0028,  0.0030],\n",
      "        [-0.0003, -0.0027, -0.0045,  ..., -0.0037, -0.0040,  0.0027],\n",
      "        [ 0.0011, -0.0022, -0.0064,  ..., -0.0024, -0.0012,  0.0088]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight \n",
      "value: tensor([[-3.3403e-03, -4.8529e-03,  7.9767e-03,  ..., -1.6236e-03,\n",
      "         -5.0757e-03, -1.8496e-05],\n",
      "        [-6.0714e-03,  9.5447e-04, -4.2902e-03,  ..., -9.7847e-03,\n",
      "          9.4856e-03, -3.3834e-05],\n",
      "        [-1.0033e-03, -4.8094e-03, -7.0423e-03,  ...,  4.3273e-03,\n",
      "          7.9508e-04,  1.0051e-02],\n",
      "        ...,\n",
      "        [ 2.5219e-03, -1.9976e-03, -4.8452e-03,  ...,  4.5623e-03,\n",
      "          3.8633e-03,  1.2315e-05],\n",
      "        [-6.3113e-03, -1.3261e-03, -9.7876e-03,  ..., -5.7621e-03,\n",
      "         -3.7873e-03,  2.1104e-03],\n",
      "        [ 3.5212e-03,  3.4515e-03,  4.0598e-03,  ...,  1.1996e-02,\n",
      "         -9.1699e-04,  4.1526e-03]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight \n",
      "value: tensor([[-6.1217e-04, -1.1263e-02, -3.3832e-03,  ..., -2.7299e-03,\n",
      "         -7.4509e-03,  4.6034e-03],\n",
      "        [ 1.9717e-03, -5.6940e-04,  5.6206e-05,  ...,  2.6488e-03,\n",
      "          5.7383e-03,  4.1919e-03],\n",
      "        [ 4.1761e-04,  1.8180e-03,  5.0229e-03,  ...,  3.9864e-03,\n",
      "          3.0261e-03, -2.0296e-04],\n",
      "        ...,\n",
      "        [-6.8800e-04, -1.3476e-03,  8.8384e-03,  ..., -3.2106e-03,\n",
      "         -1.8191e-03,  4.3479e-03],\n",
      "        [-1.9422e-03, -6.2188e-03,  3.7166e-03,  ...,  4.7350e-03,\n",
      "          7.9916e-03, -5.5167e-03],\n",
      "        [-2.0508e-03,  6.2088e-03,  3.7140e-04,  ...,  8.4022e-03,\n",
      "         -1.4558e-03, -4.3424e-03]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight \n",
      "value: tensor([[ 8.9017e-04, -2.0447e-03,  1.4458e-04,  ..., -6.9112e-03,\n",
      "          1.8789e-03,  8.4419e-04],\n",
      "        [-3.8033e-04, -2.1721e-04,  2.1061e-03,  ..., -5.7982e-05,\n",
      "          2.3324e-03, -7.0642e-03],\n",
      "        [ 5.3016e-04, -1.5192e-03, -1.8771e-03,  ..., -9.9352e-04,\n",
      "          2.7831e-03, -4.8538e-03],\n",
      "        ...,\n",
      "        [-2.7243e-03, -5.4477e-05,  1.6127e-03,  ..., -3.4938e-03,\n",
      "          1.0347e-02, -4.8051e-03],\n",
      "        [-2.3109e-03, -6.1080e-03,  7.4773e-03,  ..., -8.3470e-03,\n",
      "         -8.1224e-03, -3.6082e-03],\n",
      "        [-7.7578e-03, -2.9179e-03, -7.6297e-05,  ...,  1.2587e-05,\n",
      "         -1.3066e-03, -4.3594e-04]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight \n",
      "value: tensor([[ 0.0002,  0.0002,  0.0003,  ..., -0.0014,  0.0031, -0.0025],\n",
      "        [ 0.0029, -0.0014, -0.0009,  ..., -0.0050,  0.0038,  0.0002],\n",
      "        [ 0.0043, -0.0017, -0.0006,  ...,  0.0029,  0.0011, -0.0007],\n",
      "        ...,\n",
      "        [ 0.0014,  0.0009,  0.0015,  ...,  0.0083, -0.0029,  0.0024],\n",
      "        [-0.0076, -0.0023,  0.0018,  ...,  0.0031, -0.0023, -0.0046],\n",
      "        [-0.0002,  0.0003, -0.0022,  ..., -0.0008, -0.0060, -0.0017]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight \n",
      "value: tensor([[ 5.3544e-04, -8.7367e-03,  7.9553e-04,  ...,  5.3829e-06,\n",
      "         -5.1105e-03,  5.3385e-03],\n",
      "        [ 2.9867e-03,  1.1823e-03, -8.7936e-03,  ..., -4.9840e-03,\n",
      "         -8.7476e-05, -1.2399e-03],\n",
      "        [-1.6441e-03,  4.7020e-03,  1.9850e-03,  ...,  2.0397e-03,\n",
      "          5.6620e-03, -1.7686e-03],\n",
      "        ...,\n",
      "        [ 2.3576e-03,  2.5877e-03, -2.7738e-03,  ...,  3.2253e-04,\n",
      "          1.8838e-03, -4.8123e-03],\n",
      "        [ 2.3881e-03,  3.0381e-03, -6.7147e-03,  ...,  3.4341e-03,\n",
      "         -2.0857e-03,  1.2668e-03],\n",
      "        [ 3.2760e-03, -1.1222e-03,  5.8116e-03,  ...,  3.4693e-03,\n",
      "          3.2716e-03, -2.2348e-03]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight \n",
      "value: tensor([[ 5.1242e-03, -6.3351e-03, -2.9772e-03,  ...,  4.9727e-03,\n",
      "          3.7603e-03,  4.2931e-03],\n",
      "        [-2.3745e-03,  6.2516e-03,  1.0458e-03,  ..., -5.9598e-04,\n",
      "          2.0834e-03,  1.5903e-02],\n",
      "        [-4.2161e-03, -1.0039e-04,  6.7829e-04,  ...,  1.0660e-02,\n",
      "         -8.7505e-04, -2.4747e-03],\n",
      "        ...,\n",
      "        [ 2.0591e-03, -7.5771e-05, -2.6958e-03,  ...,  2.3496e-03,\n",
      "         -1.5836e-03,  4.4140e-03],\n",
      "        [-2.4277e-03,  5.6055e-03,  9.3829e-03,  ..., -1.6714e-03,\n",
      "         -1.1258e-03, -1.4261e-03],\n",
      "        [-6.0018e-03,  9.6971e-03, -2.7861e-03,  ..., -1.1973e-02,\n",
      "         -3.5946e-03,  2.0173e-03]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight \n",
      "value: tensor([[-0.0068, -0.0086,  0.0016,  ..., -0.0083,  0.0082, -0.0021],\n",
      "        [-0.0076,  0.0084,  0.0090,  ..., -0.0016, -0.0062,  0.0028],\n",
      "        [ 0.0007, -0.0081, -0.0057,  ..., -0.0026, -0.0013, -0.0011],\n",
      "        ...,\n",
      "        [ 0.0139, -0.0068,  0.0041,  ...,  0.0076,  0.0023, -0.0106],\n",
      "        [ 0.0067,  0.0055, -0.0067,  ..., -0.0053,  0.0003, -0.0010],\n",
      "        [ 0.0013, -0.0060, -0.0003,  ...,  0.0026,  0.0036,  0.0066]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight \n",
      "value: tensor([[-0.0019, -0.0153, -0.0040,  ..., -0.0013, -0.0081, -0.0028],\n",
      "        [ 0.0002,  0.0006, -0.0062,  ...,  0.0074, -0.0071,  0.0065],\n",
      "        [ 0.0021, -0.0004, -0.0015,  ..., -0.0055, -0.0052,  0.0046],\n",
      "        ...,\n",
      "        [-0.0034, -0.0044,  0.0052,  ...,  0.0063,  0.0013,  0.0096],\n",
      "        [-0.0090,  0.0007, -0.0044,  ...,  0.0019,  0.0121, -0.0034],\n",
      "        [-0.0021, -0.0092,  0.0036,  ..., -0.0066, -0.0093,  0.0024]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight \n",
      "value: tensor([[-0.0082,  0.0021,  0.0004,  ...,  0.0073,  0.0067,  0.0107],\n",
      "        [-0.0007, -0.0031, -0.0037,  ...,  0.0009, -0.0076,  0.0019],\n",
      "        [-0.0073,  0.0017,  0.0027,  ..., -0.0011, -0.0075, -0.0081],\n",
      "        ...,\n",
      "        [-0.0043, -0.0092,  0.0040,  ...,  0.0031,  0.0139, -0.0036],\n",
      "        [-0.0063,  0.0054,  0.0049,  ..., -0.0164, -0.0103,  0.0044],\n",
      "        [-0.0083, -0.0024, -0.0073,  ..., -0.0038, -0.0055,  0.0027]])\n",
      "\n",
      "key: unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight \n",
      "value: tensor([[ 0.0019,  0.0053, -0.0046,  ...,  0.0003, -0.0073, -0.0035],\n",
      "        [ 0.0042,  0.0002, -0.0067,  ..., -0.0025, -0.0048, -0.0006],\n",
      "        [ 0.0009, -0.0004,  0.0117,  ...,  0.0050,  0.0058,  0.0007],\n",
      "        ...,\n",
      "        [-0.0031,  0.0019, -0.0032,  ...,  0.0094, -0.0015,  0.0069],\n",
      "        [-0.0069,  0.0025,  0.0071,  ...,  0.0021, -0.0105, -0.0122],\n",
      "        [-0.0011,  0.0013,  0.0117,  ...,  0.0065, -0.0029,  0.0095]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.proj_in.lora.down.weight \n",
      "value: tensor([[[[ 2.7886e-03]],\n",
      "\n",
      "         [[-6.5786e-03]],\n",
      "\n",
      "         [[ 9.7672e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.1116e-02]],\n",
      "\n",
      "         [[-1.0217e-02]],\n",
      "\n",
      "         [[-2.7134e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3597e-02]],\n",
      "\n",
      "         [[ 3.3558e-04]],\n",
      "\n",
      "         [[-4.0664e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.6900e-03]],\n",
      "\n",
      "         [[-1.4456e-03]],\n",
      "\n",
      "         [[-9.1853e-04]]],\n",
      "\n",
      "\n",
      "        [[[-7.5169e-03]],\n",
      "\n",
      "         [[ 6.9978e-03]],\n",
      "\n",
      "         [[ 7.9758e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4427e-02]],\n",
      "\n",
      "         [[ 3.3933e-03]],\n",
      "\n",
      "         [[ 3.2500e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.8687e-02]],\n",
      "\n",
      "         [[ 8.7848e-03]],\n",
      "\n",
      "         [[ 1.9769e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3809e-03]],\n",
      "\n",
      "         [[-5.4951e-03]],\n",
      "\n",
      "         [[-1.2499e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 8.2421e-03]],\n",
      "\n",
      "         [[ 6.7305e-03]],\n",
      "\n",
      "         [[ 8.3095e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1776e-02]],\n",
      "\n",
      "         [[ 1.5065e-03]],\n",
      "\n",
      "         [[-6.1022e-04]]],\n",
      "\n",
      "\n",
      "        [[[-3.3665e-03]],\n",
      "\n",
      "         [[-9.3679e-03]],\n",
      "\n",
      "         [[-1.0425e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.2031e-05]],\n",
      "\n",
      "         [[ 3.7703e-03]],\n",
      "\n",
      "         [[ 1.7171e-03]]]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.proj_in.lora.up.weight \n",
      "value: tensor([[[[ 0.0158]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0108]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[ 0.0052]]],\n",
      "\n",
      "\n",
      "        [[[-0.0112]],\n",
      "\n",
      "         [[-0.0103]],\n",
      "\n",
      "         [[ 0.0102]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         [[-0.0152]]],\n",
      "\n",
      "\n",
      "        [[[-0.0031]],\n",
      "\n",
      "         [[-0.0174]],\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0071]],\n",
      "\n",
      "         [[ 0.0145]],\n",
      "\n",
      "         [[ 0.0140]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0085]],\n",
      "\n",
      "         [[ 0.0088]],\n",
      "\n",
      "         [[ 0.0087]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0112]],\n",
      "\n",
      "         [[ 0.0046]],\n",
      "\n",
      "         [[ 0.0134]]],\n",
      "\n",
      "\n",
      "        [[[-0.0034]],\n",
      "\n",
      "         [[ 0.0083]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0132]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[-0.0011]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0162]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0041]],\n",
      "\n",
      "         [[-0.0142]],\n",
      "\n",
      "         [[-0.0039]]]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.proj_out.lora.down.weight \n",
      "value: tensor([[[[-0.0049]],\n",
      "\n",
      "         [[-0.0003]],\n",
      "\n",
      "         [[ 0.0098]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         [[-0.0143]],\n",
      "\n",
      "         [[-0.0007]]],\n",
      "\n",
      "\n",
      "        [[[-0.0082]],\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0125]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0153]]],\n",
      "\n",
      "\n",
      "        [[[-0.0107]],\n",
      "\n",
      "         [[ 0.0111]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0062]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[ 0.0158]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0014]],\n",
      "\n",
      "         [[ 0.0115]],\n",
      "\n",
      "         [[ 0.0107]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0036]],\n",
      "\n",
      "         [[ 0.0042]],\n",
      "\n",
      "         [[ 0.0146]]],\n",
      "\n",
      "\n",
      "        [[[-0.0059]],\n",
      "\n",
      "         [[ 0.0170]],\n",
      "\n",
      "         [[-0.0091]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         [[ 0.0276]],\n",
      "\n",
      "         [[ 0.0038]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0002]],\n",
      "\n",
      "         [[ 0.0143]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0168]],\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[ 0.0028]]]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.proj_out.lora.up.weight \n",
      "value: tensor([[[[ 1.7518e-03]],\n",
      "\n",
      "         [[-8.0690e-03]],\n",
      "\n",
      "         [[ 1.1574e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.9206e-04]],\n",
      "\n",
      "         [[ 5.7612e-03]],\n",
      "\n",
      "         [[ 1.3872e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4803e-03]],\n",
      "\n",
      "         [[-4.8601e-03]],\n",
      "\n",
      "         [[-2.0531e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7107e-03]],\n",
      "\n",
      "         [[ 9.7001e-03]],\n",
      "\n",
      "         [[ 8.0872e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.7444e-04]],\n",
      "\n",
      "         [[-1.0607e-02]],\n",
      "\n",
      "         [[ 8.0374e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.6070e-03]],\n",
      "\n",
      "         [[-4.4673e-03]],\n",
      "\n",
      "         [[-6.1291e-05]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.0121e-03]],\n",
      "\n",
      "         [[ 1.3109e-03]],\n",
      "\n",
      "         [[-1.1982e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.2047e-03]],\n",
      "\n",
      "         [[ 2.0751e-03]],\n",
      "\n",
      "         [[ 5.3558e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 5.1275e-03]],\n",
      "\n",
      "         [[ 1.3702e-03]],\n",
      "\n",
      "         [[ 1.3434e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0621e-03]],\n",
      "\n",
      "         [[-8.7145e-03]],\n",
      "\n",
      "         [[ 1.3648e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4148e-02]],\n",
      "\n",
      "         [[-8.3146e-03]],\n",
      "\n",
      "         [[ 1.2483e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.0503e-03]],\n",
      "\n",
      "         [[-1.5981e-02]],\n",
      "\n",
      "         [[ 2.0799e-03]]]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight \n",
      "value: tensor([[-0.0046, -0.0073,  0.0024,  ..., -0.0088, -0.0003, -0.0027],\n",
      "        [ 0.0055, -0.0011,  0.0034,  ..., -0.0088, -0.0011,  0.0007],\n",
      "        [ 0.0031,  0.0037,  0.0018,  ...,  0.0018,  0.0006, -0.0019],\n",
      "        ...,\n",
      "        [-0.0051, -0.0108, -0.0057,  ...,  0.0040, -0.0026, -0.0007],\n",
      "        [-0.0122,  0.0114, -0.0044,  ..., -0.0009,  0.0007, -0.0111],\n",
      "        [ 0.0116, -0.0001,  0.0076,  ..., -0.0109, -0.0078,  0.0071]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight \n",
      "value: tensor([[ 0.0020,  0.0011,  0.0034,  ..., -0.0056,  0.0081,  0.0051],\n",
      "        [ 0.0026, -0.0052, -0.0067,  ..., -0.0029,  0.0076, -0.0014],\n",
      "        [ 0.0035, -0.0023, -0.0058,  ..., -0.0005, -0.0015,  0.0026],\n",
      "        ...,\n",
      "        [-0.0074, -0.0037, -0.0005,  ..., -0.0004,  0.0001, -0.0022],\n",
      "        [-0.0058, -0.0039, -0.0016,  ...,  0.0060,  0.0079, -0.0043],\n",
      "        [-0.0021, -0.0009,  0.0029,  ..., -0.0006,  0.0056, -0.0015]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight \n",
      "value: tensor([[-0.0079,  0.0069, -0.0114,  ...,  0.0023,  0.0107, -0.0118],\n",
      "        [-0.0082,  0.0075, -0.0026,  ..., -0.0042, -0.0080, -0.0155],\n",
      "        [-0.0072, -0.0015, -0.0122,  ...,  0.0016,  0.0086,  0.0066],\n",
      "        ...,\n",
      "        [-0.0085, -0.0044, -0.0033,  ...,  0.0007,  0.0012, -0.0017],\n",
      "        [ 0.0122, -0.0145, -0.0131,  ...,  0.0045, -0.0150,  0.0175],\n",
      "        [ 0.0018,  0.0005, -0.0009,  ..., -0.0039, -0.0033,  0.0045]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight \n",
      "value: tensor([[-0.0075, -0.0055, -0.0129,  ..., -0.0043,  0.0006,  0.0147],\n",
      "        [-0.0057,  0.0002,  0.0025,  ...,  0.0035,  0.0066,  0.0035],\n",
      "        [-0.0069, -0.0027, -0.0031,  ..., -0.0104, -0.0057, -0.0046],\n",
      "        ...,\n",
      "        [ 0.0133,  0.0092,  0.0215,  ..., -0.0014,  0.0009,  0.0083],\n",
      "        [-0.0014,  0.0002, -0.0112,  ...,  0.0039,  0.0093, -0.0036],\n",
      "        [ 0.0021, -0.0068, -0.0117,  ...,  0.0123, -0.0088,  0.0080]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight \n",
      "value: tensor([[ 3.7582e-03,  5.1229e-04, -1.6422e-03,  ..., -2.9723e-03,\n",
      "         -5.4122e-03,  3.9445e-03],\n",
      "        [-1.8418e-03,  6.5788e-03, -5.3313e-05,  ..., -3.8961e-03,\n",
      "          3.8910e-03,  3.1578e-03],\n",
      "        [-4.0538e-05,  8.5415e-03,  1.3338e-03,  ..., -7.8148e-04,\n",
      "         -7.6297e-04,  4.0326e-04],\n",
      "        ...,\n",
      "        [-2.6772e-03,  3.1088e-03, -6.4412e-03,  ..., -5.6675e-03,\n",
      "          4.8326e-03,  1.7133e-03],\n",
      "        [-7.6011e-03, -8.2516e-03,  5.8688e-03,  ..., -6.9848e-04,\n",
      "         -3.6383e-03,  5.6842e-03],\n",
      "        [-1.1210e-02, -1.4280e-03, -4.1849e-03,  ...,  8.1451e-03,\n",
      "         -2.0610e-03, -2.8210e-03]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight \n",
      "value: tensor([[-0.0001,  0.0040,  0.0017,  ..., -0.0035,  0.0070,  0.0064],\n",
      "        [-0.0077,  0.0057, -0.0084,  ...,  0.0055,  0.0020, -0.0006],\n",
      "        [ 0.0030, -0.0037,  0.0019,  ..., -0.0124, -0.0018, -0.0026],\n",
      "        ...,\n",
      "        [-0.0028, -0.0018, -0.0054,  ..., -0.0013,  0.0023,  0.0001],\n",
      "        [ 0.0024, -0.0021,  0.0043,  ...,  0.0002,  0.0008, -0.0016],\n",
      "        [-0.0046, -0.0024, -0.0005,  ...,  0.0019,  0.0051, -0.0017]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight \n",
      "value: tensor([[-0.0033, -0.0107, -0.0083,  ..., -0.0068, -0.0095, -0.0051],\n",
      "        [ 0.0060,  0.0059,  0.0047,  ...,  0.0069,  0.0057,  0.0018],\n",
      "        [ 0.0065,  0.0090, -0.0025,  ..., -0.0025, -0.0083, -0.0030],\n",
      "        ...,\n",
      "        [-0.0092,  0.0018, -0.0032,  ...,  0.0023, -0.0027, -0.0014],\n",
      "        [ 0.0001,  0.0031, -0.0100,  ..., -0.0084,  0.0114,  0.0142],\n",
      "        [ 0.0130, -0.0048,  0.0036,  ..., -0.0100,  0.0085, -0.0078]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight \n",
      "value: tensor([[ 0.0157, -0.0071,  0.0029,  ..., -0.0046, -0.0029,  0.0119],\n",
      "        [-0.0113,  0.0017, -0.0027,  ..., -0.0142,  0.0010,  0.0018],\n",
      "        [-0.0110, -0.0098,  0.0027,  ..., -0.0061,  0.0003, -0.0095],\n",
      "        ...,\n",
      "        [ 0.0102, -0.0045, -0.0026,  ..., -0.0079,  0.0187, -0.0117],\n",
      "        [-0.0087, -0.0004, -0.0095,  ..., -0.0003,  0.0066, -0.0074],\n",
      "        [-0.0013, -0.0109,  0.0041,  ..., -0.0027, -0.0045,  0.0086]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight \n",
      "value: tensor([[ 5.7361e-03, -5.1649e-03,  1.3327e-02,  ..., -1.3753e-03,\n",
      "          5.1206e-03,  1.6401e-03],\n",
      "        [-1.6524e-03,  1.5817e-03, -5.1595e-04,  ..., -2.2971e-04,\n",
      "          8.5323e-03,  2.8364e-03],\n",
      "        [-3.9345e-04, -3.6895e-03,  1.5433e-03,  ..., -8.7961e-04,\n",
      "         -5.1918e-03, -3.6600e-03],\n",
      "        ...,\n",
      "        [ 7.6282e-03,  3.5234e-04, -2.8887e-04,  ..., -4.8081e-03,\n",
      "         -5.2155e-03,  1.3787e-02],\n",
      "        [ 1.0030e-03, -7.3043e-03, -8.6331e-03,  ..., -4.7807e-03,\n",
      "          2.2190e-03,  6.8080e-03],\n",
      "        [ 4.5405e-06,  1.9854e-03, -7.3904e-03,  ..., -6.4361e-03,\n",
      "          3.1383e-03,  3.6083e-03]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight \n",
      "value: tensor([[-0.0036, -0.0022, -0.0001,  ..., -0.0078, -0.0073,  0.0074],\n",
      "        [-0.0012,  0.0004, -0.0019,  ..., -0.0071,  0.0045,  0.0077],\n",
      "        [ 0.0072, -0.0031, -0.0044,  ..., -0.0004,  0.0036,  0.0008],\n",
      "        ...,\n",
      "        [ 0.0007, -0.0006, -0.0038,  ...,  0.0041, -0.0017, -0.0026],\n",
      "        [ 0.0009,  0.0018,  0.0024,  ..., -0.0064, -0.0025,  0.0003],\n",
      "        [-0.0002, -0.0008, -0.0016,  ...,  0.0015,  0.0005,  0.0015]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight \n",
      "value: tensor([[ 9.5687e-06, -1.3759e-02,  6.4752e-03,  ...,  6.9279e-03,\n",
      "          4.6782e-03,  6.8068e-04],\n",
      "        [ 1.1877e-02, -8.2813e-03,  4.7936e-03,  ..., -4.7783e-03,\n",
      "          1.0000e-02, -1.1255e-04],\n",
      "        [-3.7361e-03,  3.9292e-03,  7.9088e-04,  ...,  2.8568e-03,\n",
      "          4.3946e-03,  1.5228e-03],\n",
      "        ...,\n",
      "        [ 2.7419e-03,  1.0191e-02,  8.9479e-03,  ...,  1.1131e-03,\n",
      "         -2.0498e-03, -6.4021e-03],\n",
      "        [-1.8022e-03,  9.3438e-03,  1.3053e-02,  ...,  6.1715e-03,\n",
      "          4.1392e-03, -1.5307e-03],\n",
      "        [ 3.8001e-04, -4.2668e-03, -6.8762e-03,  ...,  6.0580e-04,\n",
      "          1.3984e-03,  1.7905e-03]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight \n",
      "value: tensor([[ 0.0014,  0.0101, -0.0100,  ..., -0.0123, -0.0049, -0.0041],\n",
      "        [ 0.0009,  0.0148,  0.0027,  ..., -0.0041,  0.0048,  0.0038],\n",
      "        [ 0.0028,  0.0091, -0.0064,  ...,  0.0006, -0.0038, -0.0034],\n",
      "        ...,\n",
      "        [-0.0020,  0.0015,  0.0005,  ..., -0.0097, -0.0009,  0.0048],\n",
      "        [-0.0029,  0.0137, -0.0021,  ..., -0.0089, -0.0083,  0.0067],\n",
      "        [-0.0032,  0.0039,  0.0106,  ..., -0.0039, -0.0069, -0.0081]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight \n",
      "value: tensor([[ 0.0031, -0.0054,  0.0017,  ...,  0.0017, -0.0007,  0.0033],\n",
      "        [ 0.0118, -0.0017, -0.0009,  ...,  0.0024, -0.0047, -0.0022],\n",
      "        [ 0.0091, -0.0090, -0.0021,  ...,  0.0036, -0.0014, -0.0076],\n",
      "        ...,\n",
      "        [ 0.0024,  0.0086,  0.0098,  ...,  0.0040, -0.0014, -0.0017],\n",
      "        [-0.0005, -0.0027,  0.0006,  ..., -0.0039, -0.0055, -0.0053],\n",
      "        [ 0.0056,  0.0070, -0.0051,  ..., -0.0067,  0.0117, -0.0043]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight \n",
      "value: tensor([[ 0.0056,  0.0012, -0.0002,  ...,  0.0027,  0.0004, -0.0061],\n",
      "        [-0.0026,  0.0020, -0.0049,  ..., -0.0056, -0.0029, -0.0013],\n",
      "        [-0.0008, -0.0014,  0.0040,  ...,  0.0041,  0.0022,  0.0012],\n",
      "        ...,\n",
      "        [ 0.0015,  0.0023, -0.0007,  ..., -0.0017,  0.0013, -0.0003],\n",
      "        [ 0.0048, -0.0008,  0.0021,  ..., -0.0013,  0.0083, -0.0019],\n",
      "        [-0.0037,  0.0008, -0.0018,  ...,  0.0023,  0.0031, -0.0003]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight \n",
      "value: tensor([[-3.1688e-04,  2.8238e-03, -1.2178e-04,  ...,  5.3375e-03,\n",
      "          4.6289e-03,  9.5798e-03],\n",
      "        [ 2.2114e-03, -2.8147e-03,  2.4508e-03,  ..., -1.8350e-03,\n",
      "          5.3314e-03,  7.9171e-03],\n",
      "        [-1.3600e-03,  2.6649e-03, -3.1023e-03,  ...,  1.0076e-03,\n",
      "         -8.6698e-04,  4.5521e-03],\n",
      "        ...,\n",
      "        [-2.8509e-03, -5.4287e-03,  7.8968e-03,  ..., -1.9767e-03,\n",
      "         -3.8546e-03, -4.9691e-05],\n",
      "        [-3.1544e-03,  6.1843e-03,  2.9889e-04,  ..., -1.4672e-03,\n",
      "         -3.5280e-03, -6.7425e-03],\n",
      "        [-1.8893e-03,  6.7712e-03,  1.6034e-03,  ..., -5.4119e-04,\n",
      "          3.7746e-04,  2.8319e-03]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight \n",
      "value: tensor([[-3.1835e-03, -1.9873e-03, -6.8200e-03,  ..., -5.6382e-03,\n",
      "          5.8335e-03,  2.6176e-03],\n",
      "        [-4.1883e-03, -2.3373e-03,  1.0247e-03,  ..., -3.1487e-03,\n",
      "         -5.6593e-03, -3.8639e-03],\n",
      "        [ 7.5016e-03,  6.1411e-03, -6.3029e-04,  ...,  7.3772e-03,\n",
      "          3.1075e-03,  1.0719e-02],\n",
      "        ...,\n",
      "        [-9.4156e-03,  6.7632e-05,  5.1466e-03,  ...,  3.9970e-03,\n",
      "          2.0367e-03,  8.8552e-03],\n",
      "        [ 9.0130e-04, -9.1540e-03,  6.0635e-03,  ..., -5.3985e-03,\n",
      "         -4.2026e-03, -6.7829e-03],\n",
      "        [ 4.0828e-03,  1.0522e-02, -3.7746e-03,  ...,  2.0360e-03,\n",
      "          2.0646e-03, -6.9458e-04]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight \n",
      "value: tensor([[-4.4550e-03,  1.3608e-02, -2.8311e-03,  ...,  4.8101e-03,\n",
      "         -1.0152e-02,  6.8249e-03],\n",
      "        [-4.8137e-03, -2.1197e-03,  8.8515e-03,  ...,  3.6159e-04,\n",
      "          3.6885e-03, -5.5350e-03],\n",
      "        [ 1.3995e-02,  3.0999e-03, -2.8331e-03,  ..., -6.8266e-03,\n",
      "         -6.1815e-03,  7.3634e-03],\n",
      "        ...,\n",
      "        [-1.4056e-02, -1.6504e-02, -2.7611e-03,  ..., -2.6012e-03,\n",
      "          8.4777e-03, -2.9069e-03],\n",
      "        [-8.3384e-03, -1.0133e-02,  2.2575e-02,  ..., -6.1246e-03,\n",
      "          2.2476e-02, -7.9306e-05],\n",
      "        [-1.5536e-03,  8.5320e-03, -2.2487e-03,  ..., -3.7083e-03,\n",
      "          3.8366e-03, -1.2005e-02]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight \n",
      "value: tensor([[-0.0030,  0.0078,  0.0005,  ..., -0.0059, -0.0030, -0.0088],\n",
      "        [ 0.0049, -0.0090,  0.0028,  ...,  0.0006, -0.0011,  0.0040],\n",
      "        [-0.0073, -0.0019,  0.0026,  ..., -0.0003,  0.0022,  0.0043],\n",
      "        ...,\n",
      "        [ 0.0060,  0.0028,  0.0063,  ...,  0.0070, -0.0019, -0.0033],\n",
      "        [ 0.0062,  0.0019,  0.0101,  ..., -0.0022,  0.0101,  0.0118],\n",
      "        [-0.0078,  0.0028,  0.0124,  ...,  0.0063,  0.0065, -0.0111]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight \n",
      "value: tensor([[-0.0055, -0.0110,  0.0017,  ..., -0.0070, -0.0037, -0.0081],\n",
      "        [ 0.0056,  0.0078, -0.0101,  ..., -0.0032,  0.0064,  0.0139],\n",
      "        [ 0.0040,  0.0065,  0.0078,  ..., -0.0014, -0.0161, -0.0065],\n",
      "        ...,\n",
      "        [ 0.0093, -0.0083, -0.0042,  ...,  0.0071, -0.0006,  0.0016],\n",
      "        [ 0.0105, -0.0015, -0.0083,  ...,  0.0070,  0.0051,  0.0032],\n",
      "        [-0.0001,  0.0061, -0.0091,  ...,  0.0056, -0.0014, -0.0035]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight \n",
      "value: tensor([[-9.0809e-03, -6.5111e-03,  5.5813e-03,  ...,  1.4200e-02,\n",
      "          2.1882e-02, -1.0686e-02],\n",
      "        [ 1.2105e-02,  1.4423e-02, -1.0041e-03,  ...,  8.3353e-03,\n",
      "         -1.0921e-02, -1.3556e-03],\n",
      "        [ 3.1014e-03, -2.1291e-03,  2.8370e-03,  ...,  1.2179e-03,\n",
      "         -1.1306e-03,  2.4332e-03],\n",
      "        ...,\n",
      "        [-1.3344e-02,  6.6672e-03, -6.3778e-03,  ...,  2.2256e-03,\n",
      "         -1.1604e-02, -3.3271e-03],\n",
      "        [ 1.3781e-02,  1.7001e-05,  3.2796e-03,  ..., -1.2216e-02,\n",
      "         -3.3124e-03, -7.2194e-03],\n",
      "        [ 1.0706e-03,  7.2957e-03, -2.4914e-03,  ...,  1.3710e-03,\n",
      "          8.7425e-03,  8.1316e-04]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.proj_in.lora.down.weight \n",
      "value: tensor([[[[-0.0126]],\n",
      "\n",
      "         [[-0.0043]],\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[ 0.0095]],\n",
      "\n",
      "         [[ 0.0043]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0180]],\n",
      "\n",
      "         [[-0.0106]],\n",
      "\n",
      "         [[ 0.0062]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0055]],\n",
      "\n",
      "         [[-0.0082]],\n",
      "\n",
      "         [[-0.0108]]],\n",
      "\n",
      "\n",
      "        [[[-0.0117]],\n",
      "\n",
      "         [[-0.0065]],\n",
      "\n",
      "         [[ 0.0103]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0123]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[ 0.0061]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0167]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[-0.0059]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0075]],\n",
      "\n",
      "         [[-0.0277]],\n",
      "\n",
      "         [[-0.0057]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0083]],\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[-0.0083]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0063]],\n",
      "\n",
      "         [[ 0.0200]],\n",
      "\n",
      "         [[ 0.0077]]],\n",
      "\n",
      "\n",
      "        [[[-0.0043]],\n",
      "\n",
      "         [[ 0.0007]],\n",
      "\n",
      "         [[ 0.0080]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0064]],\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[ 0.0078]]]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.proj_in.lora.up.weight \n",
      "value: tensor([[[[-0.0046]],\n",
      "\n",
      "         [[-0.0089]],\n",
      "\n",
      "         [[ 0.0072]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0101]],\n",
      "\n",
      "         [[-0.0104]],\n",
      "\n",
      "         [[-0.0025]]],\n",
      "\n",
      "\n",
      "        [[[-0.0171]],\n",
      "\n",
      "         [[-0.0040]],\n",
      "\n",
      "         [[ 0.0041]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0051]],\n",
      "\n",
      "         [[-0.0127]],\n",
      "\n",
      "         [[ 0.0060]]],\n",
      "\n",
      "\n",
      "        [[[-0.0017]],\n",
      "\n",
      "         [[-0.0090]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[-0.0101]],\n",
      "\n",
      "         [[ 0.0012]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0101]],\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         [[-0.0053]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0079]],\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         [[-0.0039]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0134]],\n",
      "\n",
      "         [[-0.0089]],\n",
      "\n",
      "         [[ 0.0101]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0078]],\n",
      "\n",
      "         [[ 0.0039]],\n",
      "\n",
      "         [[ 0.0161]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0095]],\n",
      "\n",
      "         [[-0.0053]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0073]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[-0.0176]]]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.proj_out.lora.down.weight \n",
      "value: tensor([[[[-0.0050]],\n",
      "\n",
      "         [[ 0.0062]],\n",
      "\n",
      "         [[-0.0005]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0040]],\n",
      "\n",
      "         [[ 0.0076]],\n",
      "\n",
      "         [[ 0.0014]]],\n",
      "\n",
      "\n",
      "        [[[-0.0076]],\n",
      "\n",
      "         [[ 0.0085]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0084]],\n",
      "\n",
      "         [[ 0.0016]],\n",
      "\n",
      "         [[ 0.0107]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0135]],\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         [[ 0.0055]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0030]],\n",
      "\n",
      "         [[ 0.0055]],\n",
      "\n",
      "         [[ 0.0012]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0007]],\n",
      "\n",
      "         [[ 0.0039]],\n",
      "\n",
      "         [[ 0.0106]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[ 0.0126]],\n",
      "\n",
      "         [[-0.0039]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0048]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0059]],\n",
      "\n",
      "         [[ 0.0090]],\n",
      "\n",
      "         [[ 0.0153]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0154]],\n",
      "\n",
      "         [[-0.0077]],\n",
      "\n",
      "         [[-0.0089]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0147]],\n",
      "\n",
      "         [[-0.0082]],\n",
      "\n",
      "         [[-0.0155]]]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.proj_out.lora.up.weight \n",
      "value: tensor([[[[ 0.0039]],\n",
      "\n",
      "         [[ 0.0122]],\n",
      "\n",
      "         [[-0.0150]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0033]],\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         [[-0.0007]]],\n",
      "\n",
      "\n",
      "        [[[-0.0139]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[ 0.0079]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0076]],\n",
      "\n",
      "         [[-0.0082]],\n",
      "\n",
      "         [[ 0.0158]]],\n",
      "\n",
      "\n",
      "        [[[-0.0166]],\n",
      "\n",
      "         [[ 0.0113]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0148]],\n",
      "\n",
      "         [[ 0.0048]],\n",
      "\n",
      "         [[ 0.0036]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0073]],\n",
      "\n",
      "         [[ 0.0096]],\n",
      "\n",
      "         [[ 0.0041]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0066]],\n",
      "\n",
      "         [[-0.0038]],\n",
      "\n",
      "         [[-0.0057]]],\n",
      "\n",
      "\n",
      "        [[[-0.0006]],\n",
      "\n",
      "         [[ 0.0183]],\n",
      "\n",
      "         [[-0.0166]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0041]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[ 0.0122]]],\n",
      "\n",
      "\n",
      "        [[[-0.0128]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[-0.0065]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0128]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[-0.0060]]]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight \n",
      "value: tensor([[ 2.1746e-03, -3.0913e-03, -1.5539e-03,  ...,  1.5789e-03,\n",
      "         -5.8387e-03,  1.4323e-02],\n",
      "        [ 1.1777e-02,  3.2980e-03,  5.0002e-03,  ...,  7.4013e-03,\n",
      "          7.7070e-03, -2.9168e-03],\n",
      "        [ 2.7325e-03, -9.4114e-04, -4.7917e-03,  ...,  4.4535e-03,\n",
      "         -6.7361e-03,  4.6210e-04],\n",
      "        ...,\n",
      "        [ 9.0756e-03,  3.9985e-03,  7.5783e-03,  ..., -5.8774e-03,\n",
      "          5.3452e-03,  1.3951e-03],\n",
      "        [-5.8270e-03, -4.2380e-03,  5.8500e-03,  ..., -1.6200e-03,\n",
      "          5.8459e-03, -4.4829e-03],\n",
      "        [-6.4960e-05,  4.0563e-03,  3.2856e-03,  ..., -5.5695e-03,\n",
      "         -5.3245e-03,  2.6691e-03]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight \n",
      "value: tensor([[-0.0067, -0.0025, -0.0005,  ...,  0.0024, -0.0089,  0.0001],\n",
      "        [ 0.0020, -0.0039,  0.0036,  ...,  0.0058, -0.0029,  0.0016],\n",
      "        [-0.0033, -0.0049,  0.0017,  ..., -0.0044, -0.0004,  0.0052],\n",
      "        ...,\n",
      "        [-0.0049,  0.0030, -0.0009,  ..., -0.0009,  0.0101,  0.0005],\n",
      "        [-0.0006, -0.0037, -0.0039,  ..., -0.0028,  0.0017,  0.0051],\n",
      "        [-0.0062, -0.0038, -0.0010,  ..., -0.0091,  0.0086, -0.0046]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight \n",
      "value: tensor([[ 0.0057,  0.0096,  0.0074,  ..., -0.0007, -0.0069,  0.0130],\n",
      "        [-0.0013,  0.0034,  0.0025,  ..., -0.0048, -0.0039,  0.0001],\n",
      "        [ 0.0114, -0.0102,  0.0017,  ..., -0.0081,  0.0087,  0.0026],\n",
      "        ...,\n",
      "        [-0.0004,  0.0070, -0.0098,  ...,  0.0100, -0.0005,  0.0043],\n",
      "        [-0.0007,  0.0063, -0.0095,  ...,  0.0095,  0.0034, -0.0065],\n",
      "        [-0.0117,  0.0008, -0.0055,  ..., -0.0082,  0.0013,  0.0070]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight \n",
      "value: tensor([[-0.0036, -0.0163,  0.0053,  ..., -0.0038, -0.0002, -0.0050],\n",
      "        [-0.0059, -0.0048,  0.0110,  ...,  0.0064,  0.0013, -0.0137],\n",
      "        [-0.0035, -0.0051,  0.0049,  ..., -0.0049, -0.0055, -0.0126],\n",
      "        ...,\n",
      "        [-0.0071, -0.0096,  0.0014,  ..., -0.0128, -0.0212,  0.0129],\n",
      "        [ 0.0043,  0.0086,  0.0015,  ..., -0.0188, -0.0036,  0.0169],\n",
      "        [ 0.0017, -0.0002, -0.0061,  ..., -0.0150, -0.0054,  0.0032]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight \n",
      "value: tensor([[ 0.0027, -0.0045, -0.0004,  ..., -0.0051, -0.0053, -0.0005],\n",
      "        [-0.0013,  0.0002,  0.0024,  ..., -0.0036, -0.0078, -0.0082],\n",
      "        [-0.0102,  0.0051, -0.0003,  ..., -0.0049, -0.0010,  0.0085],\n",
      "        ...,\n",
      "        [-0.0015,  0.0006,  0.0005,  ..., -0.0020,  0.0067,  0.0058],\n",
      "        [ 0.0034,  0.0118, -0.0004,  ...,  0.0005,  0.0022,  0.0027],\n",
      "        [ 0.0037, -0.0011, -0.0008,  ..., -0.0012, -0.0055, -0.0131]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight \n",
      "value: tensor([[-3.1274e-03,  2.8085e-03,  1.6547e-04,  ...,  1.3020e-03,\n",
      "          1.7317e-03,  5.0873e-03],\n",
      "        [ 3.2448e-03,  1.1558e-03,  3.5953e-03,  ..., -1.2909e-03,\n",
      "          6.8744e-03, -4.6539e-05],\n",
      "        [ 4.7714e-03, -2.0803e-03,  3.4980e-03,  ...,  6.0124e-03,\n",
      "         -3.5742e-03,  2.1516e-03],\n",
      "        ...,\n",
      "        [-2.8466e-05, -3.5691e-03, -4.5503e-03,  ...,  3.7107e-03,\n",
      "          6.1915e-03,  4.0785e-04],\n",
      "        [-7.3194e-03, -2.5904e-03, -1.0241e-03,  ...,  1.6339e-03,\n",
      "          7.8106e-03, -4.1369e-03],\n",
      "        [ 7.0447e-03,  1.5947e-03, -4.4243e-03,  ..., -5.6199e-04,\n",
      "         -1.2052e-02, -1.5425e-03]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight \n",
      "value: tensor([[-0.0175, -0.0093, -0.0064,  ..., -0.0047,  0.0049,  0.0020],\n",
      "        [-0.0011,  0.0106, -0.0045,  ..., -0.0043,  0.0041, -0.0106],\n",
      "        [-0.0040, -0.0032,  0.0018,  ...,  0.0093, -0.0041, -0.0070],\n",
      "        ...,\n",
      "        [-0.0032,  0.0078, -0.0058,  ...,  0.0039,  0.0009,  0.0002],\n",
      "        [ 0.0024, -0.0015,  0.0018,  ...,  0.0078,  0.0196,  0.0054],\n",
      "        [-0.0080, -0.0058, -0.0078,  ...,  0.0067, -0.0079, -0.0029]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight \n",
      "value: tensor([[ 0.0078, -0.0093, -0.0015,  ..., -0.0033,  0.0037, -0.0034],\n",
      "        [-0.0023,  0.0110, -0.0158,  ...,  0.0061,  0.0015, -0.0061],\n",
      "        [-0.0033, -0.0047,  0.0069,  ..., -0.0067,  0.0011, -0.0034],\n",
      "        ...,\n",
      "        [ 0.0112, -0.0002, -0.0031,  ...,  0.0138, -0.0150, -0.0039],\n",
      "        [ 0.0009,  0.0045, -0.0084,  ...,  0.0040,  0.0149,  0.0016],\n",
      "        [ 0.0109,  0.0050, -0.0078,  ...,  0.0001, -0.0028, -0.0015]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight \n",
      "value: tensor([[ 0.0042, -0.0018,  0.0038,  ..., -0.0046, -0.0013,  0.0095],\n",
      "        [ 0.0051, -0.0006, -0.0045,  ..., -0.0075, -0.0056,  0.0125],\n",
      "        [-0.0032, -0.0070, -0.0010,  ..., -0.0109,  0.0018,  0.0069],\n",
      "        ...,\n",
      "        [-0.0047, -0.0099, -0.0006,  ..., -0.0039,  0.0009, -0.0050],\n",
      "        [ 0.0009,  0.0038, -0.0027,  ..., -0.0025,  0.0033, -0.0066],\n",
      "        [-0.0044,  0.0073,  0.0035,  ...,  0.0037, -0.0087, -0.0013]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight \n",
      "value: tensor([[ 0.0164,  0.0014,  0.0175,  ...,  0.0075, -0.0013,  0.0014],\n",
      "        [ 0.0058, -0.0048, -0.0131,  ..., -0.0104, -0.0015,  0.0038],\n",
      "        [-0.0070,  0.0042, -0.0040,  ...,  0.0054, -0.0025, -0.0101],\n",
      "        ...,\n",
      "        [-0.0019, -0.0088,  0.0078,  ..., -0.0043, -0.0047, -0.0030],\n",
      "        [-0.0025,  0.0042,  0.0058,  ...,  0.0008,  0.0042,  0.0049],\n",
      "        [ 0.0057,  0.0130,  0.0093,  ..., -0.0019,  0.0085,  0.0059]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight \n",
      "value: tensor([[-6.0722e-03,  1.1067e-02, -1.0136e-02,  ...,  1.3105e-03,\n",
      "          8.2074e-03,  1.2683e-03],\n",
      "        [-5.2051e-03, -1.4241e-02,  5.0659e-03,  ..., -8.4731e-03,\n",
      "         -2.6723e-03,  3.3155e-03],\n",
      "        [-4.2682e-04,  3.2699e-03,  2.2753e-03,  ..., -1.4873e-03,\n",
      "          6.9709e-04, -8.8300e-03],\n",
      "        ...,\n",
      "        [-4.9302e-03, -1.0188e-02,  7.2368e-03,  ..., -7.5632e-03,\n",
      "          8.3992e-04, -2.3173e-03],\n",
      "        [-3.7368e-04,  5.7501e-03, -8.9402e-05,  ...,  1.0975e-02,\n",
      "         -3.9187e-03,  2.1647e-03],\n",
      "        [-9.2983e-03, -2.7144e-03,  8.8567e-03,  ..., -6.3318e-03,\n",
      "         -8.0469e-03, -1.1159e-03]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight \n",
      "value: tensor([[-0.0178, -0.0008, -0.0015,  ..., -0.0072, -0.0001,  0.0003],\n",
      "        [-0.0075, -0.0058,  0.0060,  ...,  0.0028,  0.0003, -0.0117],\n",
      "        [-0.0061,  0.0006, -0.0046,  ...,  0.0083,  0.0061, -0.0050],\n",
      "        ...,\n",
      "        [-0.0054, -0.0040, -0.0130,  ..., -0.0114, -0.0015, -0.0040],\n",
      "        [-0.0022, -0.0049, -0.0003,  ..., -0.0018,  0.0111, -0.0024],\n",
      "        [-0.0018,  0.0027, -0.0107,  ..., -0.0050, -0.0015,  0.0011]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight \n",
      "value: tensor([[-6.4629e-03, -5.2800e-04,  6.4259e-03,  ..., -1.0782e-03,\n",
      "         -5.9651e-03,  1.4803e-03],\n",
      "        [-1.0924e-03, -7.4984e-03,  6.0726e-03,  ..., -3.3531e-03,\n",
      "          7.9490e-03,  9.5586e-05],\n",
      "        [ 1.9988e-04,  5.0192e-03,  6.9818e-03,  ..., -3.2264e-03,\n",
      "         -4.9533e-03, -1.2611e-02],\n",
      "        ...,\n",
      "        [ 5.3233e-03, -4.6855e-03, -5.8766e-03,  ...,  2.0634e-03,\n",
      "         -1.2288e-03, -1.1064e-03],\n",
      "        [-3.6952e-03,  4.2378e-03,  2.5583e-03,  ..., -7.8105e-04,\n",
      "         -2.7054e-03, -2.2541e-04],\n",
      "        [ 3.1047e-03,  5.7431e-03, -2.4623e-03,  ...,  9.9305e-04,\n",
      "         -1.5232e-03,  2.6138e-03]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight \n",
      "value: tensor([[ 0.0033,  0.0002,  0.0026,  ..., -0.0048,  0.0017,  0.0006],\n",
      "        [ 0.0130, -0.0053, -0.0055,  ...,  0.0038,  0.0047, -0.0030],\n",
      "        [ 0.0041,  0.0030, -0.0062,  ..., -0.0108, -0.0013,  0.0025],\n",
      "        ...,\n",
      "        [ 0.0088, -0.0017,  0.0047,  ..., -0.0017,  0.0072,  0.0010],\n",
      "        [-0.0058,  0.0026,  0.0005,  ...,  0.0012,  0.0050,  0.0026],\n",
      "        [ 0.0044,  0.0008, -0.0030,  ...,  0.0030, -0.0014, -0.0023]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight \n",
      "value: tensor([[-5.8246e-03,  3.4772e-03,  3.4602e-03,  ...,  7.0674e-03,\n",
      "          3.9526e-03, -8.1969e-03],\n",
      "        [ 5.4469e-03,  6.8608e-03, -7.3193e-03,  ...,  3.9514e-03,\n",
      "          8.3724e-05,  5.5112e-03],\n",
      "        [-1.4991e-02, -1.2006e-02, -8.9157e-03,  ...,  1.8632e-03,\n",
      "         -8.8879e-04, -2.3751e-03],\n",
      "        ...,\n",
      "        [-2.7091e-03, -1.7876e-03, -1.2265e-02,  ...,  6.5672e-04,\n",
      "         -5.3119e-03,  4.1205e-03],\n",
      "        [-4.6801e-03, -1.3870e-03,  2.5201e-03,  ..., -3.9784e-03,\n",
      "          1.1473e-03, -6.1505e-03],\n",
      "        [ 3.2598e-03, -2.2441e-03, -5.9925e-03,  ..., -1.9785e-03,\n",
      "         -3.4352e-03,  3.4911e-03]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight \n",
      "value: tensor([[ 0.0031, -0.0043,  0.0072,  ...,  0.0041,  0.0026, -0.0034],\n",
      "        [-0.0010, -0.0201,  0.0162,  ..., -0.0166,  0.0036,  0.0023],\n",
      "        [ 0.0068,  0.0047,  0.0141,  ...,  0.0098, -0.0080, -0.0073],\n",
      "        ...,\n",
      "        [-0.0019,  0.0022,  0.0050,  ..., -0.0077, -0.0044, -0.0038],\n",
      "        [ 0.0067,  0.0141, -0.0102,  ...,  0.0108,  0.0030,  0.0045],\n",
      "        [-0.0038, -0.0063,  0.0013,  ..., -0.0045,  0.0085, -0.0046]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight \n",
      "value: tensor([[-0.0055,  0.0003, -0.0093,  ..., -0.0086, -0.0001, -0.0007],\n",
      "        [-0.0025, -0.0098, -0.0069,  ...,  0.0053, -0.0035, -0.0096],\n",
      "        [ 0.0095,  0.0008,  0.0029,  ..., -0.0041,  0.0019, -0.0101],\n",
      "        ...,\n",
      "        [-0.0002, -0.0078,  0.0008,  ...,  0.0007,  0.0071,  0.0010],\n",
      "        [-0.0018, -0.0054, -0.0039,  ..., -0.0003, -0.0175,  0.0009],\n",
      "        [ 0.0096,  0.0055,  0.0112,  ..., -0.0005,  0.0084, -0.0086]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight \n",
      "value: tensor([[-0.0087, -0.0011, -0.0141,  ...,  0.0013,  0.0051,  0.0052],\n",
      "        [ 0.0063, -0.0007, -0.0027,  ...,  0.0029, -0.0037,  0.0084],\n",
      "        [ 0.0072,  0.0012, -0.0037,  ...,  0.0106, -0.0037,  0.0016],\n",
      "        ...,\n",
      "        [ 0.0015, -0.0001,  0.0018,  ...,  0.0011,  0.0093,  0.0032],\n",
      "        [-0.0071,  0.0066, -0.0052,  ...,  0.0040, -0.0058, -0.0011],\n",
      "        [-0.0091, -0.0021, -0.0041,  ...,  0.0118, -0.0072, -0.0006]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight \n",
      "value: tensor([[-0.0013,  0.0049,  0.0206,  ..., -0.0025,  0.0027, -0.0056],\n",
      "        [-0.0025,  0.0018, -0.0022,  ...,  0.0046, -0.0099, -0.0014],\n",
      "        [-0.0024, -0.0014,  0.0025,  ...,  0.0011,  0.0007, -0.0040],\n",
      "        ...,\n",
      "        [-0.0001,  0.0010,  0.0027,  ..., -0.0051, -0.0055, -0.0103],\n",
      "        [-0.0066, -0.0083,  0.0022,  ...,  0.0085,  0.0071,  0.0146],\n",
      "        [ 0.0020,  0.0083,  0.0044,  ..., -0.0068,  0.0017,  0.0024]])\n",
      "\n",
      "key: unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight \n",
      "value: tensor([[-0.0149,  0.0005,  0.0099,  ...,  0.0081, -0.0019, -0.0060],\n",
      "        [ 0.0073, -0.0032, -0.0148,  ...,  0.0022, -0.0021,  0.0026],\n",
      "        [-0.0070,  0.0100,  0.0068,  ...,  0.0121, -0.0008,  0.0166],\n",
      "        ...,\n",
      "        [ 0.0020,  0.0057,  0.0131,  ..., -0.0019, -0.0075, -0.0076],\n",
      "        [-0.0043, -0.0175,  0.0044,  ...,  0.0005,  0.0032,  0.0012],\n",
      "        [-0.0047,  0.0002, -0.0029,  ..., -0.0040, -0.0003,  0.0016]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.proj_in.lora.down.weight \n",
      "value: tensor([[[[-0.0058]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         [[ 0.0106]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0124]],\n",
      "\n",
      "         [[ 0.0100]],\n",
      "\n",
      "         [[-0.0002]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0047]],\n",
      "\n",
      "         [[-0.0054]],\n",
      "\n",
      "         [[ 0.0136]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         [[ 0.0027]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0047]],\n",
      "\n",
      "         [[ 0.0078]],\n",
      "\n",
      "         [[ 0.0034]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0051]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[ 0.0021]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0014]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0063]],\n",
      "\n",
      "         [[ 0.0023]],\n",
      "\n",
      "         [[-0.0018]]],\n",
      "\n",
      "\n",
      "        [[[-0.0046]],\n",
      "\n",
      "         [[ 0.0135]],\n",
      "\n",
      "         [[ 0.0139]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[-0.0089]]],\n",
      "\n",
      "\n",
      "        [[[-0.0131]],\n",
      "\n",
      "         [[-0.0104]],\n",
      "\n",
      "         [[ 0.0192]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0074]],\n",
      "\n",
      "         [[-0.0050]],\n",
      "\n",
      "         [[-0.0039]]]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.proj_in.lora.up.weight \n",
      "value: tensor([[[[-8.9024e-03]],\n",
      "\n",
      "         [[ 6.6872e-03]],\n",
      "\n",
      "         [[ 1.0494e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8775e-03]],\n",
      "\n",
      "         [[ 3.1316e-03]],\n",
      "\n",
      "         [[ 1.3683e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1135e-02]],\n",
      "\n",
      "         [[ 1.7600e-05]],\n",
      "\n",
      "         [[-2.6806e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.8389e-03]],\n",
      "\n",
      "         [[ 4.0512e-03]],\n",
      "\n",
      "         [[ 7.7188e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4264e-03]],\n",
      "\n",
      "         [[ 3.2815e-03]],\n",
      "\n",
      "         [[-7.9349e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-8.1095e-03]],\n",
      "\n",
      "         [[ 9.6264e-03]],\n",
      "\n",
      "         [[ 4.5751e-05]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 4.6238e-03]],\n",
      "\n",
      "         [[ 1.0587e-03]],\n",
      "\n",
      "         [[-2.4564e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.0294e-03]],\n",
      "\n",
      "         [[ 3.9675e-03]],\n",
      "\n",
      "         [[-7.1612e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.5863e-03]],\n",
      "\n",
      "         [[-1.7447e-02]],\n",
      "\n",
      "         [[ 1.3690e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.5607e-03]],\n",
      "\n",
      "         [[ 2.6685e-03]],\n",
      "\n",
      "         [[ 9.0411e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 5.1457e-03]],\n",
      "\n",
      "         [[-5.2097e-04]],\n",
      "\n",
      "         [[-1.0486e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.6321e-03]],\n",
      "\n",
      "         [[ 1.9234e-03]],\n",
      "\n",
      "         [[ 1.6641e-03]]]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.proj_out.lora.down.weight \n",
      "value: tensor([[[[ 0.0033]],\n",
      "\n",
      "         [[-0.0051]],\n",
      "\n",
      "         [[-0.0189]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0154]],\n",
      "\n",
      "         [[-0.0108]],\n",
      "\n",
      "         [[-0.0066]]],\n",
      "\n",
      "\n",
      "        [[[-0.0016]],\n",
      "\n",
      "         [[-0.0102]],\n",
      "\n",
      "         [[ 0.0104]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0036]],\n",
      "\n",
      "         [[ 0.0072]],\n",
      "\n",
      "         [[-0.0058]]],\n",
      "\n",
      "\n",
      "        [[[-0.0058]],\n",
      "\n",
      "         [[ 0.0147]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0092]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         [[ 0.0117]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0120]],\n",
      "\n",
      "         [[ 0.0058]],\n",
      "\n",
      "         [[ 0.0058]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         [[ 0.0046]],\n",
      "\n",
      "         [[ 0.0174]]],\n",
      "\n",
      "\n",
      "        [[[-0.0139]],\n",
      "\n",
      "         [[ 0.0185]],\n",
      "\n",
      "         [[ 0.0105]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[ 0.0052]],\n",
      "\n",
      "         [[ 0.0022]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0052]],\n",
      "\n",
      "         [[ 0.0072]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[-0.0017]]]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.proj_out.lora.up.weight \n",
      "value: tensor([[[[ 0.0011]],\n",
      "\n",
      "         [[-0.0063]],\n",
      "\n",
      "         [[-0.0110]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0034]],\n",
      "\n",
      "         [[-0.0082]],\n",
      "\n",
      "         [[-0.0014]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0072]],\n",
      "\n",
      "         [[-0.0027]],\n",
      "\n",
      "         [[-0.0075]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0233]],\n",
      "\n",
      "         [[ 0.0048]],\n",
      "\n",
      "         [[ 0.0048]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0047]],\n",
      "\n",
      "         [[-0.0036]],\n",
      "\n",
      "         [[ 0.0103]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0088]],\n",
      "\n",
      "         [[-0.0052]],\n",
      "\n",
      "         [[-0.0004]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0035]],\n",
      "\n",
      "         [[ 0.0105]],\n",
      "\n",
      "         [[-0.0056]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0059]],\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[ 0.0076]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0103]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         [[ 0.0158]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         [[ 0.0141]],\n",
      "\n",
      "         [[-0.0049]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0147]],\n",
      "\n",
      "         [[-0.0035]],\n",
      "\n",
      "         [[-0.0161]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0037]],\n",
      "\n",
      "         [[-0.0093]],\n",
      "\n",
      "         [[ 0.0010]]]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight \n",
      "value: tensor([[-0.0040, -0.0041,  0.0022,  ..., -0.0016,  0.0001,  0.0017],\n",
      "        [ 0.0050, -0.0014,  0.0048,  ...,  0.0065, -0.0038, -0.0021],\n",
      "        [-0.0008,  0.0035, -0.0037,  ..., -0.0005, -0.0006, -0.0024],\n",
      "        ...,\n",
      "        [-0.0001,  0.0049, -0.0039,  ..., -0.0034,  0.0008, -0.0008],\n",
      "        [ 0.0018,  0.0053, -0.0007,  ...,  0.0022, -0.0025, -0.0077],\n",
      "        [ 0.0011, -0.0040, -0.0022,  ...,  0.0010,  0.0004, -0.0041]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight \n",
      "value: tensor([[-6.1044e-04,  1.0133e-04, -1.1609e-03,  ..., -2.8624e-03,\n",
      "         -8.9254e-03, -7.9277e-04],\n",
      "        [-2.4698e-04, -2.5584e-04,  1.6380e-05,  ..., -1.1304e-03,\n",
      "          2.1681e-03, -6.2376e-04],\n",
      "        [-1.6036e-03,  1.2991e-03,  3.8069e-04,  ...,  2.2946e-03,\n",
      "          1.9945e-03,  6.7680e-04],\n",
      "        ...,\n",
      "        [-2.7438e-05, -1.1208e-03, -2.4730e-04,  ...,  2.6134e-03,\n",
      "          2.7318e-03, -2.2346e-03],\n",
      "        [ 2.1585e-03,  1.8580e-04, -1.6350e-03,  ..., -3.4895e-04,\n",
      "          5.7024e-03, -3.5744e-04],\n",
      "        [ 9.1397e-05,  1.2437e-03,  6.7249e-05,  ..., -1.4965e-03,\n",
      "         -7.9762e-04, -1.8115e-04]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight \n",
      "value: tensor([[ 0.0036, -0.0058, -0.0013,  ..., -0.0006,  0.0064, -0.0013],\n",
      "        [-0.0094, -0.0016,  0.0017,  ..., -0.0039,  0.0065,  0.0028],\n",
      "        [-0.0141, -0.0029,  0.0046,  ...,  0.0025,  0.0093,  0.0021],\n",
      "        ...,\n",
      "        [-0.0045, -0.0131,  0.0069,  ...,  0.0029, -0.0016,  0.0031],\n",
      "        [-0.0011,  0.0043,  0.0037,  ..., -0.0002,  0.0014,  0.0036],\n",
      "        [ 0.0095, -0.0110, -0.0014,  ...,  0.0061,  0.0133, -0.0023]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight \n",
      "value: tensor([[ 0.0058, -0.0114,  0.0046,  ..., -0.0038,  0.0006, -0.0071],\n",
      "        [-0.0032,  0.0105, -0.0029,  ...,  0.0053, -0.0062, -0.0028],\n",
      "        [-0.0027,  0.0047,  0.0005,  ...,  0.0031, -0.0030,  0.0038],\n",
      "        ...,\n",
      "        [ 0.0135, -0.0030, -0.0090,  ..., -0.0020,  0.0035, -0.0026],\n",
      "        [ 0.0003, -0.0001,  0.0010,  ...,  0.0145,  0.0010, -0.0045],\n",
      "        [ 0.0009, -0.0006, -0.0052,  ...,  0.0039,  0.0093, -0.0067]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight \n",
      "value: tensor([[-1.2361e-03,  2.8111e-03, -2.6277e-03,  ...,  3.3337e-03,\n",
      "         -7.1799e-03,  2.4202e-03],\n",
      "        [-1.9921e-04,  1.2103e-03,  3.4659e-03,  ..., -1.1278e-03,\n",
      "          4.1171e-03, -8.6935e-05],\n",
      "        [ 4.7645e-03, -1.8969e-04, -4.8548e-03,  ..., -1.1230e-03,\n",
      "          2.0906e-03, -3.7052e-03],\n",
      "        ...,\n",
      "        [ 5.3725e-03, -1.7534e-03,  2.2494e-03,  ..., -5.6495e-03,\n",
      "          5.7170e-04, -1.9269e-03],\n",
      "        [ 2.2751e-03, -7.9119e-04,  1.0794e-02,  ..., -6.4978e-03,\n",
      "          6.8874e-03,  5.3915e-03],\n",
      "        [-3.2280e-04,  4.6359e-04,  1.1452e-03,  ...,  2.0925e-05,\n",
      "         -1.0515e-04, -2.1927e-03]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight \n",
      "value: tensor([[-1.3929e-03,  2.5185e-04, -2.0897e-03,  ..., -1.3215e-03,\n",
      "          4.2353e-04, -8.6409e-04],\n",
      "        [-1.9621e-03,  2.1950e-03, -2.9684e-03,  ..., -3.8155e-04,\n",
      "         -3.6809e-04,  5.9325e-05],\n",
      "        [-6.9395e-04,  1.7823e-03,  1.7594e-03,  ...,  4.0193e-04,\n",
      "         -4.8606e-03,  1.7018e-03],\n",
      "        ...,\n",
      "        [ 1.2352e-03,  8.0010e-05, -2.7482e-04,  ..., -5.1006e-04,\n",
      "         -4.2161e-04, -4.8789e-04],\n",
      "        [ 2.5081e-03,  1.8402e-03, -1.3878e-03,  ..., -1.4500e-03,\n",
      "          2.3844e-03,  1.4453e-03],\n",
      "        [ 1.7856e-03, -1.0070e-04,  3.3630e-03,  ...,  1.2623e-03,\n",
      "          3.6883e-03,  1.7940e-04]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight \n",
      "value: tensor([[ 6.0572e-03,  3.2295e-03, -7.1446e-03,  ..., -1.3641e-02,\n",
      "          8.3154e-03, -5.8455e-03],\n",
      "        [ 8.9600e-03,  6.5912e-03, -2.4825e-03,  ..., -1.9314e-03,\n",
      "          3.6844e-03, -8.6602e-04],\n",
      "        [-5.3648e-03, -4.9912e-03,  1.4928e-03,  ..., -8.3638e-03,\n",
      "          2.3998e-03, -6.0604e-03],\n",
      "        ...,\n",
      "        [-5.9374e-03, -6.7122e-03, -8.9516e-03,  ...,  7.3622e-04,\n",
      "          3.3127e-03, -4.2005e-04],\n",
      "        [-4.2814e-03, -9.6626e-03, -9.0239e-03,  ...,  1.2397e-02,\n",
      "          1.6829e-03,  9.5669e-05],\n",
      "        [ 5.2294e-03, -4.8643e-04,  8.0121e-03,  ..., -3.3860e-03,\n",
      "          2.4604e-03, -3.7605e-03]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight \n",
      "value: tensor([[-0.0049, -0.0018,  0.0090,  ..., -0.0005, -0.0045,  0.0035],\n",
      "        [-0.0010,  0.0011, -0.0060,  ..., -0.0064, -0.0018,  0.0013],\n",
      "        [ 0.0006,  0.0022, -0.0051,  ..., -0.0018, -0.0054,  0.0026],\n",
      "        ...,\n",
      "        [ 0.0008, -0.0034, -0.0059,  ..., -0.0048, -0.0063,  0.0065],\n",
      "        [-0.0004,  0.0063,  0.0032,  ..., -0.0105,  0.0043,  0.0115],\n",
      "        [-0.0038,  0.0083,  0.0118,  ...,  0.0039,  0.0053, -0.0021]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight \n",
      "value: tensor([[-0.0018, -0.0052, -0.0029,  ...,  0.0011,  0.0066,  0.0015],\n",
      "        [ 0.0031, -0.0045,  0.0046,  ..., -0.0028, -0.0037, -0.0036],\n",
      "        [-0.0084,  0.0012, -0.0007,  ...,  0.0004,  0.0052,  0.0007],\n",
      "        ...,\n",
      "        [-0.0004,  0.0074,  0.0027,  ..., -0.0036,  0.0079,  0.0079],\n",
      "        [-0.0017,  0.0032, -0.0004,  ..., -0.0150, -0.0032,  0.0015],\n",
      "        [-0.0050,  0.0039, -0.0073,  ...,  0.0047,  0.0036,  0.0028]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight \n",
      "value: tensor([[ 6.0931e-03,  7.4842e-04,  5.7825e-03,  ..., -2.2145e-03,\n",
      "         -7.2144e-03, -5.2707e-03],\n",
      "        [-1.5265e-03,  3.9843e-03,  4.7792e-03,  ...,  5.3051e-03,\n",
      "          1.8440e-03, -3.1283e-03],\n",
      "        [ 2.8721e-05, -3.7147e-04, -3.3152e-03,  ..., -8.0260e-03,\n",
      "         -1.6502e-03,  4.1924e-03],\n",
      "        ...,\n",
      "        [-3.9359e-03, -2.4793e-03, -3.1997e-03,  ..., -5.7245e-04,\n",
      "          2.1263e-03,  1.3856e-03],\n",
      "        [-7.3962e-03, -2.2483e-03,  6.1942e-03,  ...,  3.4271e-03,\n",
      "         -4.6234e-03, -1.3703e-03],\n",
      "        [ 5.1001e-03, -1.1903e-03,  5.1867e-03,  ...,  2.5306e-03,\n",
      "         -2.8806e-03,  2.0476e-03]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight \n",
      "value: tensor([[-0.0011,  0.0016, -0.0031,  ...,  0.0027, -0.0014,  0.0005],\n",
      "        [-0.0103,  0.0073, -0.0015,  ...,  0.0044,  0.0081,  0.0021],\n",
      "        [ 0.0058, -0.0053,  0.0114,  ..., -0.0025,  0.0151,  0.0014],\n",
      "        ...,\n",
      "        [-0.0020,  0.0138,  0.0007,  ...,  0.0054, -0.0083,  0.0055],\n",
      "        [-0.0115, -0.0077, -0.0128,  ...,  0.0028,  0.0027, -0.0026],\n",
      "        [-0.0146, -0.0018,  0.0050,  ..., -0.0085,  0.0031,  0.0134]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight \n",
      "value: tensor([[-8.0207e-03,  8.1669e-03, -4.5865e-03,  ...,  3.2856e-03,\n",
      "          8.3568e-05, -5.0045e-03],\n",
      "        [-3.4536e-03,  1.7035e-03,  8.6462e-03,  ...,  5.3860e-03,\n",
      "          1.0488e-02, -3.5370e-03],\n",
      "        [-3.6435e-04, -5.3456e-04,  8.2383e-03,  ...,  4.6588e-03,\n",
      "          5.4754e-03,  5.0106e-03],\n",
      "        ...,\n",
      "        [-5.1346e-05,  1.3723e-03, -1.8591e-03,  ...,  1.4558e-03,\n",
      "         -1.0607e-03, -3.4297e-03],\n",
      "        [ 3.1557e-03,  1.6444e-02, -1.6590e-02,  ...,  2.4075e-03,\n",
      "          5.4311e-03,  1.8708e-03],\n",
      "        [-4.9509e-03, -1.6695e-02,  7.4616e-04,  ...,  2.3942e-03,\n",
      "         -5.3638e-04, -7.9061e-03]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight \n",
      "value: tensor([[ 0.0022, -0.0006, -0.0009,  ...,  0.0033, -0.0026, -0.0045],\n",
      "        [ 0.0023, -0.0035,  0.0029,  ...,  0.0009, -0.0051,  0.0012],\n",
      "        [ 0.0036, -0.0029,  0.0032,  ...,  0.0049, -0.0014, -0.0002],\n",
      "        ...,\n",
      "        [ 0.0013, -0.0021,  0.0026,  ...,  0.0027,  0.0063,  0.0007],\n",
      "        [ 0.0039, -0.0087,  0.0040,  ...,  0.0038, -0.0051,  0.0035],\n",
      "        [ 0.0041,  0.0034, -0.0039,  ...,  0.0033, -0.0023, -0.0004]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight \n",
      "value: tensor([[-0.0047,  0.0006,  0.0060,  ..., -0.0027,  0.0063,  0.0005],\n",
      "        [ 0.0010, -0.0026, -0.0003,  ...,  0.0034,  0.0048,  0.0010],\n",
      "        [-0.0073, -0.0011, -0.0030,  ...,  0.0001, -0.0035,  0.0017],\n",
      "        ...,\n",
      "        [-0.0050,  0.0001, -0.0003,  ..., -0.0012,  0.0066,  0.0014],\n",
      "        [ 0.0017,  0.0004,  0.0005,  ...,  0.0028,  0.0062,  0.0009],\n",
      "        [ 0.0067,  0.0012,  0.0007,  ...,  0.0018, -0.0056,  0.0013]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight \n",
      "value: tensor([[ 0.0123, -0.0024, -0.0009,  ..., -0.0010, -0.0016, -0.0016],\n",
      "        [ 0.0022, -0.0026,  0.0014,  ..., -0.0085, -0.0013, -0.0043],\n",
      "        [ 0.0015,  0.0055,  0.0050,  ...,  0.0092,  0.0060,  0.0009],\n",
      "        ...,\n",
      "        [-0.0035,  0.0154, -0.0005,  ..., -0.0070,  0.0018,  0.0043],\n",
      "        [-0.0096, -0.0016,  0.0002,  ...,  0.0038, -0.0027,  0.0073],\n",
      "        [ 0.0008,  0.0026,  0.0106,  ...,  0.0003, -0.0018, -0.0137]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight \n",
      "value: tensor([[-0.0130, -0.0069, -0.0026,  ..., -0.0081,  0.0065, -0.0100],\n",
      "        [ 0.0002, -0.0096,  0.0008,  ...,  0.0011,  0.0037,  0.0072],\n",
      "        [ 0.0012, -0.0007, -0.0009,  ..., -0.0018,  0.0137,  0.0065],\n",
      "        ...,\n",
      "        [ 0.0059, -0.0033,  0.0007,  ..., -0.0092,  0.0111, -0.0024],\n",
      "        [ 0.0099, -0.0076, -0.0110,  ...,  0.0086,  0.0055, -0.0048],\n",
      "        [ 0.0019, -0.0050, -0.0047,  ...,  0.0076,  0.0086, -0.0087]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight \n",
      "value: tensor([[-0.0013,  0.0077,  0.0037,  ...,  0.0013,  0.0032, -0.0060],\n",
      "        [ 0.0023,  0.0020,  0.0028,  ...,  0.0011,  0.0012,  0.0038],\n",
      "        [ 0.0096,  0.0032,  0.0074,  ...,  0.0044, -0.0048, -0.0004],\n",
      "        ...,\n",
      "        [-0.0043,  0.0011, -0.0038,  ..., -0.0047, -0.0159,  0.0005],\n",
      "        [-0.0016,  0.0067, -0.0024,  ..., -0.0117,  0.0049, -0.0077],\n",
      "        [-0.0098,  0.0026,  0.0041,  ..., -0.0027, -0.0019, -0.0023]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight \n",
      "value: tensor([[-4.7455e-03,  2.4244e-03, -1.1252e-02,  ..., -2.5323e-04,\n",
      "          2.9204e-03,  3.7550e-03],\n",
      "        [ 7.0348e-04, -2.5024e-03, -4.4276e-03,  ...,  3.1305e-03,\n",
      "          1.1403e-05,  2.3659e-03],\n",
      "        [ 2.9885e-03,  2.4557e-03, -4.6327e-04,  ..., -7.4211e-03,\n",
      "          5.3843e-03,  1.3786e-03],\n",
      "        ...,\n",
      "        [-1.7972e-02,  6.1774e-03,  8.2241e-03,  ...,  4.0710e-03,\n",
      "          8.3718e-03, -2.7259e-03],\n",
      "        [-2.8197e-03, -8.0492e-04,  6.1896e-03,  ...,  6.5395e-03,\n",
      "          5.1770e-03,  5.9827e-04],\n",
      "        [ 3.9563e-03, -2.7363e-04,  3.1404e-03,  ...,  1.5235e-02,\n",
      "         -3.0644e-03, -1.4426e-02]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight \n",
      "value: tensor([[ 0.0071,  0.0002,  0.0009,  ...,  0.0050,  0.0025,  0.0090],\n",
      "        [-0.0166,  0.0034,  0.0172,  ...,  0.0015,  0.0041,  0.0002],\n",
      "        [-0.0025, -0.0085,  0.0035,  ..., -0.0015,  0.0005,  0.0048],\n",
      "        ...,\n",
      "        [-0.0086, -0.0025,  0.0047,  ...,  0.0015, -0.0065,  0.0047],\n",
      "        [ 0.0091, -0.0117, -0.0105,  ..., -0.0010, -0.0056, -0.0052],\n",
      "        [ 0.0100, -0.0065, -0.0129,  ..., -0.0055, -0.0055, -0.0088]])\n",
      "\n",
      "key: unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight \n",
      "value: tensor([[ 0.0086,  0.0100, -0.0031,  ..., -0.0028,  0.0130,  0.0071],\n",
      "        [-0.0101,  0.0065,  0.0065,  ..., -0.0105,  0.0053, -0.0081],\n",
      "        [ 0.0060,  0.0087, -0.0069,  ..., -0.0096, -0.0041,  0.0011],\n",
      "        ...,\n",
      "        [ 0.0070, -0.0087,  0.0062,  ..., -0.0103,  0.0078, -0.0009],\n",
      "        [ 0.0044,  0.0086,  0.0064,  ..., -0.0188, -0.0109, -0.0119],\n",
      "        [ 0.0033, -0.0034,  0.0059,  ..., -0.0036,  0.0038, -0.0056]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.proj_in.lora.down.weight \n",
      "value: tensor([[[[ 0.0055]],\n",
      "\n",
      "         [[-0.0169]],\n",
      "\n",
      "         [[-0.0048]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         [[-0.0028]],\n",
      "\n",
      "         [[-0.0096]]],\n",
      "\n",
      "\n",
      "        [[[-0.0010]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0130]],\n",
      "\n",
      "         [[-0.0083]],\n",
      "\n",
      "         [[ 0.0061]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0158]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[-0.0054]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0027]],\n",
      "\n",
      "         [[ 0.0060]],\n",
      "\n",
      "         [[-0.0014]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0034]],\n",
      "\n",
      "         [[-0.0065]],\n",
      "\n",
      "         [[ 0.0054]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0045]],\n",
      "\n",
      "         [[-0.0074]],\n",
      "\n",
      "         [[ 0.0016]]],\n",
      "\n",
      "\n",
      "        [[[-0.0147]],\n",
      "\n",
      "         [[-0.0261]],\n",
      "\n",
      "         [[ 0.0121]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0130]],\n",
      "\n",
      "         [[-0.0081]],\n",
      "\n",
      "         [[-0.0147]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0003]],\n",
      "\n",
      "         [[ 0.0070]],\n",
      "\n",
      "         [[ 0.0021]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0016]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         [[ 0.0137]]]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.proj_in.lora.up.weight \n",
      "value: tensor([[[[-0.0044]],\n",
      "\n",
      "         [[-0.0099]],\n",
      "\n",
      "         [[ 0.0125]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0055]],\n",
      "\n",
      "         [[-0.0036]],\n",
      "\n",
      "         [[ 0.0141]]],\n",
      "\n",
      "\n",
      "        [[[-0.0206]],\n",
      "\n",
      "         [[-0.0189]],\n",
      "\n",
      "         [[ 0.0119]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0084]],\n",
      "\n",
      "         [[-0.0050]],\n",
      "\n",
      "         [[ 0.0010]]],\n",
      "\n",
      "\n",
      "        [[[-0.0134]],\n",
      "\n",
      "         [[ 0.0010]],\n",
      "\n",
      "         [[-0.0082]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0100]],\n",
      "\n",
      "         [[-0.0102]],\n",
      "\n",
      "         [[ 0.0021]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0225]],\n",
      "\n",
      "         [[ 0.0141]],\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0034]],\n",
      "\n",
      "         [[ 0.0138]],\n",
      "\n",
      "         [[-0.0012]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0119]],\n",
      "\n",
      "         [[-0.0189]],\n",
      "\n",
      "         [[ 0.0131]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0098]],\n",
      "\n",
      "         [[ 0.0057]],\n",
      "\n",
      "         [[ 0.0075]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0051]],\n",
      "\n",
      "         [[ 0.0038]],\n",
      "\n",
      "         [[ 0.0099]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0099]],\n",
      "\n",
      "         [[-0.0057]],\n",
      "\n",
      "         [[ 0.0111]]]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.proj_out.lora.down.weight \n",
      "value: tensor([[[[ 0.0025]],\n",
      "\n",
      "         [[ 0.0125]],\n",
      "\n",
      "         [[ 0.0172]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0200]],\n",
      "\n",
      "         [[-0.0128]],\n",
      "\n",
      "         [[-0.0241]]],\n",
      "\n",
      "\n",
      "        [[[-0.0115]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[ 0.0202]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0429]],\n",
      "\n",
      "         [[-0.0188]],\n",
      "\n",
      "         [[-0.0084]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0069]],\n",
      "\n",
      "         [[-0.0052]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0075]],\n",
      "\n",
      "         [[-0.0086]],\n",
      "\n",
      "         [[ 0.0061]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0153]],\n",
      "\n",
      "         [[ 0.0138]],\n",
      "\n",
      "         [[ 0.0035]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0091]],\n",
      "\n",
      "         [[-0.0089]],\n",
      "\n",
      "         [[-0.0051]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0250]],\n",
      "\n",
      "         [[ 0.0056]],\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0266]],\n",
      "\n",
      "         [[ 0.0110]],\n",
      "\n",
      "         [[-0.0113]]],\n",
      "\n",
      "\n",
      "        [[[-0.0037]],\n",
      "\n",
      "         [[ 0.0101]],\n",
      "\n",
      "         [[ 0.0022]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0049]],\n",
      "\n",
      "         [[ 0.0179]],\n",
      "\n",
      "         [[ 0.0105]]]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.proj_out.lora.up.weight \n",
      "value: tensor([[[[ 0.0114]],\n",
      "\n",
      "         [[-0.0168]],\n",
      "\n",
      "         [[ 0.0047]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0231]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[-0.0092]]],\n",
      "\n",
      "\n",
      "        [[[-0.0006]],\n",
      "\n",
      "         [[ 0.0121]],\n",
      "\n",
      "         [[ 0.0039]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0074]],\n",
      "\n",
      "         [[-0.0070]],\n",
      "\n",
      "         [[ 0.0112]]],\n",
      "\n",
      "\n",
      "        [[[-0.0243]],\n",
      "\n",
      "         [[ 0.0118]],\n",
      "\n",
      "         [[ 0.0104]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0234]],\n",
      "\n",
      "         [[ 0.0037]],\n",
      "\n",
      "         [[ 0.0107]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0045]],\n",
      "\n",
      "         [[-0.0255]],\n",
      "\n",
      "         [[-0.0045]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0139]],\n",
      "\n",
      "         [[-0.0171]],\n",
      "\n",
      "         [[-0.0026]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0004]],\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         [[ 0.0040]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0116]],\n",
      "\n",
      "         [[ 0.0174]],\n",
      "\n",
      "         [[ 0.0043]]],\n",
      "\n",
      "\n",
      "        [[[-0.0047]],\n",
      "\n",
      "         [[ 0.0113]],\n",
      "\n",
      "         [[ 0.0137]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0177]],\n",
      "\n",
      "         [[-0.0038]],\n",
      "\n",
      "         [[ 0.0014]]]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight \n",
      "value: tensor([[-6.2351e-03, -1.1882e-03, -1.0370e-02,  ..., -8.8132e-05,\n",
      "         -2.2072e-03, -1.2997e-03],\n",
      "        [-1.0355e-02, -5.9342e-03,  1.0532e-02,  ...,  3.4942e-03,\n",
      "         -1.1886e-03, -5.8744e-03],\n",
      "        [-1.3933e-02,  3.5168e-03, -3.3791e-04,  ..., -8.5843e-04,\n",
      "          1.0536e-02, -1.3142e-02],\n",
      "        ...,\n",
      "        [ 7.4954e-04, -1.8637e-03, -3.5136e-03,  ..., -2.4192e-03,\n",
      "          4.4320e-03, -8.6250e-04],\n",
      "        [ 1.6910e-03,  1.3583e-03,  2.7815e-03,  ...,  3.2213e-03,\n",
      "          4.3929e-03,  2.6197e-03],\n",
      "        [ 1.3647e-03,  3.4299e-03, -4.1866e-03,  ...,  1.4190e-03,\n",
      "          1.1681e-03, -4.0691e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight \n",
      "value: tensor([[ 0.0012, -0.0048, -0.0044,  ..., -0.0031, -0.0013, -0.0020],\n",
      "        [ 0.0050,  0.0023, -0.0048,  ...,  0.0101,  0.0055,  0.0052],\n",
      "        [ 0.0010,  0.0013,  0.0009,  ..., -0.0010,  0.0099,  0.0017],\n",
      "        ...,\n",
      "        [-0.0079,  0.0020,  0.0090,  ...,  0.0035, -0.0097,  0.0005],\n",
      "        [ 0.0022, -0.0043,  0.0074,  ...,  0.0066,  0.0081,  0.0035],\n",
      "        [ 0.0088, -0.0001,  0.0120,  ..., -0.0005,  0.0020,  0.0006]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight \n",
      "value: tensor([[-0.0053, -0.0009,  0.0013,  ..., -0.0160, -0.0097,  0.0059],\n",
      "        [ 0.0008, -0.0226, -0.0008,  ..., -0.0050, -0.0012,  0.0118],\n",
      "        [ 0.0003, -0.0039,  0.0057,  ...,  0.0029, -0.0071, -0.0024],\n",
      "        ...,\n",
      "        [-0.0051,  0.0062,  0.0024,  ..., -0.0018,  0.0015, -0.0018],\n",
      "        [-0.0092,  0.0033,  0.0031,  ...,  0.0091,  0.0164,  0.0125],\n",
      "        [-0.0036, -0.0011, -0.0009,  ..., -0.0002,  0.0069, -0.0107]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight \n",
      "value: tensor([[-7.9631e-03,  1.7846e-03,  2.3745e-03,  ..., -1.3183e-02,\n",
      "         -8.6515e-03,  6.4493e-03],\n",
      "        [ 8.8993e-04, -5.3066e-03,  1.3056e-03,  ...,  7.7257e-05,\n",
      "          2.2783e-03,  6.6798e-03],\n",
      "        [-7.0251e-03,  8.3071e-03, -8.4317e-04,  ...,  5.6561e-03,\n",
      "         -2.9453e-03, -4.9190e-03],\n",
      "        ...,\n",
      "        [ 5.7574e-03, -1.2859e-02, -2.1005e-03,  ...,  7.4630e-03,\n",
      "          1.4031e-02, -2.5195e-02],\n",
      "        [-1.2323e-03,  7.4374e-03, -5.2860e-04,  ...,  5.0365e-03,\n",
      "         -7.0651e-03, -2.1274e-03],\n",
      "        [ 2.1594e-04,  9.1092e-03,  1.0522e-02,  ..., -9.6625e-03,\n",
      "         -3.1017e-03,  3.1395e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight \n",
      "value: tensor([[ 0.0094, -0.0050,  0.0001,  ...,  0.0015,  0.0004,  0.0005],\n",
      "        [ 0.0096,  0.0031,  0.0026,  ..., -0.0004,  0.0050,  0.0036],\n",
      "        [-0.0003,  0.0030, -0.0013,  ..., -0.0004, -0.0003, -0.0007],\n",
      "        ...,\n",
      "        [ 0.0010, -0.0017,  0.0037,  ..., -0.0008,  0.0072,  0.0032],\n",
      "        [-0.0017,  0.0015, -0.0032,  ...,  0.0008, -0.0172, -0.0204],\n",
      "        [-0.0103,  0.0031,  0.0030,  ..., -0.0106,  0.0049, -0.0009]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight \n",
      "value: tensor([[-0.0067, -0.0083,  0.0067,  ..., -0.0046,  0.0068, -0.0002],\n",
      "        [-0.0068, -0.0030,  0.0008,  ..., -0.0049,  0.0081,  0.0006],\n",
      "        [-0.0018,  0.0005,  0.0086,  ...,  0.0023, -0.0123,  0.0063],\n",
      "        ...,\n",
      "        [-0.0006,  0.0013, -0.0059,  ..., -0.0008,  0.0041, -0.0028],\n",
      "        [-0.0057,  0.0020,  0.0018,  ...,  0.0066, -0.0025,  0.0046],\n",
      "        [-0.0111,  0.0049, -0.0073,  ...,  0.0015,  0.0039, -0.0006]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight \n",
      "value: tensor([[-6.2657e-03, -5.3721e-03,  6.7819e-03,  ..., -4.4970e-03,\n",
      "         -9.3602e-03, -1.3155e-03],\n",
      "        [ 6.0795e-03,  3.7072e-03,  9.1942e-03,  ..., -3.3268e-03,\n",
      "         -3.4817e-03, -1.1217e-03],\n",
      "        [-1.1087e-02,  6.4645e-03,  8.6257e-03,  ..., -1.4911e-02,\n",
      "          7.9251e-03, -5.0438e-03],\n",
      "        ...,\n",
      "        [-1.5852e-03,  6.4994e-03, -1.6865e-03,  ...,  5.2092e-03,\n",
      "          6.1802e-03,  2.8616e-03],\n",
      "        [ 1.1930e-02, -6.4327e-05, -7.0107e-03,  ...,  1.1382e-02,\n",
      "         -3.0388e-03,  1.4149e-03],\n",
      "        [ 3.4508e-04,  1.5893e-03, -2.6030e-03,  ..., -4.2586e-03,\n",
      "         -1.3713e-02, -1.2542e-02]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight \n",
      "value: tensor([[ 0.0069, -0.0008, -0.0003,  ...,  0.0111,  0.0024,  0.0025],\n",
      "        [-0.0033, -0.0035,  0.0119,  ...,  0.0019, -0.0100, -0.0047],\n",
      "        [-0.0050,  0.0008,  0.0085,  ..., -0.0071, -0.0103, -0.0083],\n",
      "        ...,\n",
      "        [ 0.0060, -0.0006,  0.0080,  ...,  0.0009, -0.0026, -0.0046],\n",
      "        [ 0.0016,  0.0053,  0.0062,  ..., -0.0059, -0.0100,  0.0052],\n",
      "        [ 0.0137,  0.0010, -0.0135,  ..., -0.0016, -0.0011, -0.0079]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight \n",
      "value: tensor([[ 0.0030, -0.0033, -0.0064,  ..., -0.0069,  0.0033, -0.0067],\n",
      "        [-0.0021,  0.0086, -0.0033,  ..., -0.0027, -0.0036,  0.0004],\n",
      "        [ 0.0033, -0.0013,  0.0008,  ...,  0.0021, -0.0006,  0.0027],\n",
      "        ...,\n",
      "        [-0.0015, -0.0009,  0.0029,  ..., -0.0045, -0.0049, -0.0101],\n",
      "        [-0.0043, -0.0109,  0.0057,  ..., -0.0003,  0.0090, -0.0132],\n",
      "        [-0.0008, -0.0048,  0.0011,  ...,  0.0061, -0.0041,  0.0034]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight \n",
      "value: tensor([[-2.3619e-03, -7.3645e-03, -4.3130e-03,  ...,  4.1699e-04,\n",
      "         -5.9550e-04,  9.9022e-04],\n",
      "        [-3.6522e-03, -2.2345e-03,  8.8894e-03,  ...,  4.1242e-03,\n",
      "         -5.4749e-03, -5.8275e-03],\n",
      "        [-6.0304e-03, -9.3700e-04,  6.7148e-03,  ..., -6.8157e-03,\n",
      "          5.0945e-03, -1.0340e-03],\n",
      "        ...,\n",
      "        [-3.3725e-03,  3.8165e-03, -8.3610e-03,  ...,  8.6055e-03,\n",
      "         -8.8668e-05,  6.4720e-03],\n",
      "        [ 6.2286e-03, -3.7728e-03, -1.1693e-03,  ..., -1.0893e-03,\n",
      "         -6.0814e-03,  2.7757e-03],\n",
      "        [ 2.4222e-03, -1.0199e-02,  4.6103e-03,  ...,  2.0599e-03,\n",
      "          7.6255e-03,  3.8558e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight \n",
      "value: tensor([[ 0.0081, -0.0008, -0.0014,  ..., -0.0052, -0.0005, -0.0065],\n",
      "        [-0.0003,  0.0016,  0.0022,  ...,  0.0064, -0.0008, -0.0032],\n",
      "        [ 0.0001,  0.0086, -0.0031,  ..., -0.0060,  0.0127,  0.0054],\n",
      "        ...,\n",
      "        [ 0.0094, -0.0029,  0.0034,  ..., -0.0129, -0.0093, -0.0094],\n",
      "        [-0.0055,  0.0025, -0.0063,  ..., -0.0013, -0.0179,  0.0079],\n",
      "        [-0.0050,  0.0088,  0.0149,  ...,  0.0071, -0.0002,  0.0013]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight \n",
      "value: tensor([[-1.5647e-03, -5.2197e-03, -1.3790e-03,  ..., -1.6810e-03,\n",
      "          1.7768e-03, -4.4873e-03],\n",
      "        [ 1.1614e-03, -6.3443e-03, -3.0028e-03,  ...,  1.3045e-03,\n",
      "         -3.8541e-03, -3.2987e-03],\n",
      "        [ 2.3773e-04, -6.0071e-03, -1.1217e-02,  ..., -7.4820e-03,\n",
      "          1.6931e-03, -5.0326e-03],\n",
      "        ...,\n",
      "        [ 2.9189e-03, -1.0854e-02,  4.9161e-04,  ...,  7.8999e-03,\n",
      "         -3.4423e-03,  7.0787e-03],\n",
      "        [-5.6995e-03,  2.2246e-03,  1.5587e-03,  ..., -4.7484e-03,\n",
      "          6.0396e-04,  3.3165e-05],\n",
      "        [-8.1553e-03, -6.0537e-03, -7.3933e-03,  ..., -2.0786e-03,\n",
      "         -2.1076e-03, -2.7183e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight \n",
      "value: tensor([[-4.1560e-03, -9.1585e-03, -3.9850e-03,  ...,  4.0132e-03,\n",
      "          3.1743e-03, -3.4425e-05],\n",
      "        [-6.0470e-03,  8.3292e-04, -7.7580e-04,  ...,  8.5475e-05,\n",
      "         -1.0017e-03, -4.9936e-04],\n",
      "        [ 4.8101e-03,  5.2714e-03, -4.0099e-03,  ..., -1.9862e-03,\n",
      "          3.6647e-03, -2.0447e-04],\n",
      "        ...,\n",
      "        [ 6.2854e-03,  1.0578e-03, -2.6339e-03,  ...,  4.8431e-03,\n",
      "         -1.0474e-02, -1.0578e-02],\n",
      "        [ 2.6413e-03,  5.0878e-03,  9.3880e-03,  ..., -5.7444e-03,\n",
      "          1.9530e-03, -6.0310e-03],\n",
      "        [ 6.2377e-04, -1.0517e-03, -2.4110e-03,  ..., -4.4050e-03,\n",
      "         -1.8445e-03,  2.7118e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight \n",
      "value: tensor([[ 0.0001, -0.0024,  0.0018,  ...,  0.0031,  0.0032, -0.0034],\n",
      "        [ 0.0006,  0.0056,  0.0024,  ..., -0.0030,  0.0031, -0.0032],\n",
      "        [ 0.0004, -0.0038, -0.0014,  ...,  0.0001, -0.0025, -0.0012],\n",
      "        ...,\n",
      "        [-0.0014,  0.0011, -0.0026,  ..., -0.0019,  0.0020, -0.0034],\n",
      "        [ 0.0053, -0.0057, -0.0041,  ...,  0.0052,  0.0093, -0.0025],\n",
      "        [-0.0046, -0.0040, -0.0084,  ...,  0.0030,  0.0021, -0.0015]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight \n",
      "value: tensor([[ 2.1954e-04, -1.6610e-03, -1.0513e-02,  ...,  4.2919e-03,\n",
      "         -2.1804e-03, -2.0938e-03],\n",
      "        [-6.0999e-03, -1.2165e-02, -6.0468e-03,  ...,  7.0201e-03,\n",
      "         -2.0892e-03,  7.3684e-05],\n",
      "        [-1.3542e-03,  8.2248e-03, -7.1064e-04,  ..., -3.0712e-03,\n",
      "         -4.6239e-03,  9.1779e-03],\n",
      "        ...,\n",
      "        [-3.6499e-03, -2.5428e-03, -4.4873e-03,  ..., -4.3907e-03,\n",
      "          1.6059e-03, -2.8573e-03],\n",
      "        [-9.8609e-03, -4.8803e-04,  8.6474e-05,  ...,  3.3073e-03,\n",
      "         -1.6721e-03,  9.3447e-04],\n",
      "        [ 2.0408e-03,  9.0155e-04,  4.2444e-03,  ...,  4.9673e-04,\n",
      "          3.7828e-03, -5.1962e-04]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight \n",
      "value: tensor([[-0.0068, -0.0124,  0.0007,  ...,  0.0032, -0.0102,  0.0033],\n",
      "        [ 0.0034,  0.0066,  0.0091,  ...,  0.0061,  0.0042, -0.0091],\n",
      "        [-0.0050, -0.0033, -0.0050,  ...,  0.0012, -0.0041,  0.0008],\n",
      "        ...,\n",
      "        [-0.0035, -0.0118, -0.0017,  ..., -0.0024,  0.0028, -0.0103],\n",
      "        [-0.0022, -0.0072, -0.0082,  ..., -0.0134,  0.0031, -0.0137],\n",
      "        [-0.0014,  0.0029,  0.0049,  ...,  0.0051, -0.0057,  0.0083]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight \n",
      "value: tensor([[ 4.5432e-03,  1.9760e-03,  4.2692e-03,  ...,  5.3754e-04,\n",
      "         -4.9646e-03,  4.1423e-03],\n",
      "        [-7.7753e-05, -1.2544e-03, -4.7437e-04,  ..., -7.7458e-03,\n",
      "         -2.1084e-04, -8.6475e-03],\n",
      "        [-5.4371e-03,  5.7539e-03,  5.9386e-03,  ...,  1.3889e-02,\n",
      "         -4.0487e-03, -6.0562e-05],\n",
      "        ...,\n",
      "        [ 8.2726e-04,  4.6329e-03,  1.5521e-03,  ...,  2.7313e-03,\n",
      "         -1.6993e-02,  1.7392e-02],\n",
      "        [ 4.9547e-03, -1.3321e-03, -6.5467e-05,  ...,  1.8179e-04,\n",
      "         -8.3635e-03, -3.6491e-03],\n",
      "        [ 3.6994e-03,  5.3228e-03,  6.8322e-03,  ...,  1.3637e-02,\n",
      "         -8.9376e-03, -1.0641e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight \n",
      "value: tensor([[-6.8299e-03,  3.5168e-03, -2.7979e-03,  ...,  5.0428e-03,\n",
      "          1.5753e-02,  1.7749e-04],\n",
      "        [-2.1451e-04, -1.8407e-03,  3.7372e-03,  ..., -8.4341e-04,\n",
      "         -7.2853e-03, -5.4199e-03],\n",
      "        [-3.0573e-03,  1.3430e-02,  1.1887e-02,  ...,  3.7338e-03,\n",
      "          4.6224e-04, -4.5027e-03],\n",
      "        ...,\n",
      "        [ 2.9776e-03,  7.2526e-03, -2.8375e-03,  ...,  5.0002e-05,\n",
      "         -9.0122e-03, -1.0783e-02],\n",
      "        [-7.0852e-03,  9.2234e-03,  1.4133e-03,  ..., -6.7681e-03,\n",
      "          5.6952e-03,  4.4072e-04],\n",
      "        [-5.6076e-03, -2.0268e-02,  6.1415e-03,  ..., -2.4359e-03,\n",
      "         -9.7957e-04,  7.3966e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight \n",
      "value: tensor([[-0.0099,  0.0033,  0.0023,  ..., -0.0042, -0.0105, -0.0110],\n",
      "        [ 0.0040,  0.0166, -0.0087,  ...,  0.0048, -0.0102, -0.0090],\n",
      "        [-0.0062, -0.0005,  0.0062,  ..., -0.0129,  0.0098,  0.0075],\n",
      "        ...,\n",
      "        [-0.0033, -0.0073, -0.0040,  ..., -0.0069, -0.0070, -0.0018],\n",
      "        [ 0.0128,  0.0056,  0.0028,  ...,  0.0004,  0.0132,  0.0029],\n",
      "        [-0.0090, -0.0156, -0.0096,  ..., -0.0160, -0.0022,  0.0011]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight \n",
      "value: tensor([[-0.0122, -0.0119,  0.0110,  ..., -0.0032,  0.0026,  0.0091],\n",
      "        [ 0.0035, -0.0035, -0.0093,  ..., -0.0049, -0.0020, -0.0017],\n",
      "        [ 0.0038,  0.0149, -0.0042,  ...,  0.0045,  0.0158,  0.0065],\n",
      "        ...,\n",
      "        [-0.0046,  0.0135, -0.0046,  ..., -0.0057, -0.0011, -0.0168],\n",
      "        [ 0.0017,  0.0030, -0.0011,  ...,  0.0001, -0.0047,  0.0013],\n",
      "        [-0.0079, -0.0174, -0.0059,  ...,  0.0146,  0.0012,  0.0018]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.proj_in.lora.down.weight \n",
      "value: tensor([[[[ 0.0005]],\n",
      "\n",
      "         [[ 0.0309]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0220]],\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         [[ 0.0039]]],\n",
      "\n",
      "\n",
      "        [[[-0.0094]],\n",
      "\n",
      "         [[-0.0123]],\n",
      "\n",
      "         [[ 0.0124]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0322]],\n",
      "\n",
      "         [[-0.0153]],\n",
      "\n",
      "         [[ 0.0068]]],\n",
      "\n",
      "\n",
      "        [[[-0.0004]],\n",
      "\n",
      "         [[-0.0093]],\n",
      "\n",
      "         [[ 0.0202]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0062]],\n",
      "\n",
      "         [[-0.0179]],\n",
      "\n",
      "         [[ 0.0156]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0038]],\n",
      "\n",
      "         [[ 0.0052]],\n",
      "\n",
      "         [[ 0.0095]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0087]],\n",
      "\n",
      "         [[ 0.0060]],\n",
      "\n",
      "         [[ 0.0182]]],\n",
      "\n",
      "\n",
      "        [[[-0.0007]],\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0161]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[-0.0003]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0125]],\n",
      "\n",
      "         [[-0.0003]],\n",
      "\n",
      "         [[ 0.0253]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         [[ 0.0081]],\n",
      "\n",
      "         [[ 0.0141]]]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.proj_in.lora.up.weight \n",
      "value: tensor([[[[ 5.4002e-03]],\n",
      "\n",
      "         [[ 1.6900e-02]],\n",
      "\n",
      "         [[-2.8416e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5117e-02]],\n",
      "\n",
      "         [[-7.1701e-03]],\n",
      "\n",
      "         [[-8.5818e-03]]],\n",
      "\n",
      "\n",
      "        [[[-8.7011e-03]],\n",
      "\n",
      "         [[ 2.1645e-03]],\n",
      "\n",
      "         [[ 2.7532e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.6549e-03]],\n",
      "\n",
      "         [[-1.2116e-02]],\n",
      "\n",
      "         [[-2.3314e-03]]],\n",
      "\n",
      "\n",
      "        [[[-7.4275e-07]],\n",
      "\n",
      "         [[-2.8992e-02]],\n",
      "\n",
      "         [[ 1.9608e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.5483e-02]],\n",
      "\n",
      "         [[ 4.2839e-03]],\n",
      "\n",
      "         [[-2.0445e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.2491e-02]],\n",
      "\n",
      "         [[-1.2597e-02]],\n",
      "\n",
      "         [[ 3.3417e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.8411e-04]],\n",
      "\n",
      "         [[ 2.0915e-03]],\n",
      "\n",
      "         [[-2.2226e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1586e-02]],\n",
      "\n",
      "         [[-1.9423e-03]],\n",
      "\n",
      "         [[-1.7032e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.3507e-03]],\n",
      "\n",
      "         [[-7.5259e-04]],\n",
      "\n",
      "         [[ 2.5961e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.0169e-02]],\n",
      "\n",
      "         [[-1.4623e-02]],\n",
      "\n",
      "         [[ 1.1999e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.6317e-03]],\n",
      "\n",
      "         [[-8.1078e-03]],\n",
      "\n",
      "         [[ 9.9079e-03]]]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.proj_out.lora.down.weight \n",
      "value: tensor([[[[ 0.0144]],\n",
      "\n",
      "         [[ 0.0049]],\n",
      "\n",
      "         [[-0.0147]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0002]],\n",
      "\n",
      "         [[-0.0026]],\n",
      "\n",
      "         [[ 0.0063]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0113]],\n",
      "\n",
      "         [[ 0.0252]],\n",
      "\n",
      "         [[ 0.0206]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0070]],\n",
      "\n",
      "         [[ 0.0153]],\n",
      "\n",
      "         [[ 0.0060]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0096]],\n",
      "\n",
      "         [[-0.0155]],\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0235]],\n",
      "\n",
      "         [[-0.0197]],\n",
      "\n",
      "         [[-0.0044]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0030]],\n",
      "\n",
      "         [[ 0.0067]],\n",
      "\n",
      "         [[-0.0058]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0255]],\n",
      "\n",
      "         [[ 0.0035]],\n",
      "\n",
      "         [[ 0.0100]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0078]],\n",
      "\n",
      "         [[ 0.0124]],\n",
      "\n",
      "         [[-0.0134]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0163]],\n",
      "\n",
      "         [[ 0.0165]],\n",
      "\n",
      "         [[-0.0108]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0204]],\n",
      "\n",
      "         [[-0.0005]],\n",
      "\n",
      "         [[ 0.0153]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0058]],\n",
      "\n",
      "         [[-0.0109]],\n",
      "\n",
      "         [[-0.0270]]]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.proj_out.lora.up.weight \n",
      "value: tensor([[[[ 0.0045]],\n",
      "\n",
      "         [[-0.0256]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0051]],\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[-0.0189]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0014]],\n",
      "\n",
      "         [[-0.0135]],\n",
      "\n",
      "         [[-0.0030]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0040]],\n",
      "\n",
      "         [[-0.0201]],\n",
      "\n",
      "         [[-0.0102]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0249]],\n",
      "\n",
      "         [[-0.0117]],\n",
      "\n",
      "         [[ 0.0023]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0060]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         [[ 0.0047]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0040]],\n",
      "\n",
      "         [[ 0.0174]],\n",
      "\n",
      "         [[ 0.0391]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0154]],\n",
      "\n",
      "         [[ 0.0048]],\n",
      "\n",
      "         [[ 0.0179]]],\n",
      "\n",
      "\n",
      "        [[[-0.0064]],\n",
      "\n",
      "         [[-0.0107]],\n",
      "\n",
      "         [[ 0.0252]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0068]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[ 0.0231]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0143]],\n",
      "\n",
      "         [[-0.0337]],\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0203]],\n",
      "\n",
      "         [[-0.0204]],\n",
      "\n",
      "         [[-0.0102]]]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight \n",
      "value: tensor([[ 0.0035,  0.0219, -0.0110,  ..., -0.0294,  0.0159,  0.0173],\n",
      "        [-0.0136,  0.0093,  0.0002,  ...,  0.0047,  0.0074,  0.0201],\n",
      "        [-0.0050, -0.0063,  0.0081,  ...,  0.0092, -0.0095, -0.0291],\n",
      "        ...,\n",
      "        [-0.0081, -0.0268, -0.0112,  ..., -0.0061, -0.0134, -0.0210],\n",
      "        [-0.0141, -0.0006, -0.0064,  ..., -0.0137,  0.0069, -0.0009],\n",
      "        [ 0.0079,  0.0196, -0.0142,  ...,  0.0110,  0.0083,  0.0058]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight \n",
      "value: tensor([[-0.0048, -0.0076,  0.0045,  ...,  0.0078,  0.0141,  0.0116],\n",
      "        [ 0.0137, -0.0004,  0.0020,  ...,  0.0092, -0.0090,  0.0015],\n",
      "        [-0.0051,  0.0106,  0.0011,  ...,  0.0007,  0.0089, -0.0029],\n",
      "        ...,\n",
      "        [-0.0221, -0.0242,  0.0214,  ...,  0.0264,  0.0132, -0.0006],\n",
      "        [-0.0051, -0.0017, -0.0096,  ..., -0.0126,  0.0226,  0.0003],\n",
      "        [-0.0141,  0.0031, -0.0092,  ...,  0.0077,  0.0036,  0.0191]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight \n",
      "value: tensor([[-0.0146, -0.0113,  0.0084,  ..., -0.0115, -0.0110, -0.0034],\n",
      "        [-0.0005,  0.0072,  0.0031,  ..., -0.0196, -0.0231, -0.0108],\n",
      "        [ 0.0031,  0.0009,  0.0030,  ..., -0.0035,  0.0091,  0.0286],\n",
      "        ...,\n",
      "        [ 0.0025,  0.0006,  0.0091,  ..., -0.0417, -0.0386, -0.0168],\n",
      "        [ 0.0063,  0.0060, -0.0041,  ..., -0.0134, -0.0116, -0.0211],\n",
      "        [-0.0064,  0.0087, -0.0165,  ..., -0.0079,  0.0046,  0.0165]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight \n",
      "value: tensor([[ 0.0227,  0.0029, -0.0174,  ..., -0.0080,  0.0089, -0.0063],\n",
      "        [ 0.0074, -0.0025,  0.0179,  ..., -0.0127,  0.0221,  0.0222],\n",
      "        [-0.0023,  0.0009,  0.0077,  ..., -0.0029, -0.0200,  0.0073],\n",
      "        ...,\n",
      "        [ 0.0072,  0.0114, -0.0066,  ...,  0.0046, -0.0160,  0.0075],\n",
      "        [ 0.0021, -0.0057,  0.0159,  ..., -0.0199,  0.0103, -0.0206],\n",
      "        [ 0.0113,  0.0026, -0.0180,  ...,  0.0103, -0.0077,  0.0077]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight \n",
      "value: tensor([[-0.0016,  0.0174, -0.0006,  ...,  0.0100,  0.0073, -0.0066],\n",
      "        [-0.0179, -0.0186,  0.0084,  ...,  0.0021, -0.0179,  0.0082],\n",
      "        [ 0.0003,  0.0007,  0.0120,  ...,  0.0224, -0.0010,  0.0060],\n",
      "        ...,\n",
      "        [-0.0100,  0.0059, -0.0017,  ..., -0.0101,  0.0021, -0.0010],\n",
      "        [-0.0283,  0.0016,  0.0230,  ..., -0.0094, -0.0100, -0.0007],\n",
      "        [ 0.0014,  0.0045,  0.0103,  ..., -0.0101,  0.0077, -0.0259]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight \n",
      "value: tensor([[-5.4812e-03,  8.8524e-03,  3.5497e-04,  ...,  2.1644e-02,\n",
      "          1.7290e-03, -3.8991e-03],\n",
      "        [ 1.0360e-02, -3.8887e-03,  1.0130e-02,  ...,  2.4370e-03,\n",
      "         -1.1361e-02,  6.8954e-04],\n",
      "        [ 4.8484e-04, -2.3881e-03,  2.5662e-03,  ...,  2.6538e-03,\n",
      "         -4.7201e-05, -7.7430e-03],\n",
      "        ...,\n",
      "        [ 4.4026e-02,  2.5481e-02,  1.0207e-02,  ..., -3.4509e-02,\n",
      "          1.9353e-02, -2.3496e-02],\n",
      "        [ 2.1754e-02,  8.8647e-03,  2.0521e-02,  ..., -4.2108e-02,\n",
      "          1.5222e-02, -2.0172e-02],\n",
      "        [ 2.7500e-03, -1.5899e-03,  3.1190e-02,  ...,  5.8647e-03,\n",
      "          5.0802e-02,  5.0591e-02]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight \n",
      "value: tensor([[-0.0100,  0.0027,  0.0074,  ...,  0.0153,  0.0014,  0.0197],\n",
      "        [ 0.0122, -0.0167,  0.0084,  ..., -0.0099, -0.0074,  0.0065],\n",
      "        [-0.0055, -0.0131,  0.0064,  ..., -0.0082, -0.0140, -0.0143],\n",
      "        ...,\n",
      "        [ 0.0054, -0.0003,  0.0048,  ..., -0.0045, -0.0024,  0.0020],\n",
      "        [-0.0032, -0.0221,  0.0018,  ..., -0.0125, -0.0127, -0.0117],\n",
      "        [-0.0067, -0.0043, -0.0023,  ..., -0.0053, -0.0093,  0.0016]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight \n",
      "value: tensor([[-0.0136, -0.0100, -0.0280,  ...,  0.0082, -0.0221,  0.0038],\n",
      "        [ 0.0149,  0.0013,  0.0022,  ..., -0.0106, -0.0040,  0.0172],\n",
      "        [ 0.0074,  0.0116, -0.0144,  ...,  0.0006,  0.0088, -0.0211],\n",
      "        ...,\n",
      "        [ 0.0061, -0.0234, -0.0180,  ..., -0.0145, -0.0051, -0.0069],\n",
      "        [ 0.0157, -0.0092,  0.0052,  ..., -0.0163,  0.0443,  0.0172],\n",
      "        [-0.0024, -0.0123,  0.0361,  ..., -0.0011, -0.0084, -0.0294]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight \n",
      "value: tensor([[ 0.0009, -0.0061, -0.0124,  ..., -0.0007, -0.0008, -0.0144],\n",
      "        [ 0.0037, -0.0031,  0.0008,  ..., -0.0043, -0.0030,  0.0033],\n",
      "        [ 0.0117,  0.0049, -0.0014,  ...,  0.0004, -0.0012,  0.0021],\n",
      "        ...,\n",
      "        [-0.0069,  0.0025, -0.0018,  ...,  0.0070, -0.0059, -0.0060],\n",
      "        [ 0.0098,  0.0008,  0.0058,  ...,  0.0014,  0.0113,  0.0108],\n",
      "        [ 0.0133, -0.0017,  0.0003,  ...,  0.0037, -0.0058, -0.0048]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight \n",
      "value: tensor([[ 2.7922e-03,  1.2293e-03,  1.0850e-03,  ...,  1.9659e-03,\n",
      "          1.2258e-02,  1.7970e-03],\n",
      "        [-8.7735e-04, -3.1045e-03, -1.1308e-02,  ...,  3.9487e-04,\n",
      "         -8.0590e-03,  2.1497e-03],\n",
      "        [-1.0380e-02, -7.4528e-04,  1.0911e-02,  ...,  7.6804e-03,\n",
      "         -4.9613e-03,  9.5314e-04],\n",
      "        ...,\n",
      "        [-1.0518e-02,  2.6193e-03,  6.4105e-03,  ..., -4.3174e-03,\n",
      "          6.8665e-03, -8.5419e-04],\n",
      "        [ 7.3258e-03,  2.7759e-03,  3.8781e-03,  ..., -3.7889e-03,\n",
      "          4.4429e-06,  1.2209e-03],\n",
      "        [-4.2222e-03, -3.7067e-03,  1.9953e-03,  ..., -1.1179e-03,\n",
      "          5.4659e-03, -8.7156e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight \n",
      "value: tensor([[-1.1748e-02,  3.3449e-03, -2.5856e-04,  ...,  6.0768e-04,\n",
      "          3.8508e-03,  9.9585e-03],\n",
      "        [-1.5712e-03, -2.1042e-03,  6.7327e-04,  ...,  3.5030e-03,\n",
      "         -5.6710e-04,  2.9613e-05],\n",
      "        [ 1.3280e-02,  7.0666e-03, -5.2803e-03,  ...,  3.0902e-03,\n",
      "         -1.5072e-03, -5.0731e-03],\n",
      "        ...,\n",
      "        [ 4.0878e-03,  9.8970e-03,  2.1207e-03,  ..., -1.6727e-03,\n",
      "         -3.0650e-03, -2.5693e-03],\n",
      "        [ 6.4150e-04, -6.4826e-03,  1.2459e-02,  ..., -6.9344e-03,\n",
      "         -1.7359e-03,  1.7581e-03],\n",
      "        [ 3.6121e-03,  6.6021e-03,  9.8185e-04,  ...,  1.2424e-02,\n",
      "         -3.3501e-04,  1.7270e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight \n",
      "value: tensor([[ 0.0015, -0.0076,  0.0021,  ...,  0.0060,  0.0086, -0.0009],\n",
      "        [ 0.0135, -0.0235,  0.0011,  ...,  0.0027, -0.0045,  0.0204],\n",
      "        [-0.0182, -0.0032, -0.0109,  ...,  0.0070,  0.0005, -0.0116],\n",
      "        ...,\n",
      "        [-0.0045, -0.0032, -0.0019,  ..., -0.0078,  0.0059,  0.0101],\n",
      "        [ 0.0055,  0.0057,  0.0082,  ...,  0.0075, -0.0069, -0.0049],\n",
      "        [ 0.0008, -0.0005,  0.0022,  ...,  0.0036,  0.0025,  0.0046]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight \n",
      "value: tensor([[-0.0054, -0.0054, -0.0003,  ..., -0.0043,  0.0021,  0.0006],\n",
      "        [-0.0039, -0.0066, -0.0002,  ..., -0.0018,  0.0019,  0.0039],\n",
      "        [-0.0042, -0.0035,  0.0068,  ...,  0.0080,  0.0068,  0.0002],\n",
      "        ...,\n",
      "        [ 0.0020,  0.0046, -0.0007,  ...,  0.0031, -0.0018, -0.0037],\n",
      "        [-0.0093, -0.0054,  0.0099,  ..., -0.0023, -0.0003, -0.0020],\n",
      "        [ 0.0022, -0.0010, -0.0022,  ...,  0.0031,  0.0080, -0.0025]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight \n",
      "value: tensor([[-1.1202e-02,  2.4543e-03, -4.9489e-03,  ..., -6.3129e-03,\n",
      "         -5.5477e-03,  1.6582e-03],\n",
      "        [ 4.5499e-03,  4.0890e-03, -1.3424e-03,  ...,  9.6658e-04,\n",
      "         -1.0667e-02, -5.6284e-03],\n",
      "        [-9.5147e-03, -2.8654e-03, -6.1126e-06,  ..., -1.9828e-03,\n",
      "         -1.7464e-03,  2.1355e-03],\n",
      "        ...,\n",
      "        [ 1.5033e-02,  2.2018e-03,  5.0893e-04,  ...,  1.0355e-03,\n",
      "         -8.8419e-04,  3.3655e-05],\n",
      "        [ 7.2919e-03, -1.3538e-03,  2.2380e-03,  ..., -2.0116e-03,\n",
      "          5.6815e-03,  1.4981e-03],\n",
      "        [ 7.2879e-03, -1.1236e-03,  1.4502e-03,  ..., -3.1881e-03,\n",
      "         -1.1107e-02,  1.4010e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight \n",
      "value: tensor([[ 0.0040, -0.0099,  0.0033,  ..., -0.0025,  0.0007, -0.0004],\n",
      "        [ 0.0004, -0.0006, -0.0065,  ..., -0.0029, -0.0021,  0.0016],\n",
      "        [ 0.0025, -0.0067,  0.0107,  ...,  0.0027, -0.0071,  0.0042],\n",
      "        ...,\n",
      "        [ 0.0012, -0.0050, -0.0049,  ..., -0.0022, -0.0007,  0.0066],\n",
      "        [-0.0026, -0.0010, -0.0018,  ..., -0.0004, -0.0051, -0.0039],\n",
      "        [-0.0039,  0.0027, -0.0006,  ...,  0.0033, -0.0069,  0.0047]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight \n",
      "value: tensor([[-0.0062, -0.0070, -0.0062,  ...,  0.0018,  0.0031, -0.0113],\n",
      "        [-0.0027, -0.0073,  0.0074,  ...,  0.0061, -0.0009, -0.0032],\n",
      "        [ 0.0001,  0.0081,  0.0024,  ..., -0.0063,  0.0026, -0.0005],\n",
      "        ...,\n",
      "        [ 0.0018, -0.0012, -0.0033,  ...,  0.0125, -0.0037,  0.0147],\n",
      "        [-0.0071, -0.0040, -0.0022,  ..., -0.0198,  0.0113, -0.0031],\n",
      "        [ 0.0028, -0.0020,  0.0071,  ...,  0.0063,  0.0045, -0.0091]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight \n",
      "value: tensor([[-1.5912e-02, -1.7244e-02,  2.2039e-02,  ..., -1.7437e-02,\n",
      "          7.0162e-03,  4.5954e-03],\n",
      "        [ 2.6944e-02,  1.1682e-02, -3.3524e-03,  ...,  1.2376e-02,\n",
      "          2.1416e-06,  1.1211e-02],\n",
      "        [-1.6701e-02, -7.6998e-03, -1.8398e-02,  ...,  2.7412e-02,\n",
      "          2.1086e-02, -3.6873e-03],\n",
      "        ...,\n",
      "        [ 1.9433e-02,  4.0974e-02, -4.7504e-02,  ...,  1.1063e-02,\n",
      "          2.5094e-02, -5.4815e-03],\n",
      "        [-6.4011e-03, -1.1360e-02,  2.9026e-03,  ...,  5.0888e-02,\n",
      "          3.5916e-03,  1.9006e-02],\n",
      "        [ 3.2887e-02,  3.3215e-02, -7.8264e-03,  ..., -1.8197e-02,\n",
      "          2.2360e-03, -5.1025e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight \n",
      "value: tensor([[ 2.4530e-02,  5.4597e-03,  4.3652e-03,  ...,  7.6302e-03,\n",
      "          2.0237e-03,  8.2038e-03],\n",
      "        [-1.2212e-02,  5.4459e-04, -7.9642e-03,  ..., -1.4327e-03,\n",
      "         -6.5477e-05, -2.4867e-03],\n",
      "        [-1.1038e-02,  7.2463e-03, -3.0941e-03,  ..., -1.5992e-03,\n",
      "          6.9425e-03, -1.9975e-02],\n",
      "        ...,\n",
      "        [-8.0334e-03, -5.4120e-03,  4.9689e-03,  ..., -1.6479e-02,\n",
      "         -5.5123e-03, -3.1745e-03],\n",
      "        [-2.4546e-03, -4.7437e-03,  7.9488e-04,  ..., -2.9228e-03,\n",
      "         -1.0854e-02,  7.1632e-03],\n",
      "        [-7.6806e-04, -5.2572e-03,  2.9609e-03,  ..., -3.7547e-03,\n",
      "         -1.0311e-03, -1.3879e-04]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight \n",
      "value: tensor([[-0.0172,  0.0068, -0.0014,  ...,  0.0101,  0.0045,  0.0085],\n",
      "        [-0.0131,  0.0033, -0.0145,  ..., -0.0100, -0.0085,  0.0135],\n",
      "        [-0.0065, -0.0062, -0.0157,  ..., -0.0049,  0.0112,  0.0101],\n",
      "        ...,\n",
      "        [-0.0034,  0.0069, -0.0062,  ...,  0.0052, -0.0052, -0.0028],\n",
      "        [ 0.0069,  0.0133, -0.0029,  ...,  0.0081, -0.0081, -0.0126],\n",
      "        [-0.0062,  0.0096, -0.0079,  ..., -0.0193,  0.0225, -0.0070]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight \n",
      "value: tensor([[-0.0163, -0.0523, -0.0055,  ..., -0.0216,  0.0298, -0.0054],\n",
      "        [ 0.0057,  0.0224, -0.0138,  ..., -0.0007, -0.0165,  0.0091],\n",
      "        [ 0.0003,  0.0269,  0.0199,  ..., -0.0119, -0.0055, -0.0389],\n",
      "        ...,\n",
      "        [-0.0140,  0.0289,  0.0150,  ...,  0.0035, -0.0303, -0.0126],\n",
      "        [ 0.0026,  0.0085,  0.0351,  ...,  0.0274, -0.0014, -0.0207],\n",
      "        [ 0.0143,  0.0436, -0.0287,  ...,  0.0254,  0.0008, -0.0100]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.proj_in.lora.down.weight \n",
      "value: tensor([[[[ 0.0049]],\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         [[ 0.0101]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0003]],\n",
      "\n",
      "         [[ 0.0148]],\n",
      "\n",
      "         [[-0.0236]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0181]],\n",
      "\n",
      "         [[-0.0068]],\n",
      "\n",
      "         [[-0.0115]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0104]],\n",
      "\n",
      "         [[-0.0065]],\n",
      "\n",
      "         [[-0.0009]]],\n",
      "\n",
      "\n",
      "        [[[-0.0054]],\n",
      "\n",
      "         [[ 0.0110]],\n",
      "\n",
      "         [[-0.0008]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0153]],\n",
      "\n",
      "         [[ 0.0045]],\n",
      "\n",
      "         [[ 0.0189]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0047]],\n",
      "\n",
      "         [[ 0.0139]],\n",
      "\n",
      "         [[ 0.0008]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0127]],\n",
      "\n",
      "         [[ 0.0020]],\n",
      "\n",
      "         [[-0.0073]]],\n",
      "\n",
      "\n",
      "        [[[-0.0014]],\n",
      "\n",
      "         [[-0.0076]],\n",
      "\n",
      "         [[-0.0209]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0123]],\n",
      "\n",
      "         [[-0.0066]],\n",
      "\n",
      "         [[ 0.0105]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0142]],\n",
      "\n",
      "         [[ 0.0070]],\n",
      "\n",
      "         [[ 0.0049]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0049]],\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         [[ 0.0007]]]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.proj_in.lora.up.weight \n",
      "value: tensor([[[[-4.0847e-03]],\n",
      "\n",
      "         [[-1.0622e-02]],\n",
      "\n",
      "         [[-5.2075e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.6703e-03]],\n",
      "\n",
      "         [[ 8.2085e-03]],\n",
      "\n",
      "         [[ 2.0030e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 8.2769e-03]],\n",
      "\n",
      "         [[ 6.6260e-03]],\n",
      "\n",
      "         [[-1.0670e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4341e-02]],\n",
      "\n",
      "         [[-4.1229e-03]],\n",
      "\n",
      "         [[-1.1380e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.2777e-05]],\n",
      "\n",
      "         [[-1.1589e-02]],\n",
      "\n",
      "         [[-1.0301e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.3485e-03]],\n",
      "\n",
      "         [[ 1.9340e-02]],\n",
      "\n",
      "         [[ 6.8144e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.2270e-03]],\n",
      "\n",
      "         [[ 1.1570e-02]],\n",
      "\n",
      "         [[ 1.2436e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4496e-03]],\n",
      "\n",
      "         [[ 8.6815e-05]],\n",
      "\n",
      "         [[ 1.6021e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4051e-02]],\n",
      "\n",
      "         [[ 1.0768e-02]],\n",
      "\n",
      "         [[-1.4109e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2170e-02]],\n",
      "\n",
      "         [[-1.2420e-02]],\n",
      "\n",
      "         [[ 7.3214e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 9.9667e-03]],\n",
      "\n",
      "         [[-8.6488e-03]],\n",
      "\n",
      "         [[-3.3548e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3546e-02]],\n",
      "\n",
      "         [[-8.7962e-03]],\n",
      "\n",
      "         [[-1.2889e-02]]]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.proj_out.lora.down.weight \n",
      "value: tensor([[[[ 0.0123]],\n",
      "\n",
      "         [[ 0.0386]],\n",
      "\n",
      "         [[-0.0101]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0071]],\n",
      "\n",
      "         [[-0.0175]],\n",
      "\n",
      "         [[-0.0172]]],\n",
      "\n",
      "\n",
      "        [[[-0.0059]],\n",
      "\n",
      "         [[-0.0222]],\n",
      "\n",
      "         [[-0.0258]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0283]],\n",
      "\n",
      "         [[ 0.0142]],\n",
      "\n",
      "         [[ 0.0008]]],\n",
      "\n",
      "\n",
      "        [[[-0.0114]],\n",
      "\n",
      "         [[ 0.0132]],\n",
      "\n",
      "         [[-0.0177]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0279]],\n",
      "\n",
      "         [[-0.0048]],\n",
      "\n",
      "         [[-0.0144]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0132]],\n",
      "\n",
      "         [[-0.0059]],\n",
      "\n",
      "         [[ 0.0199]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0172]],\n",
      "\n",
      "         [[-0.0003]],\n",
      "\n",
      "         [[-0.0008]]],\n",
      "\n",
      "\n",
      "        [[[-0.0014]],\n",
      "\n",
      "         [[ 0.0077]],\n",
      "\n",
      "         [[-0.0273]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0257]],\n",
      "\n",
      "         [[-0.0171]],\n",
      "\n",
      "         [[-0.0087]]],\n",
      "\n",
      "\n",
      "        [[[-0.0017]],\n",
      "\n",
      "         [[ 0.0070]],\n",
      "\n",
      "         [[-0.0034]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0195]],\n",
      "\n",
      "         [[ 0.0136]],\n",
      "\n",
      "         [[-0.0056]]]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.proj_out.lora.up.weight \n",
      "value: tensor([[[[ 1.2261e-03]],\n",
      "\n",
      "         [[-5.2648e-03]],\n",
      "\n",
      "         [[-2.8793e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.6925e-03]],\n",
      "\n",
      "         [[-1.0950e-02]],\n",
      "\n",
      "         [[-4.4632e-03]]],\n",
      "\n",
      "\n",
      "        [[[-9.6809e-03]],\n",
      "\n",
      "         [[ 2.5453e-03]],\n",
      "\n",
      "         [[-3.5502e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.9005e-03]],\n",
      "\n",
      "         [[ 6.2793e-03]],\n",
      "\n",
      "         [[-8.1507e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0304e-02]],\n",
      "\n",
      "         [[ 3.6324e-02]],\n",
      "\n",
      "         [[-2.5758e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2901e-02]],\n",
      "\n",
      "         [[-7.5059e-03]],\n",
      "\n",
      "         [[-2.2103e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.1133e-02]],\n",
      "\n",
      "         [[ 1.3668e-02]],\n",
      "\n",
      "         [[ 2.0667e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.0119e-03]],\n",
      "\n",
      "         [[-3.2841e-03]],\n",
      "\n",
      "         [[ 6.5095e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5505e-02]],\n",
      "\n",
      "         [[ 2.3954e-02]],\n",
      "\n",
      "         [[-2.0317e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.9737e-04]],\n",
      "\n",
      "         [[ 5.3483e-03]],\n",
      "\n",
      "         [[-5.0596e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 8.8962e-03]],\n",
      "\n",
      "         [[-4.3976e-03]],\n",
      "\n",
      "         [[ 1.2435e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7200e-02]],\n",
      "\n",
      "         [[ 2.6212e-03]],\n",
      "\n",
      "         [[ 9.4935e-05]]]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight \n",
      "value: tensor([[-0.0039, -0.0126, -0.0002,  ...,  0.0122,  0.0037,  0.0100],\n",
      "        [-0.0011,  0.0045, -0.0028,  ..., -0.0033,  0.0001, -0.0072],\n",
      "        [ 0.0054,  0.0074, -0.0004,  ..., -0.0017, -0.0027,  0.0050],\n",
      "        ...,\n",
      "        [ 0.0103, -0.0017, -0.0118,  ..., -0.0005, -0.0115, -0.0098],\n",
      "        [-0.0106,  0.0013, -0.0060,  ..., -0.0098,  0.0032,  0.0069],\n",
      "        [-0.0014, -0.0046,  0.0049,  ..., -0.0006,  0.0018, -0.0030]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight \n",
      "value: tensor([[-0.0128, -0.0010, -0.0095,  ..., -0.0075,  0.0127, -0.0019],\n",
      "        [-0.0072,  0.0004, -0.0030,  ...,  0.0155,  0.0017, -0.0077],\n",
      "        [-0.0174, -0.0036,  0.0032,  ...,  0.0018, -0.0034, -0.0085],\n",
      "        ...,\n",
      "        [ 0.0050, -0.0058,  0.0022,  ..., -0.0119, -0.0008, -0.0080],\n",
      "        [ 0.0146, -0.0013,  0.0162,  ...,  0.0254, -0.0034, -0.0051],\n",
      "        [ 0.0107,  0.0022,  0.0024,  ...,  0.0087, -0.0071,  0.0031]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight \n",
      "value: tensor([[-0.0199, -0.0027,  0.0019,  ..., -0.0046, -0.0209, -0.0099],\n",
      "        [ 0.0044,  0.0034, -0.0115,  ..., -0.0031, -0.0138, -0.0031],\n",
      "        [ 0.0005, -0.0009, -0.0077,  ..., -0.0091, -0.0172,  0.0102],\n",
      "        ...,\n",
      "        [ 0.0061, -0.0027,  0.0077,  ...,  0.0073,  0.0137, -0.0025],\n",
      "        [-0.0135,  0.0020, -0.0101,  ..., -0.0077,  0.0093, -0.0060],\n",
      "        [-0.0081,  0.0047, -0.0003,  ...,  0.0085,  0.0113, -0.0067]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight \n",
      "value: tensor([[-0.0020,  0.0077, -0.0077,  ...,  0.0100, -0.0068,  0.0073],\n",
      "        [ 0.0021,  0.0057,  0.0114,  ..., -0.0075,  0.0151,  0.0091],\n",
      "        [-0.0071, -0.0081, -0.0136,  ..., -0.0104,  0.0079,  0.0036],\n",
      "        ...,\n",
      "        [ 0.0011, -0.0036, -0.0064,  ..., -0.0004, -0.0018,  0.0105],\n",
      "        [ 0.0093,  0.0091,  0.0125,  ...,  0.0082, -0.0070, -0.0007],\n",
      "        [-0.0103,  0.0012, -0.0005,  ...,  0.0022,  0.0112, -0.0116]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight \n",
      "value: tensor([[-0.0100,  0.0102,  0.0017,  ..., -0.0086, -0.0003, -0.0052],\n",
      "        [-0.0041, -0.0034, -0.0067,  ...,  0.0108,  0.0054,  0.0005],\n",
      "        [ 0.0014,  0.0042, -0.0041,  ...,  0.0041, -0.0031, -0.0031],\n",
      "        ...,\n",
      "        [ 0.0055,  0.0131,  0.0086,  ...,  0.0110,  0.0179, -0.0081],\n",
      "        [ 0.0161, -0.0219, -0.0038,  ...,  0.0126, -0.0061, -0.0037],\n",
      "        [-0.0060, -0.0007, -0.0068,  ..., -0.0031, -0.0054, -0.0040]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight \n",
      "value: tensor([[-8.8012e-03, -4.2580e-03, -2.0101e-03,  ...,  2.0225e-02,\n",
      "          3.3484e-03,  2.2496e-03],\n",
      "        [ 2.4626e-03, -2.6511e-04, -4.5653e-03,  ..., -1.1799e-03,\n",
      "          3.5586e-02, -6.6313e-03],\n",
      "        [-6.1930e-03, -5.1253e-03,  9.8299e-03,  ...,  2.2537e-03,\n",
      "         -2.0756e-02,  5.9021e-03],\n",
      "        ...,\n",
      "        [ 2.2048e-03,  1.9024e-04,  1.6128e-03,  ...,  1.4085e-02,\n",
      "         -2.4762e-03,  5.3547e-03],\n",
      "        [ 7.8111e-03,  4.1567e-03,  6.5918e-03,  ...,  6.7823e-03,\n",
      "         -2.8355e-02,  7.9132e-03],\n",
      "        [-9.8468e-03,  4.4175e-05, -5.8124e-03,  ...,  8.0223e-03,\n",
      "          2.1477e-03,  3.9906e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight \n",
      "value: tensor([[-1.1286e-03, -1.9186e-03, -4.5065e-03,  ..., -5.0765e-03,\n",
      "          8.4993e-03, -1.2718e-02],\n",
      "        [ 1.0490e-02,  3.8087e-03, -1.2537e-03,  ..., -1.5319e-03,\n",
      "          2.5660e-03,  7.9406e-03],\n",
      "        [-8.6000e-03, -2.7793e-03,  6.8238e-03,  ...,  8.7356e-04,\n",
      "         -1.0279e-02,  6.2888e-03],\n",
      "        ...,\n",
      "        [ 5.2233e-03,  6.5116e-03, -2.5802e-04,  ...,  4.1767e-05,\n",
      "         -7.2930e-03,  4.5953e-03],\n",
      "        [-6.4998e-03, -3.3907e-03,  5.9840e-03,  ..., -1.0550e-02,\n",
      "         -2.1523e-03,  4.8692e-04],\n",
      "        [ 5.5926e-03,  1.0109e-02, -2.9815e-04,  ...,  2.4436e-02,\n",
      "          3.9225e-03,  2.0875e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight \n",
      "value: tensor([[-1.3521e-02, -5.2566e-03,  8.2834e-05,  ...,  3.9700e-03,\n",
      "         -2.4714e-02,  5.6425e-03],\n",
      "        [ 5.3647e-03, -3.9780e-03,  1.1363e-03,  ..., -2.6274e-03,\n",
      "          1.1859e-02, -1.5207e-03],\n",
      "        [-4.6608e-03,  7.6118e-03, -8.2130e-03,  ...,  8.3805e-03,\n",
      "          5.7905e-03, -1.4018e-03],\n",
      "        ...,\n",
      "        [-2.8080e-03, -5.2501e-03,  1.5291e-03,  ..., -2.8932e-03,\n",
      "          1.0336e-02, -8.6514e-03],\n",
      "        [-3.0098e-03,  3.7082e-03, -1.8333e-02,  ..., -5.5763e-03,\n",
      "         -1.6010e-03,  8.5177e-03],\n",
      "        [-7.1441e-03,  5.8024e-03, -1.6728e-02,  ..., -6.7664e-03,\n",
      "          4.5467e-03,  8.1397e-04]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight \n",
      "value: tensor([[-9.9765e-04,  1.3713e-03,  3.8151e-04,  ...,  1.7710e-03,\n",
      "         -4.8597e-03, -4.9578e-03],\n",
      "        [-2.8809e-05,  5.0503e-03,  4.0629e-03,  ...,  1.7543e-03,\n",
      "         -6.4819e-03, -3.0942e-03],\n",
      "        [ 3.1448e-03,  1.0743e-02,  6.2702e-03,  ...,  6.2932e-03,\n",
      "          4.0182e-03,  3.2449e-03],\n",
      "        ...,\n",
      "        [ 8.0086e-03,  5.5088e-04,  2.3261e-03,  ..., -7.5966e-04,\n",
      "          5.4330e-04,  1.6441e-03],\n",
      "        [ 1.1249e-03, -7.0019e-03, -4.9945e-03,  ...,  8.4916e-03,\n",
      "         -1.2169e-02, -2.4447e-03],\n",
      "        [-6.8951e-03, -5.5408e-04,  5.0151e-03,  ..., -9.1160e-03,\n",
      "         -4.8214e-03,  3.2029e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight \n",
      "value: tensor([[-1.7103e-03,  3.5235e-03,  3.7541e-03,  ...,  2.4146e-03,\n",
      "         -1.7023e-03,  1.2110e-03],\n",
      "        [ 2.2403e-05,  1.0194e-03,  3.8790e-03,  ...,  5.6143e-03,\n",
      "          1.1421e-03,  2.2163e-03],\n",
      "        [ 3.0510e-03, -6.8961e-04,  1.2214e-03,  ...,  1.8595e-03,\n",
      "         -8.1495e-04,  3.4779e-03],\n",
      "        ...,\n",
      "        [ 4.7084e-05,  6.1929e-05, -4.0608e-03,  ...,  1.6322e-03,\n",
      "          3.6949e-03,  5.6440e-04],\n",
      "        [-1.0823e-03, -8.9679e-04, -2.5372e-03,  ..., -2.7243e-03,\n",
      "          1.2713e-03,  1.8942e-03],\n",
      "        [-9.8324e-04, -2.6609e-03, -4.1390e-03,  ..., -1.8792e-03,\n",
      "          3.5590e-03, -5.3517e-04]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight \n",
      "value: tensor([[ 0.0058, -0.0018,  0.0017,  ..., -0.0186,  0.0037,  0.0013],\n",
      "        [-0.0039,  0.0002, -0.0016,  ...,  0.0064,  0.0033,  0.0064],\n",
      "        [ 0.0009,  0.0100,  0.0092,  ...,  0.0091,  0.0053, -0.0004],\n",
      "        ...,\n",
      "        [-0.0036,  0.0039, -0.0021,  ...,  0.0016, -0.0051, -0.0005],\n",
      "        [ 0.0063,  0.0047,  0.0091,  ..., -0.0122, -0.0104,  0.0013],\n",
      "        [ 0.0054,  0.0059,  0.0002,  ..., -0.0065, -0.0001, -0.0035]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight \n",
      "value: tensor([[ 1.5019e-02,  3.7960e-03,  6.8748e-03,  ..., -8.9889e-06,\n",
      "         -2.9306e-03, -2.2784e-03],\n",
      "        [ 1.0662e-02, -5.0301e-03, -4.5806e-03,  ...,  6.6586e-03,\n",
      "          5.2075e-04,  1.1672e-03],\n",
      "        [ 6.1147e-03, -4.1000e-04, -1.3508e-02,  ..., -3.2415e-03,\n",
      "         -1.6054e-03,  6.0155e-03],\n",
      "        ...,\n",
      "        [-9.3519e-03,  5.2042e-03,  6.8908e-04,  ..., -5.6371e-04,\n",
      "          2.2591e-03,  5.0544e-03],\n",
      "        [ 7.3921e-03,  1.7795e-03, -1.4213e-02,  ...,  3.8239e-03,\n",
      "          5.4411e-03, -4.3132e-03],\n",
      "        [-9.3384e-03,  2.8627e-03,  1.8107e-03,  ...,  5.7946e-04,\n",
      "          6.0392e-03, -2.4269e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight \n",
      "value: tensor([[-4.7563e-03,  5.2764e-03,  3.6010e-03,  ..., -1.3188e-03,\n",
      "          3.6002e-03,  6.4457e-03],\n",
      "        [ 9.2268e-03, -1.4210e-03, -1.0803e-03,  ...,  6.8545e-04,\n",
      "          3.8015e-03,  4.5886e-03],\n",
      "        [ 2.2301e-04,  5.5174e-03,  1.3203e-03,  ..., -8.0729e-05,\n",
      "         -2.1622e-03,  4.2405e-03],\n",
      "        ...,\n",
      "        [-2.2647e-03, -1.5569e-03,  6.5047e-03,  ...,  2.0097e-03,\n",
      "         -5.0766e-03,  3.2203e-03],\n",
      "        [-6.1078e-03,  5.3149e-04, -2.2450e-03,  ..., -8.4134e-03,\n",
      "         -7.6992e-03,  8.9691e-03],\n",
      "        [ 2.3582e-03,  2.7411e-03,  1.0020e-04,  ...,  2.8937e-03,\n",
      "          2.5406e-04, -4.7074e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight \n",
      "value: tensor([[-5.5240e-03, -1.3351e-03,  2.1352e-03,  ..., -4.3374e-03,\n",
      "         -3.2530e-03, -8.3595e-05],\n",
      "        [-1.1267e-03,  4.7684e-05,  5.2374e-03,  ..., -1.0360e-03,\n",
      "          6.5220e-04,  1.3042e-03],\n",
      "        [-4.3942e-03, -2.6190e-03, -5.1245e-03,  ..., -2.9900e-04,\n",
      "          6.4661e-03,  4.8695e-04],\n",
      "        ...,\n",
      "        [ 3.1249e-03, -3.3106e-04, -1.4800e-03,  ..., -3.6560e-03,\n",
      "         -4.7462e-03, -4.2050e-05],\n",
      "        [ 7.4008e-03,  7.4791e-04, -1.4430e-03,  ...,  3.6129e-03,\n",
      "         -9.4434e-03, -1.2908e-03],\n",
      "        [-4.6113e-03,  1.6097e-03,  1.3643e-03,  ..., -1.4854e-03,\n",
      "          2.6099e-03, -6.1185e-04]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight \n",
      "value: tensor([[ 2.0323e-03,  1.4261e-03,  2.8369e-03,  ..., -5.7418e-04,\n",
      "          6.0066e-03,  1.1513e-03],\n",
      "        [ 2.3499e-03,  5.1492e-03,  2.5631e-03,  ..., -2.6918e-03,\n",
      "         -4.3412e-03, -5.3235e-03],\n",
      "        [ 7.8232e-04,  1.8243e-03, -2.1706e-04,  ...,  3.0165e-03,\n",
      "          9.6578e-04,  4.5765e-03],\n",
      "        ...,\n",
      "        [ 5.0031e-04, -7.0102e-04,  2.4662e-04,  ..., -3.9989e-03,\n",
      "          2.8302e-03,  4.2315e-03],\n",
      "        [-4.4906e-03, -1.9888e-02, -5.3993e-03,  ..., -5.3847e-04,\n",
      "         -5.5773e-03, -3.7414e-03],\n",
      "        [-3.5293e-03,  6.2545e-03,  6.5097e-03,  ...,  2.1040e-05,\n",
      "          1.0786e-03,  4.7820e-04]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight \n",
      "value: tensor([[ 3.1101e-03,  1.0766e-03,  5.3525e-04,  ..., -8.3977e-05,\n",
      "         -3.3776e-03, -2.1481e-03],\n",
      "        [ 2.7015e-03, -2.0734e-03,  6.2003e-03,  ...,  3.1092e-03,\n",
      "          7.7014e-03, -7.9213e-03],\n",
      "        [-2.1490e-03, -2.4573e-03,  5.8028e-03,  ...,  3.9264e-03,\n",
      "         -1.4949e-03,  5.1256e-03],\n",
      "        ...,\n",
      "        [-1.0644e-03,  2.9242e-03,  2.0324e-03,  ...,  9.2029e-03,\n",
      "         -1.8442e-03,  6.7368e-03],\n",
      "        [ 5.3472e-03, -1.2216e-03,  9.7018e-03,  ..., -4.0042e-04,\n",
      "         -6.2596e-03, -6.2586e-03],\n",
      "        [ 2.4485e-03, -1.2513e-02,  1.9024e-03,  ..., -3.5470e-03,\n",
      "         -3.6130e-03,  1.5950e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.down.weight \n",
      "value: tensor([[ 0.0019,  0.0208,  0.0014,  ...,  0.0262,  0.0128, -0.0126],\n",
      "        [-0.0073,  0.0235,  0.0060,  ..., -0.0012,  0.0087, -0.0084],\n",
      "        [-0.0061,  0.0429,  0.0099,  ...,  0.0225,  0.0218, -0.0241],\n",
      "        ...,\n",
      "        [-0.0193, -0.0038, -0.0043,  ..., -0.0041, -0.0040,  0.0212],\n",
      "        [-0.0115, -0.0033, -0.0049,  ...,  0.0199,  0.0051, -0.0240],\n",
      "        [ 0.0218,  0.0063,  0.0065,  ...,  0.0088,  0.0042, -0.0099]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.up.weight \n",
      "value: tensor([[-7.5227e-03,  9.8086e-03, -1.9822e-02,  ..., -6.6350e-03,\n",
      "         -4.1031e-03, -8.5627e-03],\n",
      "        [-7.6180e-03,  7.4284e-03, -7.0328e-03,  ...,  9.4195e-04,\n",
      "         -4.4491e-03, -4.7140e-04],\n",
      "        [-4.1146e-03,  1.7523e-03,  3.3409e-03,  ..., -4.1162e-03,\n",
      "         -2.3639e-03, -4.0591e-03],\n",
      "        ...,\n",
      "        [ 1.3876e-02, -5.0067e-05, -1.8590e-03,  ..., -2.6007e-05,\n",
      "         -1.4032e-04,  3.9330e-03],\n",
      "        [-6.7881e-02,  1.5841e-02,  1.1399e-02,  ..., -4.9545e-02,\n",
      "         -9.1475e-02, -9.6641e-02],\n",
      "        [ 2.9681e-03, -1.4588e-03,  1.9327e-02,  ..., -4.9692e-03,\n",
      "         -6.0627e-03, -4.8431e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.lora.down.weight \n",
      "value: tensor([[-3.3142e-03, -1.5520e-02,  1.0201e-02,  ..., -7.4870e-03,\n",
      "         -3.5995e-02,  3.5241e-03],\n",
      "        [ 1.4842e-02,  4.8750e-03,  1.0931e-02,  ..., -8.5630e-03,\n",
      "          2.1292e-02,  9.0216e-04],\n",
      "        [ 4.5444e-03,  1.3816e-02,  7.2487e-03,  ..., -1.3947e-02,\n",
      "         -1.0828e-02, -1.6087e-02],\n",
      "        ...,\n",
      "        [ 3.4939e-03,  1.4042e-02,  3.5390e-03,  ..., -9.5049e-03,\n",
      "          8.6958e-02,  6.2003e-04],\n",
      "        [ 1.0521e-02, -7.7420e-03,  7.4751e-03,  ..., -7.6708e-03,\n",
      "         -6.1136e-02,  5.0678e-03],\n",
      "        [ 1.4614e-02, -3.6038e-05,  7.1813e-03,  ..., -1.4676e-02,\n",
      "         -2.3704e-02, -1.5242e-03]])\n",
      "\n",
      "key: unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.lora.up.weight \n",
      "value: tensor([[-0.0193, -0.0104,  0.0173,  ...,  0.0071,  0.0044, -0.0156],\n",
      "        [-0.0103,  0.0104, -0.0112,  ..., -0.0109,  0.0127,  0.0165],\n",
      "        [-0.0181, -0.0100,  0.0020,  ..., -0.0014, -0.0105, -0.0020],\n",
      "        ...,\n",
      "        [-0.0001, -0.0012, -0.0254,  ...,  0.0185, -0.0032,  0.0088],\n",
      "        [ 0.0094,  0.0025, -0.0074,  ..., -0.0040,  0.0256,  0.0022],\n",
      "        [-0.0012, -0.0074, -0.0015,  ...,  0.0025, -0.0141,  0.0168]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.proj_in.lora.down.weight \n",
      "value: tensor([[[[ 0.0031]],\n",
      "\n",
      "         [[-0.0035]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0042]],\n",
      "\n",
      "         [[ 0.0055]],\n",
      "\n",
      "         [[ 0.0082]]],\n",
      "\n",
      "\n",
      "        [[[-0.0063]],\n",
      "\n",
      "         [[-0.0198]],\n",
      "\n",
      "         [[-0.0049]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0091]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         [[ 0.0103]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0099]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0099]],\n",
      "\n",
      "         [[ 0.0052]],\n",
      "\n",
      "         [[-0.0082]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0007]],\n",
      "\n",
      "         [[ 0.0086]],\n",
      "\n",
      "         [[-0.0097]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0040]],\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         [[-0.0024]]],\n",
      "\n",
      "\n",
      "        [[[-0.0048]],\n",
      "\n",
      "         [[-0.0133]],\n",
      "\n",
      "         [[-0.0078]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0117]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[-0.0063]]],\n",
      "\n",
      "\n",
      "        [[[-0.0010]],\n",
      "\n",
      "         [[ 0.0027]],\n",
      "\n",
      "         [[-0.0049]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[-0.0093]],\n",
      "\n",
      "         [[ 0.0174]]]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.proj_in.lora.up.weight \n",
      "value: tensor([[[[ 1.9808e-03]],\n",
      "\n",
      "         [[-1.0490e-04]],\n",
      "\n",
      "         [[-5.2898e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5450e-03]],\n",
      "\n",
      "         [[ 8.0303e-03]],\n",
      "\n",
      "         [[ 6.2732e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 3.2847e-03]],\n",
      "\n",
      "         [[ 3.3707e-03]],\n",
      "\n",
      "         [[ 1.0252e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.7739e-03]],\n",
      "\n",
      "         [[-6.7410e-03]],\n",
      "\n",
      "         [[-1.6066e-03]]],\n",
      "\n",
      "\n",
      "        [[[-5.7984e-03]],\n",
      "\n",
      "         [[ 2.0624e-03]],\n",
      "\n",
      "         [[ 1.1180e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.8924e-04]],\n",
      "\n",
      "         [[-2.4905e-03]],\n",
      "\n",
      "         [[-6.4852e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-7.2571e-05]],\n",
      "\n",
      "         [[-2.4276e-03]],\n",
      "\n",
      "         [[ 8.2450e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.5758e-03]],\n",
      "\n",
      "         [[-5.4426e-03]],\n",
      "\n",
      "         [[ 4.8368e-05]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5768e-04]],\n",
      "\n",
      "         [[ 7.3250e-03]],\n",
      "\n",
      "         [[ 6.1540e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.9346e-03]],\n",
      "\n",
      "         [[ 1.3081e-02]],\n",
      "\n",
      "         [[ 3.5773e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2348e-02]],\n",
      "\n",
      "         [[-1.4097e-04]],\n",
      "\n",
      "         [[ 6.1836e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0348e-03]],\n",
      "\n",
      "         [[-5.6561e-03]],\n",
      "\n",
      "         [[ 7.8694e-03]]]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.proj_out.lora.down.weight \n",
      "value: tensor([[[[ 0.0007]],\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[ 0.0048]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0073]],\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         [[-0.0038]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0051]],\n",
      "\n",
      "         [[ 0.0042]],\n",
      "\n",
      "         [[-0.0173]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0187]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[ 0.0121]]],\n",
      "\n",
      "\n",
      "        [[[-0.0082]],\n",
      "\n",
      "         [[ 0.0153]],\n",
      "\n",
      "         [[ 0.0073]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0037]],\n",
      "\n",
      "         [[-0.0001]],\n",
      "\n",
      "         [[-0.0050]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0203]],\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0132]],\n",
      "\n",
      "         [[-0.0067]],\n",
      "\n",
      "         [[ 0.0153]]],\n",
      "\n",
      "\n",
      "        [[[-0.0037]],\n",
      "\n",
      "         [[-0.0298]],\n",
      "\n",
      "         [[-0.0180]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0067]],\n",
      "\n",
      "         [[-0.0039]],\n",
      "\n",
      "         [[ 0.0095]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0009]],\n",
      "\n",
      "         [[ 0.0133]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[ 0.0078]],\n",
      "\n",
      "         [[ 0.0001]]]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.proj_out.lora.up.weight \n",
      "value: tensor([[[[ 0.0105]],\n",
      "\n",
      "         [[ 0.0046]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0048]],\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[-0.0121]]],\n",
      "\n",
      "\n",
      "        [[[-0.0106]],\n",
      "\n",
      "         [[-0.0098]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[ 0.0047]]],\n",
      "\n",
      "\n",
      "        [[[-0.0044]],\n",
      "\n",
      "         [[ 0.0147]],\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[ 0.0036]],\n",
      "\n",
      "         [[-0.0015]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0005]],\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         [[-0.0079]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0040]],\n",
      "\n",
      "         [[ 0.0100]],\n",
      "\n",
      "         [[-0.0042]]],\n",
      "\n",
      "\n",
      "        [[[-0.0023]],\n",
      "\n",
      "         [[-0.0035]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[ 0.0070]],\n",
      "\n",
      "         [[-0.0044]]],\n",
      "\n",
      "\n",
      "        [[[-0.0021]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[-0.0030]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0054]],\n",
      "\n",
      "         [[ 0.0032]],\n",
      "\n",
      "         [[ 0.0063]]]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight \n",
      "value: tensor([[ 0.0014,  0.0086,  0.0043,  ...,  0.0026,  0.0092, -0.0059],\n",
      "        [-0.0005, -0.0046,  0.0038,  ...,  0.0020, -0.0054, -0.0084],\n",
      "        [ 0.0096, -0.0050,  0.0041,  ..., -0.0071, -0.0025, -0.0026],\n",
      "        ...,\n",
      "        [ 0.0037, -0.0034,  0.0073,  ..., -0.0045,  0.0009,  0.0062],\n",
      "        [ 0.0043,  0.0047, -0.0073,  ...,  0.0003,  0.0062, -0.0004],\n",
      "        [-0.0021,  0.0041, -0.0119,  ...,  0.0044,  0.0049,  0.0015]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight \n",
      "value: tensor([[-8.1533e-03,  2.6005e-05,  1.3727e-03,  ...,  7.8287e-03,\n",
      "          1.7004e-03,  3.1319e-03],\n",
      "        [ 8.3804e-03,  1.9652e-03, -4.4545e-03,  ...,  6.2585e-04,\n",
      "          1.3353e-02,  1.7682e-04],\n",
      "        [ 4.8141e-03,  1.4939e-03, -1.0258e-02,  ...,  5.9062e-03,\n",
      "         -8.5100e-03, -1.4654e-03],\n",
      "        ...,\n",
      "        [ 3.6862e-03, -1.5105e-03,  3.2238e-03,  ..., -6.9192e-03,\n",
      "          4.6648e-04,  3.9273e-03],\n",
      "        [ 8.5383e-03, -8.0796e-03, -7.4369e-04,  ..., -2.6945e-03,\n",
      "         -2.4115e-03,  3.8117e-04],\n",
      "        [-4.8097e-04, -5.4189e-03,  3.6823e-03,  ...,  6.5656e-03,\n",
      "          6.2739e-03, -4.0685e-03]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight \n",
      "value: tensor([[-0.0008, -0.0043,  0.0086,  ...,  0.0172, -0.0092,  0.0012],\n",
      "        [-0.0026,  0.0029, -0.0024,  ...,  0.0011, -0.0005,  0.0109],\n",
      "        [ 0.0056, -0.0026, -0.0001,  ..., -0.0061,  0.0008,  0.0050],\n",
      "        ...,\n",
      "        [ 0.0063,  0.0064, -0.0050,  ...,  0.0092, -0.0005, -0.0049],\n",
      "        [ 0.0047,  0.0010,  0.0054,  ..., -0.0028, -0.0010, -0.0013],\n",
      "        [ 0.0029, -0.0028,  0.0031,  ..., -0.0095,  0.0062,  0.0052]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight \n",
      "value: tensor([[ 0.0069,  0.0024, -0.0080,  ..., -0.0010, -0.0024, -0.0104],\n",
      "        [-0.0040,  0.0081, -0.0008,  ..., -0.0064, -0.0038,  0.0050],\n",
      "        [ 0.0066,  0.0065, -0.0070,  ...,  0.0114,  0.0039,  0.0085],\n",
      "        ...,\n",
      "        [ 0.0048, -0.0057,  0.0061,  ...,  0.0066, -0.0141, -0.0041],\n",
      "        [ 0.0014,  0.0008, -0.0048,  ...,  0.0031,  0.0042, -0.0029],\n",
      "        [ 0.0086,  0.0077, -0.0051,  ...,  0.0040, -0.0018,  0.0059]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight \n",
      "value: tensor([[-0.0061,  0.0152, -0.0055,  ..., -0.0021, -0.0001,  0.0080],\n",
      "        [ 0.0008, -0.0068,  0.0007,  ...,  0.0016, -0.0004,  0.0017],\n",
      "        [ 0.0020,  0.0022,  0.0009,  ...,  0.0019,  0.0024, -0.0116],\n",
      "        ...,\n",
      "        [-0.0063,  0.0034,  0.0014,  ..., -0.0006, -0.0010, -0.0017],\n",
      "        [ 0.0069,  0.0040,  0.0004,  ..., -0.0147,  0.0052,  0.0049],\n",
      "        [ 0.0042,  0.0007, -0.0068,  ...,  0.0039,  0.0117, -0.0034]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight \n",
      "value: tensor([[-0.0004, -0.0016,  0.0015,  ..., -0.0032,  0.0144,  0.0028],\n",
      "        [ 0.0008,  0.0011, -0.0030,  ..., -0.0127, -0.0048,  0.0064],\n",
      "        [ 0.0031, -0.0088, -0.0005,  ...,  0.0084,  0.0028,  0.0002],\n",
      "        ...,\n",
      "        [ 0.0073, -0.0071, -0.0051,  ..., -0.0033,  0.0020,  0.0067],\n",
      "        [-0.0024, -0.0034,  0.0022,  ..., -0.0069,  0.0019,  0.0029],\n",
      "        [-0.0002, -0.0041, -0.0091,  ..., -0.0018, -0.0064, -0.0022]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight \n",
      "value: tensor([[ 0.0049,  0.0011,  0.0068,  ...,  0.0022, -0.0018, -0.0042],\n",
      "        [-0.0025,  0.0006,  0.0006,  ..., -0.0013,  0.0060, -0.0007],\n",
      "        [ 0.0010,  0.0077,  0.0020,  ...,  0.0025,  0.0018, -0.0036],\n",
      "        ...,\n",
      "        [-0.0055, -0.0070, -0.0013,  ...,  0.0101, -0.0140,  0.0064],\n",
      "        [-0.0127,  0.0057,  0.0038,  ..., -0.0035,  0.0032, -0.0063],\n",
      "        [-0.0001,  0.0017,  0.0031,  ...,  0.0067, -0.0084,  0.0003]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight \n",
      "value: tensor([[-0.0113, -0.0008, -0.0030,  ..., -0.0050,  0.0106, -0.0056],\n",
      "        [ 0.0065,  0.0028,  0.0021,  ..., -0.0022,  0.0040,  0.0010],\n",
      "        [ 0.0017,  0.0052,  0.0001,  ..., -0.0036, -0.0003,  0.0043],\n",
      "        ...,\n",
      "        [-0.0002, -0.0014, -0.0053,  ...,  0.0042,  0.0066,  0.0005],\n",
      "        [-0.0065,  0.0058, -0.0041,  ..., -0.0008, -0.0023, -0.0045],\n",
      "        [-0.0012,  0.0062, -0.0075,  ..., -0.0119,  0.0015,  0.0044]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight \n",
      "value: tensor([[-0.0117, -0.0015,  0.0044,  ..., -0.0041,  0.0008,  0.0014],\n",
      "        [-0.0077,  0.0048, -0.0025,  ...,  0.0005, -0.0035, -0.0008],\n",
      "        [ 0.0020, -0.0004, -0.0017,  ...,  0.0005,  0.0016,  0.0028],\n",
      "        ...,\n",
      "        [-0.0065, -0.0036, -0.0037,  ...,  0.0025,  0.0011,  0.0031],\n",
      "        [ 0.0008,  0.0025,  0.0039,  ..., -0.0036, -0.0009,  0.0018],\n",
      "        [-0.0037,  0.0005,  0.0019,  ...,  0.0020, -0.0042, -0.0021]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight \n",
      "value: tensor([[-1.4668e-04,  9.3848e-04,  4.7011e-04,  ...,  8.0149e-06,\n",
      "          1.7784e-03,  3.2960e-04],\n",
      "        [-1.0430e-04, -4.0805e-04,  1.9878e-04,  ...,  1.8932e-04,\n",
      "         -1.7919e-03, -5.9293e-04],\n",
      "        [-1.3573e-03,  1.6914e-04, -1.3577e-04,  ...,  4.0135e-04,\n",
      "          5.5383e-04,  9.6047e-04],\n",
      "        ...,\n",
      "        [-3.0584e-04,  1.0233e-03, -5.8246e-05,  ...,  5.1365e-04,\n",
      "          1.9298e-03, -5.1543e-05],\n",
      "        [ 6.1624e-04, -1.1869e-03,  1.9935e-04,  ...,  2.5650e-04,\n",
      "         -1.1196e-03, -2.6923e-04],\n",
      "        [ 1.4160e-04,  5.2983e-04, -7.9546e-04,  ...,  7.6105e-04,\n",
      "         -3.5495e-03, -5.1207e-04]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight \n",
      "value: tensor([[-0.0012, -0.0087,  0.0011,  ...,  0.0001,  0.0032, -0.0014],\n",
      "        [-0.0038, -0.0023,  0.0008,  ..., -0.0037, -0.0011, -0.0017],\n",
      "        [ 0.0016,  0.0006,  0.0015,  ...,  0.0032,  0.0036, -0.0017],\n",
      "        ...,\n",
      "        [-0.0027,  0.0023,  0.0028,  ...,  0.0007,  0.0007, -0.0039],\n",
      "        [ 0.0061,  0.0011,  0.0049,  ..., -0.0035,  0.0003, -0.0014],\n",
      "        [-0.0007, -0.0084,  0.0020,  ...,  0.0033, -0.0027,  0.0011]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight \n",
      "value: tensor([[ 2.4427e-03, -3.2469e-03,  1.9743e-03,  ...,  1.8834e-03,\n",
      "         -3.1304e-03,  4.7226e-04],\n",
      "        [-3.0650e-03,  3.6852e-03,  2.6078e-03,  ...,  7.7884e-04,\n",
      "          4.5360e-03,  2.9185e-04],\n",
      "        [-1.4074e-03,  4.0814e-03, -3.9619e-03,  ...,  1.3674e-03,\n",
      "          2.1401e-03,  1.3935e-03],\n",
      "        ...,\n",
      "        [-4.1373e-03,  3.0541e-03, -5.8452e-03,  ..., -4.8170e-04,\n",
      "         -3.7488e-03, -3.2517e-04],\n",
      "        [-3.7510e-03,  5.1559e-03,  7.0505e-04,  ..., -3.7212e-03,\n",
      "         -3.9467e-03, -3.8433e-03],\n",
      "        [ 4.6835e-04,  3.2791e-03, -1.4483e-06,  ..., -2.7418e-03,\n",
      "         -1.1646e-03,  2.4479e-03]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight \n",
      "value: tensor([[ 6.8443e-03, -2.6846e-04,  2.0795e-03,  ...,  3.4033e-03,\n",
      "         -3.3275e-05, -2.8010e-04],\n",
      "        [-1.2721e-03,  1.5286e-04, -2.6774e-03,  ..., -5.6438e-04,\n",
      "          2.8275e-03, -1.8134e-03],\n",
      "        [ 3.6602e-03, -1.0329e-03, -2.5796e-03,  ...,  4.6174e-03,\n",
      "          4.5791e-03,  4.9936e-04],\n",
      "        ...,\n",
      "        [ 4.6165e-03, -2.7128e-03,  6.0797e-03,  ...,  3.1005e-04,\n",
      "          2.2477e-03,  3.0751e-03],\n",
      "        [-5.4085e-03, -7.5358e-05, -2.5386e-03,  ..., -3.5754e-03,\n",
      "          3.9988e-03, -1.9986e-03],\n",
      "        [-9.2795e-04,  6.3195e-03, -7.9569e-04,  ..., -2.7683e-03,\n",
      "         -3.2273e-03,  2.1488e-03]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight \n",
      "value: tensor([[-1.8611e-03, -4.8055e-04,  5.7555e-04,  ..., -7.1805e-04,\n",
      "         -1.9734e-03, -3.3067e-04],\n",
      "        [-8.7081e-04,  5.7245e-04, -3.2252e-04,  ...,  4.0710e-04,\n",
      "          1.1171e-03,  4.3185e-04],\n",
      "        [-3.8598e-04, -1.7762e-04,  3.7802e-05,  ..., -1.1212e-04,\n",
      "         -3.6564e-03,  4.9143e-04],\n",
      "        ...,\n",
      "        [ 1.0891e-04, -1.7083e-04, -2.3224e-04,  ...,  4.8952e-04,\n",
      "         -9.1130e-04,  7.3670e-04],\n",
      "        [-7.9848e-05,  6.1596e-04, -7.9457e-04,  ...,  5.1024e-04,\n",
      "          1.7271e-03,  3.6210e-04],\n",
      "        [-5.6249e-04, -4.1258e-04, -6.7102e-04,  ..., -6.5279e-04,\n",
      "          1.7527e-03,  5.1182e-04]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight \n",
      "value: tensor([[ 3.7113e-03, -4.5506e-03, -3.0510e-03,  ..., -1.7233e-03,\n",
      "          1.2746e-03, -4.5322e-03],\n",
      "        [-3.0706e-03,  2.1830e-04,  3.7680e-03,  ..., -2.4835e-03,\n",
      "         -5.9119e-04,  3.7952e-03],\n",
      "        [ 6.3287e-04,  1.9155e-03, -2.9813e-04,  ...,  2.5652e-03,\n",
      "          1.9025e-03,  1.9952e-03],\n",
      "        ...,\n",
      "        [-6.9585e-03,  1.2880e-03, -6.7339e-03,  ...,  4.5180e-03,\n",
      "         -9.1165e-05,  6.0511e-04],\n",
      "        [ 1.6692e-03,  5.5468e-03, -2.7106e-03,  ...,  5.0704e-03,\n",
      "         -5.7560e-03, -9.1147e-03],\n",
      "        [ 4.2514e-03, -2.5800e-03, -5.7186e-04,  ...,  9.8344e-04,\n",
      "         -2.5988e-03,  1.1336e-03]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight \n",
      "value: tensor([[ 0.0011, -0.0039,  0.0018,  ...,  0.0049,  0.0046, -0.0031],\n",
      "        [ 0.0008, -0.0038,  0.0036,  ..., -0.0114, -0.0016,  0.0006],\n",
      "        [-0.0009,  0.0031, -0.0072,  ...,  0.0029,  0.0029,  0.0053],\n",
      "        ...,\n",
      "        [ 0.0022,  0.0078, -0.0091,  ..., -0.0054,  0.0028,  0.0006],\n",
      "        [-0.0013, -0.0032,  0.0086,  ...,  0.0071,  0.0029,  0.0021],\n",
      "        [-0.0060,  0.0004, -0.0008,  ..., -0.0027,  0.0032, -0.0015]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight \n",
      "value: tensor([[ 0.0068, -0.0060,  0.0002,  ..., -0.0055,  0.0067, -0.0021],\n",
      "        [ 0.0010, -0.0055, -0.0019,  ..., -0.0005, -0.0063, -0.0015],\n",
      "        [ 0.0056,  0.0036,  0.0008,  ...,  0.0042, -0.0032, -0.0056],\n",
      "        ...,\n",
      "        [ 0.0013,  0.0015,  0.0069,  ..., -0.0015, -0.0023, -0.0035],\n",
      "        [ 0.0033,  0.0137,  0.0029,  ..., -0.0037, -0.0052, -0.0020],\n",
      "        [-0.0005,  0.0044,  0.0073,  ...,  0.0112, -0.0038,  0.0114]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight \n",
      "value: tensor([[ 0.0107,  0.0041, -0.0024,  ...,  0.0037,  0.0061,  0.0017],\n",
      "        [-0.0015, -0.0030,  0.0019,  ...,  0.0018, -0.0153,  0.0012],\n",
      "        [ 0.0017,  0.0106,  0.0054,  ...,  0.0016, -0.0035, -0.0061],\n",
      "        ...,\n",
      "        [ 0.0032,  0.0098,  0.0014,  ...,  0.0036, -0.0045,  0.0063],\n",
      "        [ 0.0054, -0.0032, -0.0087,  ..., -0.0060,  0.0030,  0.0006],\n",
      "        [ 0.0118,  0.0046,  0.0114,  ...,  0.0071,  0.0111, -0.0083]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight \n",
      "value: tensor([[ 0.0127, -0.0026,  0.0017,  ...,  0.0039, -0.0010,  0.0021],\n",
      "        [ 0.0071, -0.0038,  0.0027,  ...,  0.0047, -0.0121,  0.0139],\n",
      "        [-0.0009,  0.0057,  0.0005,  ...,  0.0016, -0.0002,  0.0017],\n",
      "        ...,\n",
      "        [-0.0018,  0.0107,  0.0079,  ...,  0.0025,  0.0057,  0.0002],\n",
      "        [-0.0052, -0.0168, -0.0057,  ..., -0.0086,  0.0052, -0.0029],\n",
      "        [ 0.0089, -0.0036,  0.0019,  ..., -0.0104, -0.0057, -0.0020]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight \n",
      "value: tensor([[ 2.5552e-03,  8.0036e-03, -2.8387e-03,  ..., -1.5711e-03,\n",
      "          6.0962e-03, -1.0727e-03],\n",
      "        [ 8.0791e-03,  4.4323e-03, -1.5549e-03,  ...,  1.0708e-02,\n",
      "          5.0678e-03,  1.1023e-02],\n",
      "        [ 6.7060e-03,  2.4234e-03, -5.0706e-03,  ...,  3.7482e-03,\n",
      "          4.3543e-03,  1.2713e-03],\n",
      "        ...,\n",
      "        [ 3.6537e-03,  1.0832e-02,  4.7005e-03,  ...,  3.5815e-03,\n",
      "          1.4507e-02, -4.3961e-03],\n",
      "        [-1.8272e-03, -4.1467e-03,  1.4494e-03,  ..., -4.7656e-03,\n",
      "         -2.3520e-03,  5.4510e-03],\n",
      "        [-4.3411e-03,  5.9939e-05,  6.8421e-04,  ..., -3.5674e-03,\n",
      "          1.2288e-03, -6.8232e-03]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.proj_in.lora.down.weight \n",
      "value: tensor([[[[ 0.0047]],\n",
      "\n",
      "         [[-0.0192]],\n",
      "\n",
      "         [[ 0.0075]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0066]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         [[-0.0143]]],\n",
      "\n",
      "\n",
      "        [[[-0.0083]],\n",
      "\n",
      "         [[ 0.0143]],\n",
      "\n",
      "         [[ 0.0021]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0074]],\n",
      "\n",
      "         [[-0.0060]],\n",
      "\n",
      "         [[-0.0029]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0014]],\n",
      "\n",
      "         [[-0.0089]],\n",
      "\n",
      "         [[ 0.0001]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0042]],\n",
      "\n",
      "         [[ 0.0001]],\n",
      "\n",
      "         [[ 0.0054]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0075]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         [[-0.0042]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0017]],\n",
      "\n",
      "         [[ 0.0095]],\n",
      "\n",
      "         [[-0.0104]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0045]],\n",
      "\n",
      "         [[ 0.0134]],\n",
      "\n",
      "         [[ 0.0016]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0007]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0020]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0084]],\n",
      "\n",
      "         [[ 0.0066]],\n",
      "\n",
      "         [[ 0.0007]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0073]],\n",
      "\n",
      "         [[ 0.0055]],\n",
      "\n",
      "         [[ 0.0068]]]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.proj_in.lora.up.weight \n",
      "value: tensor([[[[ 0.0015]],\n",
      "\n",
      "         [[-0.0040]],\n",
      "\n",
      "         [[-0.0070]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0057]],\n",
      "\n",
      "         [[ 0.0058]],\n",
      "\n",
      "         [[ 0.0021]]],\n",
      "\n",
      "\n",
      "        [[[-0.0069]],\n",
      "\n",
      "         [[-0.0056]],\n",
      "\n",
      "         [[ 0.0111]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0003]],\n",
      "\n",
      "         [[-0.0014]],\n",
      "\n",
      "         [[ 0.0049]]],\n",
      "\n",
      "\n",
      "        [[[-0.0036]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         [[-0.0062]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0133]],\n",
      "\n",
      "         [[ 0.0092]],\n",
      "\n",
      "         [[-0.0019]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0010]],\n",
      "\n",
      "         [[-0.0102]],\n",
      "\n",
      "         [[-0.0002]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         [[ 0.0039]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0050]],\n",
      "\n",
      "         [[-0.0021]],\n",
      "\n",
      "         [[ 0.0095]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0073]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[ 0.0166]]],\n",
      "\n",
      "\n",
      "        [[[-0.0091]],\n",
      "\n",
      "         [[ 0.0187]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0075]],\n",
      "\n",
      "         [[ 0.0050]],\n",
      "\n",
      "         [[-0.0038]]]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.proj_out.lora.down.weight \n",
      "value: tensor([[[[-4.2065e-03]],\n",
      "\n",
      "         [[-4.5456e-03]],\n",
      "\n",
      "         [[-7.0698e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0159e-02]],\n",
      "\n",
      "         [[ 9.0844e-03]],\n",
      "\n",
      "         [[ 5.8438e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.1073e-04]],\n",
      "\n",
      "         [[-1.1089e-02]],\n",
      "\n",
      "         [[ 5.2777e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.0090e-03]],\n",
      "\n",
      "         [[ 1.7467e-03]],\n",
      "\n",
      "         [[-1.1903e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 7.2277e-03]],\n",
      "\n",
      "         [[-1.2827e-02]],\n",
      "\n",
      "         [[ 3.0147e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.6392e-04]],\n",
      "\n",
      "         [[ 1.8615e-03]],\n",
      "\n",
      "         [[ 6.2614e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.0745e-03]],\n",
      "\n",
      "         [[ 5.8923e-03]],\n",
      "\n",
      "         [[-3.1314e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4048e-03]],\n",
      "\n",
      "         [[-1.1793e-03]],\n",
      "\n",
      "         [[ 8.7100e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3496e-03]],\n",
      "\n",
      "         [[ 1.4571e-02]],\n",
      "\n",
      "         [[ 1.7123e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4917e-03]],\n",
      "\n",
      "         [[ 1.8461e-03]],\n",
      "\n",
      "         [[ 1.1901e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.9161e-04]],\n",
      "\n",
      "         [[-5.6313e-03]],\n",
      "\n",
      "         [[ 1.2507e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0617e-02]],\n",
      "\n",
      "         [[ 7.6819e-03]],\n",
      "\n",
      "         [[ 7.5939e-03]]]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.proj_out.lora.up.weight \n",
      "value: tensor([[[[-0.0031]],\n",
      "\n",
      "         [[ 0.0021]],\n",
      "\n",
      "         [[ 0.0032]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[-0.0250]],\n",
      "\n",
      "         [[-0.0071]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0024]],\n",
      "\n",
      "         [[ 0.0111]],\n",
      "\n",
      "         [[ 0.0097]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0069]],\n",
      "\n",
      "         [[-0.0230]],\n",
      "\n",
      "         [[-0.0029]]],\n",
      "\n",
      "\n",
      "        [[[-0.0028]],\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         [[ 0.0064]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         [[-0.0001]],\n",
      "\n",
      "         [[ 0.0061]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0044]],\n",
      "\n",
      "         [[-0.0028]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[ 0.0111]],\n",
      "\n",
      "         [[-0.0002]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0041]],\n",
      "\n",
      "         [[-0.0010]],\n",
      "\n",
      "         [[ 0.0073]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[ 0.0054]],\n",
      "\n",
      "         [[ 0.0043]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0103]],\n",
      "\n",
      "         [[ 0.0071]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0022]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         [[ 0.0058]]]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight \n",
      "value: tensor([[-1.4111e-03,  1.0183e-03, -7.1876e-03,  ...,  4.8271e-03,\n",
      "          3.6046e-03,  3.0937e-04],\n",
      "        [-3.3411e-03,  2.5301e-03,  2.1428e-03,  ...,  8.7691e-03,\n",
      "         -3.4918e-03,  3.2339e-03],\n",
      "        [ 1.5802e-03,  1.6941e-03,  8.9483e-04,  ..., -1.5873e-04,\n",
      "         -2.3749e-03, -4.1655e-04],\n",
      "        ...,\n",
      "        [-1.2452e-03, -3.8621e-03,  8.6842e-03,  ..., -3.4809e-03,\n",
      "         -9.6252e-05, -8.5070e-03],\n",
      "        [-9.0805e-03, -1.2638e-03, -1.0089e-03,  ..., -1.0292e-02,\n",
      "          1.3041e-02, -4.0614e-03],\n",
      "        [-6.0249e-04, -3.1321e-03,  1.1865e-03,  ..., -2.7611e-03,\n",
      "         -1.2044e-02,  3.2832e-03]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight \n",
      "value: tensor([[-0.0131, -0.0033, -0.0022,  ..., -0.0023, -0.0010,  0.0064],\n",
      "        [-0.0210,  0.0025, -0.0019,  ..., -0.0070, -0.0049,  0.0028],\n",
      "        [ 0.0037, -0.0041, -0.0014,  ...,  0.0003,  0.0034,  0.0028],\n",
      "        ...,\n",
      "        [-0.0134,  0.0017, -0.0074,  ..., -0.0019, -0.0017,  0.0055],\n",
      "        [ 0.0089,  0.0073,  0.0049,  ..., -0.0029, -0.0071,  0.0046],\n",
      "        [-0.0008,  0.0021,  0.0010,  ...,  0.0011, -0.0135, -0.0014]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight \n",
      "value: tensor([[ 8.3219e-03,  4.7762e-03, -1.7744e-03,  ..., -3.1617e-03,\n",
      "         -6.4551e-03,  1.2127e-02],\n",
      "        [ 1.2967e-03,  5.4560e-03, -1.3484e-02,  ..., -4.9478e-03,\n",
      "         -5.6100e-03, -2.5914e-03],\n",
      "        [-8.8706e-05,  3.4995e-03, -8.6157e-03,  ...,  2.0266e-03,\n",
      "          6.8997e-04, -5.1929e-05],\n",
      "        ...,\n",
      "        [-2.9820e-03, -1.2837e-02, -3.1278e-04,  ...,  4.5070e-03,\n",
      "         -1.3970e-03,  5.3319e-03],\n",
      "        [-8.1368e-03,  5.6375e-03, -1.0452e-03,  ...,  5.4658e-03,\n",
      "          6.1214e-04,  6.0963e-03],\n",
      "        [-6.5497e-04, -2.4523e-03, -3.5207e-03,  ...,  1.6101e-03,\n",
      "          2.6980e-03, -1.1814e-02]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight \n",
      "value: tensor([[ 3.0484e-03, -1.7148e-03, -5.0160e-03,  ...,  3.4344e-03,\n",
      "          5.4638e-03, -6.0302e-03],\n",
      "        [ 4.3335e-03,  5.5627e-03, -1.2835e-02,  ..., -9.2687e-03,\n",
      "         -4.1220e-03, -5.4366e-03],\n",
      "        [-2.3192e-03,  1.2921e-03, -2.4881e-03,  ...,  6.1044e-03,\n",
      "          4.5650e-06, -2.0734e-03],\n",
      "        ...,\n",
      "        [-1.4674e-03, -1.6142e-03, -8.5965e-03,  ...,  1.4465e-02,\n",
      "         -3.6695e-03, -2.8239e-03],\n",
      "        [-5.1644e-05, -5.6616e-03, -6.4942e-03,  ..., -2.8591e-03,\n",
      "          8.9056e-03, -5.1170e-03],\n",
      "        [-5.0713e-03, -2.2004e-04,  4.9203e-03,  ..., -3.9018e-03,\n",
      "          6.8889e-03,  4.2229e-03]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight \n",
      "value: tensor([[ 0.0025,  0.0006,  0.0027,  ...,  0.0033,  0.0035, -0.0084],\n",
      "        [-0.0039,  0.0038,  0.0114,  ..., -0.0026, -0.0015, -0.0010],\n",
      "        [ 0.0034, -0.0061,  0.0024,  ..., -0.0015, -0.0016, -0.0047],\n",
      "        ...,\n",
      "        [ 0.0003, -0.0040, -0.0047,  ..., -0.0033, -0.0067, -0.0005],\n",
      "        [ 0.0048, -0.0070,  0.0086,  ...,  0.0131,  0.0001, -0.0131],\n",
      "        [-0.0001,  0.0040, -0.0015,  ..., -0.0005,  0.0070,  0.0049]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight \n",
      "value: tensor([[-1.4325e-02,  4.1298e-03, -1.4997e-03,  ..., -2.8709e-03,\n",
      "          2.5996e-03, -3.3676e-04],\n",
      "        [-5.0538e-04,  1.1141e-03,  4.4664e-03,  ...,  8.8944e-03,\n",
      "          1.0870e-03,  2.1740e-03],\n",
      "        [ 5.9392e-03, -6.6520e-03,  1.3390e-02,  ...,  6.8909e-03,\n",
      "          5.9717e-03,  5.2671e-03],\n",
      "        ...,\n",
      "        [-1.3177e-02, -1.2515e-03, -9.7220e-05,  ..., -6.8055e-03,\n",
      "         -2.8435e-03, -8.3947e-03],\n",
      "        [-5.1695e-03, -5.0319e-03,  9.6934e-04,  ...,  7.0745e-03,\n",
      "         -8.7720e-03, -4.5946e-03],\n",
      "        [ 2.7885e-04, -2.3679e-03, -5.0826e-03,  ...,  3.2016e-03,\n",
      "         -6.8770e-03,  1.0644e-03]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight \n",
      "value: tensor([[-8.4509e-03, -5.1915e-03, -5.4258e-03,  ...,  1.3602e-03,\n",
      "         -2.6490e-03,  2.8780e-03],\n",
      "        [ 2.3559e-03, -3.5950e-04, -1.4251e-04,  ..., -3.3308e-03,\n",
      "         -3.3819e-04, -1.2844e-03],\n",
      "        [-4.4959e-03,  1.1002e-02, -1.6134e-03,  ..., -2.3160e-03,\n",
      "          4.2679e-03, -2.3320e-03],\n",
      "        ...,\n",
      "        [-5.7696e-03, -1.4337e-03,  3.1026e-03,  ..., -2.3078e-03,\n",
      "          9.8967e-03,  1.7729e-04],\n",
      "        [-3.9944e-03,  2.5320e-03, -3.4283e-03,  ...,  1.1380e-04,\n",
      "         -2.6636e-04,  3.1333e-03],\n",
      "        [ 4.7406e-04,  6.0874e-03, -6.3148e-03,  ...,  8.6547e-05,\n",
      "          1.7489e-03, -1.0890e-02]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight \n",
      "value: tensor([[ 1.8862e-04, -1.5686e-03,  1.1347e-02,  ...,  1.3675e-03,\n",
      "         -8.8817e-03,  1.7772e-03],\n",
      "        [-7.0919e-03, -2.7653e-03, -8.1631e-03,  ..., -6.3662e-03,\n",
      "          4.8003e-03, -1.5892e-06],\n",
      "        [-3.0044e-03,  4.3559e-03,  4.9296e-03,  ..., -9.1521e-03,\n",
      "          6.7515e-03, -3.5758e-03],\n",
      "        ...,\n",
      "        [-4.2915e-03, -6.8056e-03, -4.3166e-03,  ..., -4.5628e-03,\n",
      "         -3.4892e-04, -8.0051e-05],\n",
      "        [-1.4154e-04, -4.7925e-04,  1.0282e-03,  ...,  4.4873e-03,\n",
      "          5.6207e-03,  2.2630e-03],\n",
      "        [ 2.9825e-03, -1.2966e-03,  5.3112e-04,  ..., -2.8682e-03,\n",
      "          3.8911e-04, -5.0516e-04]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight \n",
      "value: tensor([[ 0.0041, -0.0004,  0.0032,  ..., -0.0073,  0.0028, -0.0082],\n",
      "        [ 0.0026, -0.0003, -0.0002,  ...,  0.0009, -0.0022, -0.0061],\n",
      "        [-0.0011, -0.0013, -0.0012,  ..., -0.0018,  0.0018, -0.0021],\n",
      "        ...,\n",
      "        [-0.0009,  0.0054,  0.0033,  ..., -0.0005,  0.0024,  0.0009],\n",
      "        [-0.0031,  0.0047, -0.0027,  ...,  0.0026,  0.0005,  0.0090],\n",
      "        [-0.0077,  0.0009, -0.0030,  ..., -0.0058,  0.0038,  0.0038]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight \n",
      "value: tensor([[-1.3608e-03,  1.1378e-03, -3.1571e-03,  ...,  4.0765e-04,\n",
      "         -8.3166e-04,  3.2410e-03],\n",
      "        [-3.3391e-04,  7.9433e-04,  5.5215e-04,  ..., -2.3727e-04,\n",
      "         -1.1658e-03,  6.9003e-05],\n",
      "        [-1.4893e-03,  2.6056e-04, -1.6581e-03,  ...,  7.5406e-04,\n",
      "          2.5883e-03, -3.2234e-04],\n",
      "        ...,\n",
      "        [ 2.1051e-03, -1.2517e-03,  1.5053e-03,  ...,  8.4603e-04,\n",
      "         -2.2417e-03,  1.2687e-03],\n",
      "        [ 1.3448e-03, -1.9400e-03, -7.3272e-04,  ...,  1.8369e-03,\n",
      "          3.9937e-04, -8.4960e-04],\n",
      "        [ 7.7279e-04, -1.1256e-03, -1.9701e-03,  ...,  6.5775e-05,\n",
      "         -1.9894e-03,  3.2162e-03]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight \n",
      "value: tensor([[ 7.3249e-03,  6.1806e-03,  1.5025e-04,  ...,  3.7649e-03,\n",
      "          3.4946e-03,  1.9203e-03],\n",
      "        [ 4.6806e-03,  9.3046e-04,  2.8160e-03,  ...,  4.8893e-03,\n",
      "          2.4481e-03, -5.7942e-03],\n",
      "        [-4.0065e-03,  1.3439e-03, -5.6159e-03,  ...,  4.1150e-05,\n",
      "         -6.6166e-03, -6.0750e-04],\n",
      "        ...,\n",
      "        [-3.5400e-03,  7.8312e-04, -3.9916e-03,  ...,  9.4324e-04,\n",
      "          2.5915e-03, -1.1207e-03],\n",
      "        [-1.4825e-03, -9.6674e-03,  2.7665e-03,  ...,  3.0693e-03,\n",
      "          3.7373e-04, -1.1060e-03],\n",
      "        [ 2.5175e-03,  6.8314e-03, -3.6497e-03,  ...,  3.4201e-03,\n",
      "          7.2376e-05, -1.0166e-04]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight \n",
      "value: tensor([[-6.4872e-03, -1.1978e-03,  3.6698e-03,  ...,  1.5850e-03,\n",
      "          1.9610e-03,  9.5350e-03],\n",
      "        [-6.9855e-03, -5.2760e-03,  3.4579e-03,  ..., -7.3822e-05,\n",
      "         -3.8027e-03, -5.9683e-03],\n",
      "        [ 1.2386e-02, -3.2248e-03, -1.5678e-03,  ...,  2.8485e-03,\n",
      "          3.6499e-03,  7.7312e-03],\n",
      "        ...,\n",
      "        [ 3.2764e-03, -3.8484e-03,  8.3375e-03,  ...,  4.0306e-03,\n",
      "         -5.7399e-03, -3.3466e-04],\n",
      "        [-4.2626e-03,  6.8530e-03, -1.0046e-03,  ..., -3.0010e-03,\n",
      "         -6.4654e-03,  3.0941e-03],\n",
      "        [-5.8502e-03,  8.7605e-03,  5.6446e-04,  ...,  2.0290e-03,\n",
      "          1.0805e-02,  7.7365e-04]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight \n",
      "value: tensor([[ 2.8537e-03,  1.3595e-03,  4.9495e-03,  ..., -5.5421e-03,\n",
      "          2.8533e-03, -2.5974e-03],\n",
      "        [ 7.3035e-03,  5.8590e-03, -2.7028e-03,  ...,  7.1959e-04,\n",
      "          9.4871e-04, -4.7715e-03],\n",
      "        [-2.6367e-03,  6.4939e-04,  3.9938e-04,  ..., -5.1775e-03,\n",
      "          3.2225e-03,  4.8223e-04],\n",
      "        ...,\n",
      "        [-8.2600e-03,  3.7404e-03, -2.8624e-03,  ..., -1.6704e-03,\n",
      "         -2.0517e-03,  5.8314e-04],\n",
      "        [-8.4302e-04, -1.0769e-03,  2.1871e-03,  ..., -2.2851e-03,\n",
      "         -3.4489e-03,  1.6596e-03],\n",
      "        [-1.8344e-03,  4.1968e-04,  4.2878e-03,  ..., -4.7050e-04,\n",
      "         -5.3015e-05, -4.5886e-03]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight \n",
      "value: tensor([[ 1.0056e-03, -2.3981e-03, -2.2403e-04,  ...,  1.5996e-04,\n",
      "         -6.0577e-03,  6.6870e-04],\n",
      "        [ 3.2110e-04,  1.3169e-04,  5.8952e-04,  ..., -3.3214e-05,\n",
      "          9.1319e-03,  9.1187e-04],\n",
      "        [-1.5893e-03, -9.6425e-04, -1.5989e-03,  ...,  5.1846e-04,\n",
      "          4.2572e-03, -1.6136e-03],\n",
      "        ...,\n",
      "        [ 7.2990e-04, -3.5652e-04, -1.5505e-03,  ..., -4.8429e-04,\n",
      "          3.5277e-04, -1.1274e-03],\n",
      "        [-2.7022e-04,  3.0686e-04, -3.8474e-04,  ..., -6.9108e-04,\n",
      "         -2.7691e-03, -1.8204e-03],\n",
      "        [-9.7846e-04,  7.6710e-05, -8.7116e-05,  ..., -1.4117e-03,\n",
      "         -1.0281e-03, -2.0907e-04]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight \n",
      "value: tensor([[ 0.0019, -0.0002,  0.0045,  ...,  0.0009, -0.0010, -0.0099],\n",
      "        [-0.0044, -0.0001,  0.0003,  ...,  0.0027, -0.0021,  0.0049],\n",
      "        [ 0.0007,  0.0029, -0.0045,  ...,  0.0037,  0.0010, -0.0001],\n",
      "        ...,\n",
      "        [ 0.0023, -0.0010,  0.0025,  ...,  0.0024,  0.0039,  0.0062],\n",
      "        [ 0.0011, -0.0032,  0.0051,  ...,  0.0004,  0.0044, -0.0024],\n",
      "        [ 0.0026,  0.0029,  0.0013,  ...,  0.0014, -0.0013,  0.0028]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight \n",
      "value: tensor([[ 0.0018,  0.0004, -0.0004,  ..., -0.0023, -0.0060,  0.0053],\n",
      "        [ 0.0014,  0.0043,  0.0125,  ..., -0.0002, -0.0040,  0.0038],\n",
      "        [-0.0014, -0.0075,  0.0007,  ...,  0.0027, -0.0010,  0.0049],\n",
      "        ...,\n",
      "        [-0.0028, -0.0048, -0.0021,  ..., -0.0088,  0.0045,  0.0086],\n",
      "        [ 0.0042, -0.0070, -0.0062,  ..., -0.0030, -0.0054,  0.0018],\n",
      "        [-0.0025, -0.0117, -0.0059,  ...,  0.0024, -0.0021,  0.0007]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight \n",
      "value: tensor([[ 0.0119, -0.0164, -0.0020,  ..., -0.0150,  0.0011,  0.0029],\n",
      "        [ 0.0057, -0.0027,  0.0071,  ...,  0.0055,  0.0036,  0.0088],\n",
      "        [-0.0049, -0.0009, -0.0094,  ..., -0.0008, -0.0006, -0.0118],\n",
      "        ...,\n",
      "        [ 0.0053, -0.0024, -0.0011,  ...,  0.0035, -0.0055,  0.0049],\n",
      "        [-0.0100,  0.0084,  0.0158,  ...,  0.0082, -0.0067,  0.0209],\n",
      "        [-0.0051,  0.0091, -0.0017,  ...,  0.0040, -0.0062,  0.0051]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight \n",
      "value: tensor([[ 0.0117, -0.0087,  0.0088,  ..., -0.0002, -0.0262, -0.0120],\n",
      "        [-0.0036, -0.0029,  0.0065,  ..., -0.0066,  0.0010, -0.0023],\n",
      "        [ 0.0040,  0.0017,  0.0019,  ...,  0.0015, -0.0031,  0.0063],\n",
      "        ...,\n",
      "        [ 0.0047, -0.0020,  0.0038,  ..., -0.0005, -0.0031, -0.0017],\n",
      "        [ 0.0019,  0.0066, -0.0006,  ..., -0.0037, -0.0015,  0.0088],\n",
      "        [ 0.0069, -0.0087, -0.0016,  ..., -0.0025, -0.0006, -0.0043]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight \n",
      "value: tensor([[ 3.0412e-02, -1.7206e-03,  3.2953e-05,  ...,  2.9187e-03,\n",
      "          2.9731e-03,  3.5488e-03],\n",
      "        [ 4.7859e-03, -2.3481e-04,  3.2863e-03,  ..., -2.9577e-03,\n",
      "          7.3692e-03, -1.2182e-02],\n",
      "        [ 1.3061e-02, -1.0347e-02, -2.4797e-04,  ..., -6.7301e-03,\n",
      "         -3.4209e-03, -8.5630e-03],\n",
      "        ...,\n",
      "        [-2.4380e-04, -2.4633e-03,  2.4757e-03,  ..., -7.8518e-03,\n",
      "         -6.5075e-04, -2.2378e-04],\n",
      "        [-2.1970e-02, -1.6563e-03,  1.3066e-02,  ..., -1.1121e-03,\n",
      "         -3.3218e-03, -1.2101e-02],\n",
      "        [ 1.1119e-02,  4.4334e-03, -2.2775e-03,  ...,  3.3383e-03,\n",
      "         -6.8714e-03, -3.9148e-03]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight \n",
      "value: tensor([[ 0.0048, -0.0026,  0.0027,  ...,  0.0018, -0.0128,  0.0005],\n",
      "        [-0.0004,  0.0042, -0.0008,  ...,  0.0016, -0.0078, -0.0037],\n",
      "        [-0.0062, -0.0063,  0.0077,  ...,  0.0020,  0.0005, -0.0065],\n",
      "        ...,\n",
      "        [-0.0024,  0.0075,  0.0072,  ..., -0.0001,  0.0051, -0.0042],\n",
      "        [-0.0075, -0.0112, -0.0001,  ...,  0.0007,  0.0014, -0.0064],\n",
      "        [ 0.0127,  0.0045,  0.0066,  ...,  0.0064, -0.0119, -0.0036]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.proj_in.lora.down.weight \n",
      "value: tensor([[[[-7.1607e-03]],\n",
      "\n",
      "         [[ 2.2288e-03]],\n",
      "\n",
      "         [[-6.4521e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2050e-02]],\n",
      "\n",
      "         [[-5.0266e-03]],\n",
      "\n",
      "         [[-1.9814e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4061e-03]],\n",
      "\n",
      "         [[ 1.4723e-02]],\n",
      "\n",
      "         [[-2.1521e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.7509e-03]],\n",
      "\n",
      "         [[ 2.2000e-03]],\n",
      "\n",
      "         [[-8.1117e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 8.4217e-03]],\n",
      "\n",
      "         [[-6.2427e-04]],\n",
      "\n",
      "         [[ 5.8103e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.6644e-03]],\n",
      "\n",
      "         [[ 9.6938e-04]],\n",
      "\n",
      "         [[ 3.8825e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-3.0900e-05]],\n",
      "\n",
      "         [[ 8.5566e-04]],\n",
      "\n",
      "         [[-1.5181e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.9582e-03]],\n",
      "\n",
      "         [[ 1.1970e-02]],\n",
      "\n",
      "         [[-7.2802e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.0937e-03]],\n",
      "\n",
      "         [[-4.6152e-03]],\n",
      "\n",
      "         [[-6.7080e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.5772e-03]],\n",
      "\n",
      "         [[ 1.1904e-02]],\n",
      "\n",
      "         [[-9.5089e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.0679e-03]],\n",
      "\n",
      "         [[ 6.2851e-03]],\n",
      "\n",
      "         [[-5.3430e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4000e-02]],\n",
      "\n",
      "         [[ 4.1743e-03]],\n",
      "\n",
      "         [[-7.4867e-03]]]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.proj_in.lora.up.weight \n",
      "value: tensor([[[[ 0.0088]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[-0.0112]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[-0.0104]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0079]],\n",
      "\n",
      "         [[ 0.0059]],\n",
      "\n",
      "         [[ 0.0032]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0055]],\n",
      "\n",
      "         [[-0.0038]],\n",
      "\n",
      "         [[ 0.0026]]],\n",
      "\n",
      "\n",
      "        [[[-0.0059]],\n",
      "\n",
      "         [[-0.0058]],\n",
      "\n",
      "         [[-0.0030]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[ 0.0037]],\n",
      "\n",
      "         [[ 0.0022]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0038]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0056]],\n",
      "\n",
      "         [[ 0.0046]],\n",
      "\n",
      "         [[-0.0035]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0014]],\n",
      "\n",
      "         [[-0.0048]],\n",
      "\n",
      "         [[-0.0042]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0057]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[-0.0019]]],\n",
      "\n",
      "\n",
      "        [[[-0.0054]],\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         [[-0.0059]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         [[ 0.0079]],\n",
      "\n",
      "         [[-0.0007]]]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.proj_out.lora.down.weight \n",
      "value: tensor([[[[ 0.0072]],\n",
      "\n",
      "         [[-0.0148]],\n",
      "\n",
      "         [[ 0.0098]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0113]],\n",
      "\n",
      "         [[ 0.0176]],\n",
      "\n",
      "         [[-0.0116]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0043]],\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[-0.0104]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0125]],\n",
      "\n",
      "         [[-0.0124]],\n",
      "\n",
      "         [[ 0.0081]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0016]],\n",
      "\n",
      "         [[ 0.0005]],\n",
      "\n",
      "         [[ 0.0048]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0034]],\n",
      "\n",
      "         [[ 0.0104]],\n",
      "\n",
      "         [[-0.0056]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0033]],\n",
      "\n",
      "         [[-0.0039]],\n",
      "\n",
      "         [[-0.0047]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0097]],\n",
      "\n",
      "         [[-0.0068]],\n",
      "\n",
      "         [[-0.0012]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0003]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[-0.0125]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0065]],\n",
      "\n",
      "         [[ 0.0048]],\n",
      "\n",
      "         [[-0.0128]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0027]],\n",
      "\n",
      "         [[-0.0085]],\n",
      "\n",
      "         [[ 0.0025]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0154]],\n",
      "\n",
      "         [[-0.0092]],\n",
      "\n",
      "         [[-0.0217]]]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.proj_out.lora.up.weight \n",
      "value: tensor([[[[-5.3034e-05]],\n",
      "\n",
      "         [[ 8.8798e-03]],\n",
      "\n",
      "         [[ 9.6420e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.3607e-03]],\n",
      "\n",
      "         [[-1.2391e-02]],\n",
      "\n",
      "         [[-3.5910e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4709e-02]],\n",
      "\n",
      "         [[ 6.7392e-03]],\n",
      "\n",
      "         [[ 1.1677e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 9.2003e-03]],\n",
      "\n",
      "         [[ 1.6279e-02]],\n",
      "\n",
      "         [[-6.3076e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.4802e-03]],\n",
      "\n",
      "         [[-4.6874e-03]],\n",
      "\n",
      "         [[ 1.4449e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1404e-03]],\n",
      "\n",
      "         [[-1.7124e-03]],\n",
      "\n",
      "         [[ 1.4797e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 5.2937e-03]],\n",
      "\n",
      "         [[ 3.4030e-03]],\n",
      "\n",
      "         [[-4.4616e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.7555e-03]],\n",
      "\n",
      "         [[-1.9094e-03]],\n",
      "\n",
      "         [[ 8.4024e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6098e-03]],\n",
      "\n",
      "         [[-1.4711e-03]],\n",
      "\n",
      "         [[-1.9273e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0085e-03]],\n",
      "\n",
      "         [[-6.2312e-03]],\n",
      "\n",
      "         [[ 3.0164e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 7.1903e-03]],\n",
      "\n",
      "         [[-2.4448e-03]],\n",
      "\n",
      "         [[ 1.4598e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1802e-02]],\n",
      "\n",
      "         [[ 9.0687e-03]],\n",
      "\n",
      "         [[ 9.8370e-04]]]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight \n",
      "value: tensor([[-9.0624e-04, -4.0349e-03, -2.3734e-03,  ..., -3.3772e-04,\n",
      "         -9.7821e-03, -1.4088e-05],\n",
      "        [-1.0744e-02, -4.6216e-03, -8.6200e-03,  ...,  1.4648e-03,\n",
      "          1.9037e-03,  8.5828e-03],\n",
      "        [ 4.4181e-03, -3.4319e-03,  4.8632e-03,  ..., -8.0429e-05,\n",
      "         -6.8028e-03, -3.1781e-03],\n",
      "        ...,\n",
      "        [ 8.4288e-04,  2.9237e-03,  4.2369e-03,  ...,  8.9302e-03,\n",
      "          4.8326e-04, -8.1882e-03],\n",
      "        [-7.4551e-03, -4.0146e-03,  4.7686e-04,  ..., -9.7260e-03,\n",
      "          1.9090e-02,  3.4401e-03],\n",
      "        [ 4.6965e-03,  3.6273e-03, -7.3460e-04,  ..., -2.1678e-03,\n",
      "          6.2808e-03,  1.0167e-04]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight \n",
      "value: tensor([[-0.0004, -0.0006, -0.0014,  ..., -0.0077,  0.0020,  0.0019],\n",
      "        [-0.0104, -0.0114,  0.0007,  ..., -0.0005, -0.0071,  0.0056],\n",
      "        [ 0.0024,  0.0015, -0.0050,  ...,  0.0012, -0.0045,  0.0030],\n",
      "        ...,\n",
      "        [-0.0001, -0.0021,  0.0068,  ...,  0.0047, -0.0151, -0.0012],\n",
      "        [ 0.0161,  0.0028, -0.0027,  ..., -0.0001, -0.0072, -0.0007],\n",
      "        [ 0.0065, -0.0022, -0.0091,  ..., -0.0024,  0.0031,  0.0027]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight \n",
      "value: tensor([[-0.0130, -0.0090,  0.0057,  ..., -0.0088,  0.0061,  0.0082],\n",
      "        [-0.0019,  0.0020,  0.0037,  ..., -0.0008, -0.0037,  0.0017],\n",
      "        [-0.0044,  0.0010,  0.0018,  ..., -0.0043,  0.0031, -0.0022],\n",
      "        ...,\n",
      "        [ 0.0005, -0.0065,  0.0100,  ...,  0.0095,  0.0011,  0.0042],\n",
      "        [-0.0101,  0.0013,  0.0104,  ..., -0.0083,  0.0007,  0.0058],\n",
      "        [ 0.0012,  0.0080,  0.0059,  ...,  0.0069,  0.0035, -0.0045]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight \n",
      "value: tensor([[-0.0013,  0.0026, -0.0027,  ...,  0.0014,  0.0027,  0.0071],\n",
      "        [-0.0090, -0.0043,  0.0002,  ...,  0.0006, -0.0001, -0.0001],\n",
      "        [-0.0038, -0.0029,  0.0066,  ...,  0.0065, -0.0009, -0.0100],\n",
      "        ...,\n",
      "        [-0.0011,  0.0034, -0.0026,  ...,  0.0089,  0.0151, -0.0076],\n",
      "        [-0.0076, -0.0037, -0.0040,  ..., -0.0011,  0.0019, -0.0063],\n",
      "        [ 0.0013, -0.0032,  0.0057,  ..., -0.0028, -0.0085, -0.0127]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight \n",
      "value: tensor([[ 0.0004, -0.0008, -0.0045,  ...,  0.0002, -0.0034,  0.0122],\n",
      "        [-0.0047, -0.0015, -0.0019,  ..., -0.0012,  0.0062,  0.0084],\n",
      "        [ 0.0035, -0.0045, -0.0022,  ..., -0.0034,  0.0011, -0.0040],\n",
      "        ...,\n",
      "        [-0.0016, -0.0018,  0.0074,  ...,  0.0045, -0.0050,  0.0026],\n",
      "        [-0.0073,  0.0068, -0.0096,  ..., -0.0046, -0.0050,  0.0053],\n",
      "        [-0.0057,  0.0070, -0.0075,  ...,  0.0017,  0.0067,  0.0020]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight \n",
      "value: tensor([[-1.1557e-02, -2.4458e-04,  6.2397e-03,  ...,  6.0124e-03,\n",
      "          1.1655e-03, -8.0653e-03],\n",
      "        [ 8.4684e-03,  8.2188e-04, -9.0567e-03,  ...,  7.0299e-04,\n",
      "         -5.4655e-03, -7.0370e-03],\n",
      "        [ 1.1281e-03, -2.9434e-03,  1.2447e-03,  ..., -5.6774e-03,\n",
      "         -3.7474e-03, -1.8349e-03],\n",
      "        ...,\n",
      "        [-7.5640e-03, -5.4286e-04, -1.3200e-03,  ...,  7.7192e-05,\n",
      "         -8.1867e-03, -2.6344e-03],\n",
      "        [-7.8410e-03,  8.3057e-04, -3.0817e-03,  ..., -3.8604e-04,\n",
      "         -1.4972e-02, -3.7938e-03],\n",
      "        [ 5.0231e-03, -1.4555e-03,  1.2198e-03,  ..., -1.0173e-03,\n",
      "         -1.9022e-03, -3.6478e-03]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight \n",
      "value: tensor([[ 0.0017,  0.0004, -0.0076,  ...,  0.0014, -0.0042, -0.0031],\n",
      "        [-0.0012,  0.0053,  0.0001,  ...,  0.0103,  0.0036, -0.0066],\n",
      "        [-0.0152, -0.0046, -0.0012,  ...,  0.0078, -0.0030, -0.0021],\n",
      "        ...,\n",
      "        [ 0.0077,  0.0017, -0.0044,  ...,  0.0076,  0.0041,  0.0039],\n",
      "        [ 0.0007,  0.0094, -0.0076,  ...,  0.0079, -0.0133, -0.0119],\n",
      "        [-0.0074,  0.0081, -0.0027,  ...,  0.0041,  0.0116, -0.0026]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight \n",
      "value: tensor([[ 0.0046, -0.0048, -0.0057,  ...,  0.0016,  0.0001,  0.0049],\n",
      "        [-0.0132,  0.0025, -0.0019,  ..., -0.0064, -0.0145,  0.0022],\n",
      "        [ 0.0080, -0.0011, -0.0078,  ...,  0.0027, -0.0025, -0.0014],\n",
      "        ...,\n",
      "        [-0.0067, -0.0016, -0.0033,  ...,  0.0020, -0.0092, -0.0022],\n",
      "        [ 0.0038,  0.0006,  0.0047,  ..., -0.0018, -0.0072, -0.0095],\n",
      "        [-0.0060,  0.0055, -0.0002,  ..., -0.0015, -0.0041, -0.0013]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight \n",
      "value: tensor([[ 0.0003,  0.0105,  0.0005,  ...,  0.0015, -0.0028,  0.0033],\n",
      "        [-0.0005, -0.0003, -0.0004,  ..., -0.0044,  0.0028, -0.0003],\n",
      "        [-0.0023, -0.0024, -0.0038,  ..., -0.0018,  0.0006,  0.0004],\n",
      "        ...,\n",
      "        [-0.0028,  0.0035,  0.0013,  ...,  0.0012,  0.0015, -0.0035],\n",
      "        [-0.0003, -0.0028, -0.0031,  ...,  0.0047, -0.0010,  0.0069],\n",
      "        [ 0.0004, -0.0042, -0.0033,  ..., -0.0013, -0.0058, -0.0060]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight \n",
      "value: tensor([[-1.7575e-03, -4.4784e-06,  2.1006e-03,  ...,  3.0802e-03,\n",
      "          1.8284e-03,  2.9692e-04],\n",
      "        [ 7.9893e-04, -7.7100e-04, -1.4667e-03,  ..., -8.9140e-05,\n",
      "         -6.7035e-03, -9.5499e-05],\n",
      "        [-1.3976e-04, -2.6037e-04, -3.6158e-04,  ..., -3.9455e-04,\n",
      "         -2.5164e-03,  5.8218e-04],\n",
      "        ...,\n",
      "        [-2.0699e-03,  5.9289e-04, -1.0685e-03,  ...,  3.9244e-03,\n",
      "          4.2064e-03, -1.9593e-04],\n",
      "        [-9.0335e-05, -6.2064e-05,  2.1021e-04,  ...,  6.0757e-04,\n",
      "          5.1226e-03,  1.1050e-03],\n",
      "        [-8.3646e-04,  2.0334e-04,  5.7931e-04,  ..., -1.8223e-04,\n",
      "          3.1716e-03, -6.8399e-04]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight \n",
      "value: tensor([[-0.0068, -0.0006, -0.0041,  ..., -0.0077,  0.0039,  0.0006],\n",
      "        [ 0.0110, -0.0058,  0.0020,  ..., -0.0038, -0.0008, -0.0067],\n",
      "        [ 0.0007, -0.0049, -0.0033,  ...,  0.0053,  0.0031, -0.0022],\n",
      "        ...,\n",
      "        [-0.0032,  0.0013,  0.0013,  ..., -0.0004, -0.0088,  0.0060],\n",
      "        [-0.0128, -0.0012, -0.0014,  ...,  0.0042,  0.0015, -0.0081],\n",
      "        [ 0.0032, -0.0006,  0.0012,  ..., -0.0027,  0.0003, -0.0032]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight \n",
      "value: tensor([[ 2.1752e-03, -7.9214e-03,  4.7791e-03,  ..., -4.0684e-03,\n",
      "          1.0348e-03,  7.5728e-03],\n",
      "        [-4.0321e-04, -3.9072e-03,  4.6564e-03,  ...,  9.5840e-04,\n",
      "          5.5512e-03, -7.1345e-03],\n",
      "        [ 3.2646e-03,  6.8386e-03,  4.8497e-04,  ...,  1.0488e-04,\n",
      "         -1.0805e-02, -5.0220e-03],\n",
      "        ...,\n",
      "        [ 1.9840e-03, -6.5218e-03,  6.2445e-03,  ..., -1.6207e-03,\n",
      "          5.6459e-03,  1.3778e-04],\n",
      "        [ 3.2500e-03, -1.5539e-03, -3.5697e-03,  ...,  1.7312e-05,\n",
      "         -5.2062e-03,  1.9522e-03],\n",
      "        [ 9.5588e-04,  7.2712e-03, -5.1071e-03,  ...,  7.0116e-03,\n",
      "         -3.9726e-03, -7.2369e-05]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight \n",
      "value: tensor([[-0.0004,  0.0027,  0.0061,  ...,  0.0067, -0.0005, -0.0014],\n",
      "        [-0.0021, -0.0038,  0.0006,  ..., -0.0005,  0.0006,  0.0018],\n",
      "        [ 0.0022, -0.0017,  0.0014,  ...,  0.0052,  0.0025,  0.0045],\n",
      "        ...,\n",
      "        [-0.0023,  0.0044,  0.0015,  ...,  0.0064,  0.0072, -0.0032],\n",
      "        [-0.0046, -0.0082, -0.0057,  ..., -0.0102, -0.0013,  0.0125],\n",
      "        [ 0.0025, -0.0038,  0.0018,  ..., -0.0030,  0.0042,  0.0010]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight \n",
      "value: tensor([[ 5.8376e-04,  3.0567e-04,  4.1868e-05,  ...,  2.5749e-03,\n",
      "          4.5270e-03, -4.9470e-04],\n",
      "        [ 2.7747e-04,  2.4777e-04, -4.7209e-04,  ...,  4.8168e-03,\n",
      "          4.2723e-03,  1.4716e-03],\n",
      "        [-1.4863e-03,  5.1250e-04, -7.5735e-05,  ..., -8.5304e-04,\n",
      "          2.9341e-03, -8.2735e-04],\n",
      "        ...,\n",
      "        [-5.2400e-04, -2.1663e-04, -1.4448e-03,  ...,  1.2543e-03,\n",
      "          5.3284e-03,  1.0563e-04],\n",
      "        [ 7.7594e-04,  6.5091e-04, -1.0614e-03,  ...,  1.0898e-03,\n",
      "          4.4832e-03,  1.4890e-04],\n",
      "        [ 8.6387e-04, -8.7781e-05, -8.6689e-05,  ...,  2.0933e-03,\n",
      "         -4.0987e-03,  7.7422e-04]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight \n",
      "value: tensor([[-2.6379e-03, -5.2832e-05,  2.8727e-03,  ..., -5.8898e-04,\n",
      "         -5.4233e-04, -4.5027e-03],\n",
      "        [ 4.6659e-03,  9.8013e-04,  2.7005e-03,  ..., -1.6858e-03,\n",
      "         -7.7942e-04,  3.5467e-03],\n",
      "        [ 7.5498e-03, -2.5902e-03,  4.4584e-03,  ...,  5.9793e-03,\n",
      "          1.1364e-03, -6.0842e-04],\n",
      "        ...,\n",
      "        [ 1.1691e-03,  2.9612e-03, -2.6377e-03,  ...,  3.9237e-03,\n",
      "          4.4451e-03,  3.0540e-03],\n",
      "        [-3.8710e-03, -6.1179e-03, -1.5489e-03,  ..., -4.5688e-03,\n",
      "         -2.1345e-03, -4.4715e-03],\n",
      "        [ 3.1726e-04, -8.3252e-04, -6.9145e-03,  ...,  8.9108e-04,\n",
      "         -3.5438e-04, -2.8503e-03]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight \n",
      "value: tensor([[ 0.0017, -0.0062,  0.0057,  ...,  0.0042,  0.0009,  0.0007],\n",
      "        [ 0.0037, -0.0073, -0.0019,  ..., -0.0062,  0.0034, -0.0020],\n",
      "        [ 0.0024,  0.0049,  0.0043,  ...,  0.0014, -0.0009,  0.0007],\n",
      "        ...,\n",
      "        [ 0.0030,  0.0031, -0.0078,  ..., -0.0035, -0.0033,  0.0069],\n",
      "        [-0.0023,  0.0009, -0.0044,  ..., -0.0058,  0.0062,  0.0042],\n",
      "        [ 0.0004, -0.0031,  0.0083,  ...,  0.0142,  0.0025,  0.0045]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.down.weight \n",
      "value: tensor([[ 0.0047, -0.0097, -0.0045,  ...,  0.0085,  0.0041,  0.0127],\n",
      "        [-0.0024, -0.0052, -0.0049,  ...,  0.0003,  0.0052, -0.0027],\n",
      "        [ 0.0009,  0.0090, -0.0064,  ...,  0.0014,  0.0047, -0.0105],\n",
      "        ...,\n",
      "        [ 0.0013,  0.0141, -0.0003,  ..., -0.0022, -0.0020,  0.0083],\n",
      "        [ 0.0040,  0.0185,  0.0031,  ...,  0.0021,  0.0026, -0.0013],\n",
      "        [ 0.0088,  0.0068, -0.0017,  ...,  0.0135, -0.0170, -0.0008]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.up.weight \n",
      "value: tensor([[-3.5696e-03, -4.8468e-03, -5.5432e-03,  ...,  2.1405e-03,\n",
      "          5.9534e-03, -1.5966e-03],\n",
      "        [ 4.2452e-03, -4.0663e-04,  6.2075e-03,  ..., -1.9519e-03,\n",
      "          7.1689e-03, -8.1471e-04],\n",
      "        [-4.5975e-05,  1.0328e-02,  1.9704e-03,  ..., -9.2127e-03,\n",
      "         -1.1735e-03, -1.8981e-04],\n",
      "        ...,\n",
      "        [ 2.3327e-03,  1.1132e-04, -1.0760e-03,  ..., -1.8650e-03,\n",
      "          4.5439e-03,  2.4768e-03],\n",
      "        [ 4.7555e-04,  6.0349e-03, -5.8572e-04,  ...,  7.9443e-03,\n",
      "          2.8781e-03,  8.6264e-03],\n",
      "        [ 3.1386e-04, -2.8372e-03, -1.2759e-03,  ...,  1.2387e-03,\n",
      "         -7.2587e-04, -1.2292e-02]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.lora.down.weight \n",
      "value: tensor([[ 0.0014,  0.0036, -0.0016,  ..., -0.0009,  0.0073, -0.0019],\n",
      "        [ 0.0109, -0.0002, -0.0113,  ...,  0.0032, -0.0135, -0.0032],\n",
      "        [-0.0027,  0.0048,  0.0016,  ..., -0.0041, -0.0033,  0.0021],\n",
      "        ...,\n",
      "        [-0.0005,  0.0061, -0.0101,  ...,  0.0067,  0.0073, -0.0031],\n",
      "        [ 0.0080,  0.0066,  0.0045,  ..., -0.0097,  0.0040, -0.0020],\n",
      "        [ 0.0010, -0.0017, -0.0003,  ..., -0.0043,  0.0079, -0.0090]])\n",
      "\n",
      "key: unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.lora.up.weight \n",
      "value: tensor([[-8.3186e-03, -9.9069e-03, -3.0861e-03,  ..., -1.5557e-03,\n",
      "          4.4580e-03, -8.4889e-03],\n",
      "        [-5.2233e-03,  3.2908e-03,  3.1484e-03,  ...,  1.9869e-03,\n",
      "         -2.3225e-03, -2.2721e-03],\n",
      "        [-6.0582e-03, -3.2971e-03, -6.6939e-04,  ...,  1.8518e-03,\n",
      "          4.2475e-03,  5.7548e-03],\n",
      "        ...,\n",
      "        [ 1.7288e-02,  3.1407e-03,  9.3960e-03,  ..., -6.1078e-03,\n",
      "          7.1512e-03, -3.7009e-03],\n",
      "        [ 6.6772e-05,  5.1036e-04, -2.8626e-03,  ..., -4.3036e-03,\n",
      "         -2.2894e-02, -4.9003e-03],\n",
      "        [ 6.0351e-03, -5.2125e-05, -9.8887e-03,  ...,  2.0271e-03,\n",
      "          6.0621e-03,  8.6647e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.proj_in.lora.down.weight \n",
      "value: tensor([[[[-0.0005]],\n",
      "\n",
      "         [[-0.0055]],\n",
      "\n",
      "         [[-0.0113]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         [[-0.0022]],\n",
      "\n",
      "         [[ 0.0008]]],\n",
      "\n",
      "\n",
      "        [[[-0.0006]],\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0030]],\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         [[-0.0035]]],\n",
      "\n",
      "\n",
      "        [[[-0.0008]],\n",
      "\n",
      "         [[ 0.0050]],\n",
      "\n",
      "         [[ 0.0067]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0007]],\n",
      "\n",
      "         [[-0.0045]],\n",
      "\n",
      "         [[-0.0023]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0006]],\n",
      "\n",
      "         [[ 0.0060]],\n",
      "\n",
      "         [[ 0.0041]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0025]],\n",
      "\n",
      "         [[ 0.0004]],\n",
      "\n",
      "         [[-0.0003]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0008]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[ 0.0054]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0016]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         [[ 0.0071]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0036]],\n",
      "\n",
      "         [[-0.0024]],\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0040]],\n",
      "\n",
      "         [[-0.0008]],\n",
      "\n",
      "         [[-0.0004]]]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.proj_in.lora.up.weight \n",
      "value: tensor([[[[-5.6429e-04]],\n",
      "\n",
      "         [[-2.9892e-03]],\n",
      "\n",
      "         [[-4.0488e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.9237e-03]],\n",
      "\n",
      "         [[ 6.2249e-03]],\n",
      "\n",
      "         [[ 1.4288e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.3319e-04]],\n",
      "\n",
      "         [[ 6.0361e-03]],\n",
      "\n",
      "         [[-9.5847e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.8592e-03]],\n",
      "\n",
      "         [[-2.9655e-03]],\n",
      "\n",
      "         [[-3.5364e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.5710e-03]],\n",
      "\n",
      "         [[-7.1399e-03]],\n",
      "\n",
      "         [[-2.6870e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.1916e-03]],\n",
      "\n",
      "         [[-7.0099e-03]],\n",
      "\n",
      "         [[ 2.6902e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 6.1882e-03]],\n",
      "\n",
      "         [[ 5.0317e-03]],\n",
      "\n",
      "         [[ 4.3596e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3539e-03]],\n",
      "\n",
      "         [[ 2.5780e-04]],\n",
      "\n",
      "         [[ 1.1621e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.9191e-03]],\n",
      "\n",
      "         [[ 1.1327e-03]],\n",
      "\n",
      "         [[ 1.9750e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.5162e-03]],\n",
      "\n",
      "         [[-2.6365e-03]],\n",
      "\n",
      "         [[-2.9534e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.3138e-03]],\n",
      "\n",
      "         [[-6.8288e-05]],\n",
      "\n",
      "         [[ 3.2774e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.4097e-03]],\n",
      "\n",
      "         [[ 6.0005e-03]],\n",
      "\n",
      "         [[-5.1414e-04]]]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.proj_out.lora.down.weight \n",
      "value: tensor([[[[-0.0054]],\n",
      "\n",
      "         [[ 0.0123]],\n",
      "\n",
      "         [[ 0.0017]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0064]],\n",
      "\n",
      "         [[-0.0029]],\n",
      "\n",
      "         [[ 0.0008]]],\n",
      "\n",
      "\n",
      "        [[[-0.0091]],\n",
      "\n",
      "         [[ 0.0045]],\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0014]],\n",
      "\n",
      "         [[ 0.0008]],\n",
      "\n",
      "         [[-0.0010]]],\n",
      "\n",
      "\n",
      "        [[[-0.0049]],\n",
      "\n",
      "         [[ 0.0034]],\n",
      "\n",
      "         [[-0.0028]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0070]],\n",
      "\n",
      "         [[-0.0081]],\n",
      "\n",
      "         [[ 0.0054]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0062]],\n",
      "\n",
      "         [[ 0.0012]],\n",
      "\n",
      "         [[ 0.0071]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         [[ 0.0063]],\n",
      "\n",
      "         [[-0.0004]]],\n",
      "\n",
      "\n",
      "        [[[-0.0037]],\n",
      "\n",
      "         [[ 0.0075]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0029]],\n",
      "\n",
      "         [[-0.0140]],\n",
      "\n",
      "         [[ 0.0039]]],\n",
      "\n",
      "\n",
      "        [[[-0.0058]],\n",
      "\n",
      "         [[ 0.0076]],\n",
      "\n",
      "         [[ 0.0062]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0060]],\n",
      "\n",
      "         [[-0.0028]],\n",
      "\n",
      "         [[ 0.0008]]]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.proj_out.lora.up.weight \n",
      "value: tensor([[[[-2.6835e-03]],\n",
      "\n",
      "         [[ 2.7549e-03]],\n",
      "\n",
      "         [[ 5.8249e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.3005e-03]],\n",
      "\n",
      "         [[ 7.3834e-03]],\n",
      "\n",
      "         [[ 2.4851e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.3129e-06]],\n",
      "\n",
      "         [[ 1.7996e-03]],\n",
      "\n",
      "         [[-2.3285e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.0747e-04]],\n",
      "\n",
      "         [[-4.3283e-03]],\n",
      "\n",
      "         [[-1.9710e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.5116e-03]],\n",
      "\n",
      "         [[ 3.4045e-03]],\n",
      "\n",
      "         [[-2.6818e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0301e-02]],\n",
      "\n",
      "         [[-2.0932e-03]],\n",
      "\n",
      "         [[-5.4722e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 4.3803e-03]],\n",
      "\n",
      "         [[-2.8570e-03]],\n",
      "\n",
      "         [[ 3.2252e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.4722e-03]],\n",
      "\n",
      "         [[ 4.3426e-03]],\n",
      "\n",
      "         [[ 1.7769e-04]]],\n",
      "\n",
      "\n",
      "        [[[-6.6074e-03]],\n",
      "\n",
      "         [[ 4.3869e-03]],\n",
      "\n",
      "         [[ 4.7322e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.5524e-03]],\n",
      "\n",
      "         [[-1.8792e-03]],\n",
      "\n",
      "         [[-5.5921e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.1417e-03]],\n",
      "\n",
      "         [[ 3.3701e-04]],\n",
      "\n",
      "         [[ 5.4005e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.2160e-03]],\n",
      "\n",
      "         [[-3.6533e-03]],\n",
      "\n",
      "         [[ 4.0511e-03]]]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight \n",
      "value: tensor([[ 0.0033, -0.0066,  0.0025,  ..., -0.0013, -0.0017, -0.0036],\n",
      "        [ 0.0051, -0.0012, -0.0037,  ...,  0.0040,  0.0077,  0.0041],\n",
      "        [-0.0035, -0.0033, -0.0034,  ..., -0.0004, -0.0049,  0.0011],\n",
      "        ...,\n",
      "        [-0.0054, -0.0006, -0.0044,  ..., -0.0014,  0.0094,  0.0005],\n",
      "        [-0.0013,  0.0048, -0.0032,  ..., -0.0016, -0.0024, -0.0057],\n",
      "        [ 0.0043, -0.0031,  0.0007,  ...,  0.0025, -0.0103,  0.0014]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight \n",
      "value: tensor([[-1.1276e-02,  1.6035e-03,  1.0812e-03,  ...,  3.5353e-03,\n",
      "          3.9275e-03,  1.0715e-03],\n",
      "        [ 4.2143e-03,  5.1437e-04, -2.5374e-03,  ...,  1.2231e-03,\n",
      "         -1.8050e-03, -6.7935e-03],\n",
      "        [-2.7682e-03, -2.3786e-03,  3.7557e-03,  ..., -2.0901e-03,\n",
      "         -1.8634e-03, -1.1254e-03],\n",
      "        ...,\n",
      "        [ 4.2964e-03, -2.8813e-03,  1.3631e-02,  ...,  7.4981e-04,\n",
      "          2.9336e-03,  1.5291e-03],\n",
      "        [-3.0694e-03, -6.2017e-05,  5.6652e-03,  ..., -1.4842e-03,\n",
      "          7.5436e-03, -1.0033e-02],\n",
      "        [ 4.4620e-03,  9.3873e-04, -8.4388e-03,  ...,  1.5632e-03,\n",
      "         -5.5852e-04, -2.7825e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight \n",
      "value: tensor([[ 3.5200e-04, -3.4180e-03,  9.4942e-05,  ...,  8.7608e-04,\n",
      "         -1.6040e-03, -4.6890e-03],\n",
      "        [-3.6868e-03, -2.8528e-03, -4.6374e-03,  ...,  9.5261e-03,\n",
      "          7.5611e-03,  2.5960e-03],\n",
      "        [ 7.2546e-03, -1.9127e-03, -1.7230e-03,  ...,  1.1348e-03,\n",
      "         -3.5193e-03, -2.8699e-03],\n",
      "        ...,\n",
      "        [-5.3818e-03,  3.9386e-04, -5.5853e-04,  ..., -3.2538e-03,\n",
      "          4.5667e-03,  2.6876e-03],\n",
      "        [ 3.4114e-03, -3.7537e-03, -8.6656e-03,  ..., -3.9326e-03,\n",
      "         -3.4102e-03,  1.2631e-03],\n",
      "        [ 2.9043e-03,  7.7643e-03, -4.1062e-03,  ..., -3.8136e-03,\n",
      "         -9.0359e-04, -3.5325e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight \n",
      "value: tensor([[-2.2563e-03,  1.2333e-02,  5.1973e-03,  ..., -1.7154e-03,\n",
      "          5.6850e-03, -7.5853e-04],\n",
      "        [ 8.6221e-04, -5.3811e-03,  3.0199e-03,  ..., -3.1718e-04,\n",
      "          4.4985e-04,  2.6647e-03],\n",
      "        [ 1.7152e-03,  4.6177e-05,  1.1490e-03,  ..., -4.6242e-03,\n",
      "          5.2833e-03, -3.6440e-03],\n",
      "        ...,\n",
      "        [ 1.1839e-03,  5.0517e-03,  1.3417e-03,  ..., -2.0420e-03,\n",
      "          6.1980e-04,  6.3754e-03],\n",
      "        [ 2.3144e-04, -1.4828e-03,  7.3475e-03,  ..., -2.0994e-03,\n",
      "         -2.9184e-03,  1.0020e-03],\n",
      "        [-2.9606e-03, -1.0506e-04, -5.3398e-03,  ..., -1.3999e-04,\n",
      "          3.0206e-03, -3.4986e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight \n",
      "value: tensor([[ 1.2412e-02, -8.0499e-04,  7.6613e-03,  ...,  3.4197e-03,\n",
      "         -5.1668e-03, -7.9027e-03],\n",
      "        [-6.5106e-03, -6.6615e-04, -8.6178e-03,  ..., -5.7829e-03,\n",
      "          7.3838e-03,  1.8454e-03],\n",
      "        [ 7.1122e-04, -5.4984e-03,  4.7779e-03,  ...,  3.0028e-04,\n",
      "         -3.0880e-03, -6.4091e-03],\n",
      "        ...,\n",
      "        [-2.4835e-03, -3.5024e-03, -7.3374e-03,  ...,  4.4494e-03,\n",
      "          6.9378e-03, -7.9364e-04],\n",
      "        [-1.0932e-02, -6.6520e-04, -4.9754e-03,  ..., -3.9672e-05,\n",
      "         -1.0212e-03,  2.5577e-03],\n",
      "        [ 3.0045e-03, -1.6839e-03, -1.2904e-03,  ..., -4.3363e-03,\n",
      "          4.0137e-03, -3.5055e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight \n",
      "value: tensor([[ 4.5847e-03,  2.4728e-03, -7.7772e-04,  ...,  3.7608e-03,\n",
      "         -5.3792e-03, -2.8193e-03],\n",
      "        [ 1.1020e-02,  1.6800e-03,  3.8916e-03,  ..., -3.4073e-03,\n",
      "          9.4547e-03, -2.8370e-04],\n",
      "        [ 6.7839e-04, -6.1576e-03,  5.8055e-03,  ..., -8.2925e-05,\n",
      "          2.7118e-03, -1.9812e-04],\n",
      "        ...,\n",
      "        [ 1.3949e-03, -1.3218e-03, -3.7045e-03,  ..., -6.5058e-03,\n",
      "         -5.6040e-03, -3.3664e-04],\n",
      "        [-8.1410e-03,  1.5463e-03,  2.5744e-04,  ..., -3.3058e-03,\n",
      "         -6.3856e-03,  2.4124e-03],\n",
      "        [-7.3917e-04, -2.8164e-03, -3.7597e-03,  ...,  2.8952e-03,\n",
      "         -8.9977e-03, -3.5496e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight \n",
      "value: tensor([[ 0.0033, -0.0013,  0.0020,  ...,  0.0053,  0.0023, -0.0045],\n",
      "        [-0.0099, -0.0093,  0.0092,  ...,  0.0028, -0.0043,  0.0027],\n",
      "        [ 0.0021,  0.0034,  0.0035,  ..., -0.0001,  0.0104, -0.0065],\n",
      "        ...,\n",
      "        [-0.0021, -0.0046,  0.0070,  ...,  0.0004,  0.0047, -0.0010],\n",
      "        [-0.0012,  0.0041, -0.0028,  ...,  0.0005,  0.0003,  0.0058],\n",
      "        [-0.0025, -0.0043,  0.0016,  ..., -0.0020,  0.0009, -0.0063]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight \n",
      "value: tensor([[-1.8656e-03, -3.1967e-03, -1.5873e-03,  ..., -6.1294e-03,\n",
      "         -6.0870e-03, -5.1594e-03],\n",
      "        [ 2.5361e-04,  6.2308e-03,  5.2193e-03,  ..., -1.5751e-03,\n",
      "          5.0426e-03, -5.6764e-03],\n",
      "        [ 7.5297e-03, -3.5481e-04, -4.3194e-03,  ..., -2.7924e-04,\n",
      "          4.6226e-03,  4.6502e-03],\n",
      "        ...,\n",
      "        [ 1.4381e-03, -1.0810e-03,  5.2376e-03,  ..., -8.3206e-04,\n",
      "         -7.4554e-03, -3.1155e-03],\n",
      "        [ 4.6828e-03, -2.0892e-03,  3.7578e-03,  ...,  1.0978e-03,\n",
      "         -1.4317e-03,  3.2156e-03],\n",
      "        [ 1.4728e-03,  8.8110e-04,  9.9762e-04,  ..., -2.5399e-03,\n",
      "          6.6363e-05,  5.6085e-04]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight \n",
      "value: tensor([[-0.0002,  0.0040, -0.0015,  ...,  0.0023,  0.0009, -0.0033],\n",
      "        [ 0.0022, -0.0042,  0.0010,  ...,  0.0016, -0.0021,  0.0027],\n",
      "        [ 0.0032, -0.0013, -0.0015,  ...,  0.0050, -0.0029, -0.0026],\n",
      "        ...,\n",
      "        [ 0.0009, -0.0045, -0.0001,  ...,  0.0027,  0.0027,  0.0022],\n",
      "        [ 0.0010, -0.0027,  0.0029,  ...,  0.0010, -0.0015,  0.0013],\n",
      "        [-0.0033, -0.0012,  0.0025,  ..., -0.0013, -0.0013, -0.0064]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight \n",
      "value: tensor([[-6.3831e-04,  7.8847e-04, -2.1371e-04,  ...,  2.8133e-04,\n",
      "         -3.1756e-03,  7.7815e-05],\n",
      "        [ 8.0639e-04, -8.6731e-04, -4.2078e-05,  ..., -6.6790e-04,\n",
      "          1.7044e-03,  3.2337e-04],\n",
      "        [-6.5563e-04,  1.2286e-05,  4.2798e-04,  ...,  9.5113e-04,\n",
      "          1.4623e-04,  4.9603e-04],\n",
      "        ...,\n",
      "        [-1.4532e-03, -8.1081e-04, -2.1394e-04,  ...,  2.1083e-03,\n",
      "          8.5958e-05, -1.0524e-03],\n",
      "        [-1.8401e-03, -5.4771e-04,  7.0251e-05,  ...,  6.7541e-05,\n",
      "          2.4709e-04,  1.5287e-04],\n",
      "        [-4.3869e-04, -2.4813e-04, -9.2750e-04,  ...,  6.7394e-04,\n",
      "         -1.8145e-04, -2.4809e-04]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight \n",
      "value: tensor([[ 2.9947e-03, -3.2787e-03,  9.8440e-05,  ...,  6.1006e-03,\n",
      "          5.7752e-03, -2.3558e-03],\n",
      "        [ 6.4150e-03,  3.9469e-03, -2.7895e-03,  ...,  7.4923e-04,\n",
      "         -2.0153e-03, -5.5853e-04],\n",
      "        [-4.6464e-03, -4.1012e-03, -3.4849e-03,  ..., -3.2134e-03,\n",
      "          2.9622e-03,  4.0711e-03],\n",
      "        ...,\n",
      "        [-7.3106e-03, -1.6461e-03,  2.6873e-04,  ..., -4.0236e-03,\n",
      "          2.6739e-03, -4.6905e-03],\n",
      "        [ 2.5557e-03, -5.1214e-03, -5.8027e-04,  ..., -1.8158e-03,\n",
      "          2.7151e-03,  5.1752e-03],\n",
      "        [ 3.6435e-03, -1.1212e-02,  6.1483e-03,  ...,  6.0557e-03,\n",
      "          4.5920e-03,  5.3275e-04]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight \n",
      "value: tensor([[-0.0004,  0.0051, -0.0028,  ..., -0.0015,  0.0056,  0.0033],\n",
      "        [ 0.0004, -0.0014,  0.0081,  ..., -0.0024,  0.0007,  0.0008],\n",
      "        [ 0.0079, -0.0021, -0.0012,  ...,  0.0023,  0.0015,  0.0099],\n",
      "        ...,\n",
      "        [ 0.0021, -0.0033, -0.0003,  ..., -0.0029,  0.0048,  0.0007],\n",
      "        [ 0.0001,  0.0089, -0.0057,  ...,  0.0005, -0.0006, -0.0097],\n",
      "        [ 0.0041, -0.0034,  0.0036,  ..., -0.0002, -0.0025, -0.0036]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight \n",
      "value: tensor([[-0.0002, -0.0004, -0.0012,  ..., -0.0042, -0.0021, -0.0024],\n",
      "        [-0.0019,  0.0028,  0.0011,  ..., -0.0060,  0.0015,  0.0014],\n",
      "        [-0.0009, -0.0051, -0.0005,  ...,  0.0057,  0.0017,  0.0015],\n",
      "        ...,\n",
      "        [-0.0024, -0.0015, -0.0063,  ..., -0.0037,  0.0023,  0.0050],\n",
      "        [-0.0030, -0.0031, -0.0014,  ...,  0.0027,  0.0013,  0.0045],\n",
      "        [ 0.0016,  0.0024,  0.0070,  ..., -0.0067, -0.0017, -0.0013]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight \n",
      "value: tensor([[ 1.2564e-03,  1.1157e-03,  1.2586e-03,  ...,  7.5369e-04,\n",
      "          3.0338e-04, -1.7474e-03],\n",
      "        [ 4.9530e-04,  1.6739e-03,  2.4743e-04,  ..., -1.3010e-03,\n",
      "          4.7681e-03, -3.5627e-04],\n",
      "        [-1.7030e-05, -6.9106e-04,  6.0757e-05,  ..., -6.7341e-04,\n",
      "          2.3748e-04,  7.5927e-04],\n",
      "        ...,\n",
      "        [-2.3608e-03,  1.6009e-03, -5.0858e-03,  ..., -9.0432e-04,\n",
      "          4.2808e-05, -7.5953e-04],\n",
      "        [ 6.5688e-04, -1.4161e-03,  6.0979e-04,  ..., -1.7816e-04,\n",
      "         -3.8688e-03,  1.9228e-03],\n",
      "        [-2.2410e-04,  3.4909e-05,  1.8431e-04,  ...,  6.4959e-04,\n",
      "         -1.7002e-03, -3.7893e-04]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight \n",
      "value: tensor([[ 2.7272e-03, -6.6737e-03,  5.4618e-04,  ...,  6.5977e-04,\n",
      "         -3.1676e-03,  5.9019e-03],\n",
      "        [ 3.5088e-04,  6.7978e-03, -2.7784e-05,  ..., -3.5752e-03,\n",
      "         -6.6906e-04, -5.4726e-03],\n",
      "        [-2.6335e-04,  4.6678e-04, -1.1895e-03,  ...,  6.5950e-05,\n",
      "          2.7324e-03, -1.2276e-04],\n",
      "        ...,\n",
      "        [ 7.3093e-03,  4.4176e-03, -9.2779e-04,  ..., -1.5624e-03,\n",
      "          1.8140e-03, -4.6646e-03],\n",
      "        [-1.2811e-03, -3.2651e-03,  5.4584e-03,  ...,  2.3019e-03,\n",
      "          1.8331e-03, -2.5256e-03],\n",
      "        [ 4.6513e-03,  4.1921e-03,  3.8338e-03,  ...,  6.2712e-03,\n",
      "         -3.3398e-03, -5.6274e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight \n",
      "value: tensor([[-4.1299e-04, -3.5676e-03,  1.9420e-03,  ...,  2.7209e-03,\n",
      "          2.5038e-03, -5.2719e-03],\n",
      "        [-1.7656e-03,  9.1162e-04, -4.4482e-03,  ..., -3.3707e-05,\n",
      "         -2.7838e-03,  8.0920e-04],\n",
      "        [ 1.1780e-03,  5.8582e-03, -1.0446e-03,  ..., -2.7263e-03,\n",
      "         -7.8998e-04,  1.1976e-03],\n",
      "        ...,\n",
      "        [-2.6152e-03,  6.2094e-04,  3.4687e-04,  ..., -1.8001e-03,\n",
      "          2.5109e-03, -2.6528e-04],\n",
      "        [-1.7289e-03, -1.4470e-03,  1.0634e-03,  ...,  9.0555e-05,\n",
      "          2.7702e-03, -6.4789e-03],\n",
      "        [-3.0795e-03, -3.8861e-03,  4.8317e-03,  ...,  2.3729e-04,\n",
      "         -2.5187e-03,  2.6274e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight \n",
      "value: tensor([[-3.4494e-03, -3.0548e-03,  6.8391e-03,  ..., -7.1958e-03,\n",
      "          6.4207e-03,  1.4414e-03],\n",
      "        [-6.1200e-03, -3.1907e-03,  4.0174e-03,  ..., -4.0870e-03,\n",
      "          1.4828e-03, -2.0198e-03],\n",
      "        [ 4.4450e-03,  1.4968e-03, -1.7677e-03,  ...,  9.4862e-04,\n",
      "          3.9264e-05,  7.8685e-04],\n",
      "        ...,\n",
      "        [ 1.0919e-03, -2.2074e-03,  2.8472e-03,  ...,  7.5369e-04,\n",
      "          4.3350e-03,  7.9623e-04],\n",
      "        [-1.9995e-03,  8.3693e-03, -6.6271e-03,  ...,  7.8926e-03,\n",
      "          1.5951e-03,  2.1003e-03],\n",
      "        [ 4.5608e-04,  1.3606e-03, -7.8028e-03,  ...,  5.2628e-04,\n",
      "         -4.7524e-03,  6.3290e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight \n",
      "value: tensor([[-6.1369e-04, -3.7667e-03, -4.8214e-05,  ..., -5.6442e-03,\n",
      "          2.8240e-03, -2.5931e-03],\n",
      "        [ 3.6614e-03, -1.1420e-03, -2.7714e-03,  ...,  3.4012e-03,\n",
      "         -5.6403e-03,  6.9750e-03],\n",
      "        [-3.4962e-03,  8.1487e-04,  4.1332e-03,  ..., -1.9186e-03,\n",
      "         -4.5566e-03, -3.1392e-03],\n",
      "        ...,\n",
      "        [-3.8514e-03,  1.7801e-03, -8.7222e-03,  ..., -1.9567e-03,\n",
      "          1.3388e-04,  3.2685e-03],\n",
      "        [-4.3632e-03, -6.2643e-03,  1.6409e-03,  ..., -4.2099e-03,\n",
      "          4.8566e-03, -4.9573e-03],\n",
      "        [-4.6354e-04,  4.4612e-03,  4.8955e-03,  ...,  4.7319e-03,\n",
      "          8.1519e-04,  6.0817e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight \n",
      "value: tensor([[ 0.0071, -0.0003,  0.0084,  ...,  0.0079,  0.0031, -0.0018],\n",
      "        [ 0.0028,  0.0031, -0.0048,  ...,  0.0047,  0.0044, -0.0003],\n",
      "        [ 0.0040, -0.0002, -0.0085,  ..., -0.0018, -0.0015,  0.0032],\n",
      "        ...,\n",
      "        [ 0.0080,  0.0016, -0.0064,  ...,  0.0034, -0.0032,  0.0073],\n",
      "        [ 0.0018, -0.0071, -0.0039,  ...,  0.0002,  0.0084, -0.0118],\n",
      "        [-0.0042, -0.0031,  0.0017,  ..., -0.0045, -0.0006, -0.0041]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight \n",
      "value: tensor([[-4.7403e-04,  6.3556e-03, -3.8314e-03,  ..., -9.0258e-05,\n",
      "          2.4794e-03, -4.8615e-04],\n",
      "        [-3.2807e-03,  1.9187e-03, -3.6326e-03,  ..., -2.2321e-03,\n",
      "          7.2083e-04, -2.4889e-03],\n",
      "        [-8.3363e-04,  1.1149e-03,  6.2296e-04,  ...,  4.4608e-03,\n",
      "          2.0525e-02,  3.4584e-03],\n",
      "        ...,\n",
      "        [-8.4809e-03,  4.1741e-03, -4.0184e-03,  ...,  1.4187e-03,\n",
      "          7.2167e-03,  3.0536e-03],\n",
      "        [ 1.8066e-03, -1.3576e-03, -1.1424e-03,  ...,  3.7510e-04,\n",
      "         -7.2134e-03, -5.4681e-03],\n",
      "        [-4.4148e-03,  1.5693e-03, -6.2276e-03,  ..., -5.4358e-04,\n",
      "         -4.9586e-03, -3.1162e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.proj_in.lora.down.weight \n",
      "value: tensor([[[[-7.8513e-03]],\n",
      "\n",
      "         [[-7.9696e-03]],\n",
      "\n",
      "         [[-7.9526e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.7908e-03]],\n",
      "\n",
      "         [[-8.9674e-03]],\n",
      "\n",
      "         [[-1.0587e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 7.9170e-04]],\n",
      "\n",
      "         [[ 1.8723e-03]],\n",
      "\n",
      "         [[-4.4928e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.8712e-03]],\n",
      "\n",
      "         [[-2.3517e-04]],\n",
      "\n",
      "         [[ 7.4767e-04]]],\n",
      "\n",
      "\n",
      "        [[[-6.6264e-03]],\n",
      "\n",
      "         [[ 9.4472e-04]],\n",
      "\n",
      "         [[-3.9426e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.7600e-04]],\n",
      "\n",
      "         [[ 1.6717e-03]],\n",
      "\n",
      "         [[-4.3837e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.4666e-03]],\n",
      "\n",
      "         [[-1.2778e-03]],\n",
      "\n",
      "         [[-2.4908e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.4298e-03]],\n",
      "\n",
      "         [[-5.3237e-03]],\n",
      "\n",
      "         [[ 8.4626e-05]]],\n",
      "\n",
      "\n",
      "        [[[-6.3578e-04]],\n",
      "\n",
      "         [[-5.9817e-04]],\n",
      "\n",
      "         [[ 3.6881e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.7123e-03]],\n",
      "\n",
      "         [[ 3.8522e-03]],\n",
      "\n",
      "         [[ 5.7903e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 5.8008e-03]],\n",
      "\n",
      "         [[-9.8251e-04]],\n",
      "\n",
      "         [[-3.2391e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0669e-03]],\n",
      "\n",
      "         [[ 4.2571e-03]],\n",
      "\n",
      "         [[ 6.3756e-03]]]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.proj_in.lora.up.weight \n",
      "value: tensor([[[[ 5.1399e-03]],\n",
      "\n",
      "         [[ 2.4077e-03]],\n",
      "\n",
      "         [[-3.4974e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6579e-04]],\n",
      "\n",
      "         [[-3.3721e-03]],\n",
      "\n",
      "         [[-2.8031e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4424e-03]],\n",
      "\n",
      "         [[-3.7650e-03]],\n",
      "\n",
      "         [[-2.3823e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.6757e-03]],\n",
      "\n",
      "         [[ 7.8545e-03]],\n",
      "\n",
      "         [[ 3.4132e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.8610e-03]],\n",
      "\n",
      "         [[-2.9338e-03]],\n",
      "\n",
      "         [[ 3.3981e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.1807e-03]],\n",
      "\n",
      "         [[-6.9151e-03]],\n",
      "\n",
      "         [[-2.5346e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-8.2644e-03]],\n",
      "\n",
      "         [[ 3.9971e-04]],\n",
      "\n",
      "         [[ 9.3992e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.0283e-04]],\n",
      "\n",
      "         [[-6.9171e-03]],\n",
      "\n",
      "         [[-8.6669e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 9.4572e-03]],\n",
      "\n",
      "         [[ 1.3000e-05]],\n",
      "\n",
      "         [[ 1.9385e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.5967e-03]],\n",
      "\n",
      "         [[-2.4504e-04]],\n",
      "\n",
      "         [[-3.6831e-03]]],\n",
      "\n",
      "\n",
      "        [[[-5.8838e-03]],\n",
      "\n",
      "         [[ 4.5251e-03]],\n",
      "\n",
      "         [[ 5.0425e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.0776e-04]],\n",
      "\n",
      "         [[ 6.4779e-04]],\n",
      "\n",
      "         [[-5.1285e-03]]]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.proj_out.lora.down.weight \n",
      "value: tensor([[[[-0.0033]],\n",
      "\n",
      "         [[ 0.0021]],\n",
      "\n",
      "         [[-0.0047]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0042]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[ 0.0126]]],\n",
      "\n",
      "\n",
      "        [[[-0.0019]],\n",
      "\n",
      "         [[-0.0084]],\n",
      "\n",
      "         [[-0.0044]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0022]],\n",
      "\n",
      "         [[-0.0029]],\n",
      "\n",
      "         [[-0.0103]]],\n",
      "\n",
      "\n",
      "        [[[-0.0050]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         [[ 0.0043]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0053]],\n",
      "\n",
      "         [[ 0.0040]],\n",
      "\n",
      "         [[ 0.0072]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0054]],\n",
      "\n",
      "         [[-0.0063]],\n",
      "\n",
      "         [[-0.0018]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         [[-0.0042]],\n",
      "\n",
      "         [[ 0.0010]]],\n",
      "\n",
      "\n",
      "        [[[-0.0084]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         [[-0.0093]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0069]],\n",
      "\n",
      "         [[ 0.0018]],\n",
      "\n",
      "         [[ 0.0096]]],\n",
      "\n",
      "\n",
      "        [[[-0.0022]],\n",
      "\n",
      "         [[ 0.0052]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         [[-0.0015]],\n",
      "\n",
      "         [[-0.0017]]]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.proj_out.lora.up.weight \n",
      "value: tensor([[[[-0.0047]],\n",
      "\n",
      "         [[ 0.0055]],\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0009]],\n",
      "\n",
      "         [[-0.0068]],\n",
      "\n",
      "         [[ 0.0019]]],\n",
      "\n",
      "\n",
      "        [[[-0.0196]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0033]],\n",
      "\n",
      "         [[-0.0115]],\n",
      "\n",
      "         [[-0.0030]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0051]],\n",
      "\n",
      "         [[ 0.0030]],\n",
      "\n",
      "         [[-0.0037]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0005]],\n",
      "\n",
      "         [[ 0.0062]],\n",
      "\n",
      "         [[ 0.0042]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0035]],\n",
      "\n",
      "         [[ 0.0003]],\n",
      "\n",
      "         [[ 0.0030]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0007]],\n",
      "\n",
      "         [[ 0.0089]],\n",
      "\n",
      "         [[-0.0036]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0058]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         [[-0.0069]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0002]],\n",
      "\n",
      "         [[-0.0060]],\n",
      "\n",
      "         [[ 0.0080]]],\n",
      "\n",
      "\n",
      "        [[[-0.0094]],\n",
      "\n",
      "         [[ 0.0051]],\n",
      "\n",
      "         [[-0.0011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0004]],\n",
      "\n",
      "         [[-0.0047]],\n",
      "\n",
      "         [[ 0.0008]]]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight \n",
      "value: tensor([[ 0.0100, -0.0045, -0.0040,  ...,  0.0029, -0.0076,  0.0047],\n",
      "        [ 0.0036,  0.0008,  0.0095,  ...,  0.0037, -0.0018,  0.0047],\n",
      "        [ 0.0003,  0.0014, -0.0036,  ...,  0.0040,  0.0022,  0.0005],\n",
      "        ...,\n",
      "        [-0.0009, -0.0105,  0.0047,  ...,  0.0026,  0.0010,  0.0034],\n",
      "        [-0.0082,  0.0173, -0.0011,  ...,  0.0058,  0.0017, -0.0034],\n",
      "        [-0.0026, -0.0043,  0.0014,  ...,  0.0056, -0.0020,  0.0002]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight \n",
      "value: tensor([[-0.0023,  0.0012,  0.0002,  ..., -0.0044, -0.0036,  0.0027],\n",
      "        [ 0.0020,  0.0025,  0.0024,  ...,  0.0003, -0.0015,  0.0033],\n",
      "        [ 0.0034,  0.0002, -0.0009,  ...,  0.0012, -0.0025,  0.0015],\n",
      "        ...,\n",
      "        [ 0.0018,  0.0019,  0.0061,  ...,  0.0054, -0.0101,  0.0025],\n",
      "        [ 0.0050, -0.0036, -0.0029,  ..., -0.0004, -0.0070,  0.0036],\n",
      "        [ 0.0023, -0.0007,  0.0026,  ...,  0.0002, -0.0073,  0.0042]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight \n",
      "value: tensor([[ 0.0053,  0.0032,  0.0057,  ..., -0.0058,  0.0106, -0.0035],\n",
      "        [ 0.0023, -0.0038,  0.0067,  ...,  0.0021,  0.0038, -0.0002],\n",
      "        [-0.0012,  0.0018,  0.0046,  ...,  0.0060, -0.0011,  0.0068],\n",
      "        ...,\n",
      "        [ 0.0009,  0.0007,  0.0029,  ..., -0.0003,  0.0037, -0.0018],\n",
      "        [ 0.0155, -0.0045,  0.0069,  ..., -0.0221, -0.0002,  0.0019],\n",
      "        [-0.0010, -0.0034, -0.0017,  ..., -0.0027,  0.0030,  0.0070]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight \n",
      "value: tensor([[-5.2638e-03,  3.6994e-03,  3.6665e-03,  ...,  9.5543e-05,\n",
      "          1.2235e-03,  1.4311e-03],\n",
      "        [-3.8527e-04, -4.0845e-04, -1.7732e-03,  ..., -7.5885e-03,\n",
      "          2.2956e-03,  2.6038e-03],\n",
      "        [ 3.9479e-03,  4.5187e-03, -2.8549e-03,  ..., -8.4055e-04,\n",
      "          2.3713e-03, -7.5745e-04],\n",
      "        ...,\n",
      "        [ 1.5468e-03, -6.9543e-04,  5.4631e-03,  ..., -8.0216e-04,\n",
      "          1.9622e-03,  6.0144e-03],\n",
      "        [-2.8071e-03,  4.3344e-04,  1.7981e-04,  ..., -1.0765e-03,\n",
      "          7.6533e-03,  1.5395e-03],\n",
      "        [-7.0214e-03,  2.9132e-03, -3.7309e-03,  ...,  1.3787e-03,\n",
      "         -2.2104e-03, -3.4164e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight \n",
      "value: tensor([[-0.0032, -0.0053,  0.0047,  ..., -0.0039, -0.0026, -0.0022],\n",
      "        [ 0.0089, -0.0071, -0.0050,  ...,  0.0033, -0.0004,  0.0046],\n",
      "        [ 0.0016, -0.0029, -0.0021,  ...,  0.0004,  0.0126, -0.0043],\n",
      "        ...,\n",
      "        [ 0.0086, -0.0071, -0.0022,  ..., -0.0010,  0.0033,  0.0022],\n",
      "        [-0.0025, -0.0076, -0.0126,  ..., -0.0043,  0.0002,  0.0020],\n",
      "        [ 0.0014, -0.0024, -0.0070,  ...,  0.0018,  0.0067,  0.0027]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight \n",
      "value: tensor([[-4.3409e-03, -3.0389e-05,  1.5112e-03,  ..., -1.5354e-04,\n",
      "         -4.9145e-03, -2.4306e-03],\n",
      "        [-1.0336e-02,  2.5644e-03,  1.5217e-03,  ..., -6.1570e-04,\n",
      "         -9.5366e-03,  8.6694e-04],\n",
      "        [-2.0826e-03, -6.5876e-04,  6.7205e-03,  ..., -2.5589e-03,\n",
      "          1.0117e-02,  5.1669e-04],\n",
      "        ...,\n",
      "        [-1.5002e-02, -1.2177e-03, -3.7135e-03,  ..., -7.4603e-03,\n",
      "         -1.5857e-02,  1.9314e-03],\n",
      "        [-9.2850e-03, -4.1163e-04, -3.1451e-03,  ...,  2.7990e-03,\n",
      "         -7.2020e-03,  8.4723e-03],\n",
      "        [-6.2092e-03,  3.2170e-03, -2.7299e-03,  ..., -9.4012e-04,\n",
      "         -1.0716e-02,  2.4437e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight \n",
      "value: tensor([[-0.0051,  0.0009,  0.0038,  ..., -0.0023,  0.0010,  0.0024],\n",
      "        [ 0.0015,  0.0028,  0.0013,  ..., -0.0048, -0.0059, -0.0021],\n",
      "        [ 0.0040,  0.0003, -0.0045,  ...,  0.0029,  0.0027,  0.0003],\n",
      "        ...,\n",
      "        [-0.0045, -0.0031, -0.0031,  ...,  0.0034, -0.0004, -0.0007],\n",
      "        [-0.0013, -0.0041, -0.0030,  ...,  0.0039, -0.0042, -0.0029],\n",
      "        [-0.0018, -0.0016, -0.0036,  ..., -0.0031, -0.0055, -0.0040]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight \n",
      "value: tensor([[ 0.0011, -0.0018, -0.0021,  ..., -0.0027,  0.0047,  0.0022],\n",
      "        [-0.0045, -0.0016,  0.0002,  ...,  0.0015,  0.0076, -0.0008],\n",
      "        [-0.0022, -0.0076, -0.0015,  ..., -0.0025, -0.0016, -0.0030],\n",
      "        ...,\n",
      "        [ 0.0055, -0.0005,  0.0016,  ...,  0.0042, -0.0083, -0.0006],\n",
      "        [-0.0049, -0.0003,  0.0028,  ..., -0.0029, -0.0062, -0.0010],\n",
      "        [ 0.0015,  0.0004, -0.0002,  ..., -0.0004,  0.0002, -0.0060]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight \n",
      "value: tensor([[-0.0028, -0.0051, -0.0074,  ..., -0.0002, -0.0027,  0.0011],\n",
      "        [-0.0025,  0.0041,  0.0034,  ...,  0.0034, -0.0035, -0.0035],\n",
      "        [ 0.0027,  0.0014, -0.0040,  ...,  0.0012, -0.0023, -0.0030],\n",
      "        ...,\n",
      "        [-0.0045,  0.0007, -0.0021,  ..., -0.0015,  0.0038, -0.0021],\n",
      "        [ 0.0026,  0.0029,  0.0013,  ..., -0.0003, -0.0017,  0.0032],\n",
      "        [ 0.0017,  0.0036, -0.0003,  ...,  0.0044, -0.0027,  0.0011]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight \n",
      "value: tensor([[-9.2924e-04, -4.7624e-04, -9.1675e-05,  ...,  2.6221e-04,\n",
      "         -2.0138e-03, -1.1605e-03],\n",
      "        [-1.5810e-03,  4.8497e-04, -1.5734e-03,  ..., -9.0087e-05,\n",
      "          4.0053e-03,  2.0080e-04],\n",
      "        [-1.5040e-03,  4.7774e-04,  4.0792e-05,  ..., -4.2483e-04,\n",
      "          3.7126e-03, -3.4772e-04],\n",
      "        ...,\n",
      "        [ 8.5789e-04, -1.1538e-03, -8.5204e-04,  ..., -3.9569e-04,\n",
      "         -4.1461e-04, -4.1799e-04],\n",
      "        [ 1.2658e-03,  1.6058e-04, -5.5890e-04,  ..., -4.4753e-04,\n",
      "          1.0847e-03, -1.3451e-04],\n",
      "        [ 1.7686e-04,  7.6614e-04, -1.3864e-03,  ..., -7.1321e-04,\n",
      "          1.2075e-03,  1.0361e-04]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight \n",
      "value: tensor([[ 0.0016,  0.0006,  0.0027,  ...,  0.0019,  0.0039,  0.0116],\n",
      "        [ 0.0027,  0.0062,  0.0032,  ..., -0.0006, -0.0082,  0.0020],\n",
      "        [-0.0006, -0.0003,  0.0048,  ..., -0.0011, -0.0036,  0.0007],\n",
      "        ...,\n",
      "        [ 0.0006, -0.0020,  0.0096,  ...,  0.0033, -0.0039, -0.0031],\n",
      "        [-0.0082, -0.0018, -0.0015,  ...,  0.0072,  0.0053,  0.0028],\n",
      "        [ 0.0063, -0.0022,  0.0015,  ...,  0.0006,  0.0029, -0.0022]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight \n",
      "value: tensor([[ 6.7366e-03,  4.1118e-03,  3.6173e-03,  ...,  5.1087e-03,\n",
      "         -4.3686e-03,  3.1460e-03],\n",
      "        [-5.0291e-03,  4.8655e-03, -2.1330e-03,  ...,  2.9788e-03,\n",
      "         -2.5478e-03, -1.3362e-03],\n",
      "        [ 1.3502e-03, -4.2610e-03,  4.8576e-03,  ...,  8.1459e-04,\n",
      "         -5.4294e-03, -4.0791e-03],\n",
      "        ...,\n",
      "        [-6.1154e-04, -1.0100e-02,  4.2272e-03,  ..., -4.9187e-03,\n",
      "          5.8961e-03, -5.0605e-03],\n",
      "        [-1.5117e-03,  1.7369e-03,  5.5195e-03,  ...,  4.5946e-03,\n",
      "         -9.9883e-04,  1.4818e-03],\n",
      "        [ 5.4698e-03, -5.1258e-06, -5.6070e-03,  ...,  1.6153e-03,\n",
      "          1.3034e-03, -7.6934e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight \n",
      "value: tensor([[ 6.2340e-04, -4.1295e-04,  8.9642e-04,  ...,  4.6016e-04,\n",
      "         -4.4870e-03,  2.2551e-03],\n",
      "        [ 5.1583e-03,  1.5864e-03,  7.5742e-03,  ..., -9.2845e-05,\n",
      "          4.0649e-03, -4.7808e-03],\n",
      "        [-5.7658e-03, -2.5008e-03, -4.5706e-03,  ...,  1.9299e-04,\n",
      "          4.0312e-03, -5.7544e-03],\n",
      "        ...,\n",
      "        [ 9.9718e-04,  2.3314e-03,  1.0872e-03,  ..., -2.4528e-03,\n",
      "          8.5626e-03, -3.7457e-03],\n",
      "        [ 8.9245e-03,  3.6347e-03,  1.0359e-02,  ...,  1.1139e-02,\n",
      "          2.7831e-04,  8.1938e-04],\n",
      "        [-2.6598e-03, -1.9227e-03,  2.2265e-03,  ...,  1.4721e-03,\n",
      "          3.4650e-03,  2.8441e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight \n",
      "value: tensor([[ 6.0672e-04,  2.3879e-04,  1.0082e-03,  ..., -3.4428e-04,\n",
      "         -9.3877e-04,  3.3480e-04],\n",
      "        [ 3.1881e-05, -2.2481e-03,  1.7021e-03,  ..., -1.3845e-04,\n",
      "         -9.1142e-03,  4.2286e-04],\n",
      "        [-9.6408e-04, -1.7645e-05,  1.4747e-03,  ...,  2.0597e-03,\n",
      "         -1.0405e-03, -1.1916e-04],\n",
      "        ...,\n",
      "        [-1.6189e-05, -1.6078e-03, -3.5291e-04,  ..., -1.4677e-03,\n",
      "          5.7324e-03,  1.9789e-04],\n",
      "        [-1.2922e-03,  1.0113e-03, -1.1712e-03,  ...,  1.5743e-03,\n",
      "         -6.8299e-03, -4.3844e-04],\n",
      "        [ 1.9759e-03,  1.8421e-04,  1.5102e-03,  ..., -1.4284e-03,\n",
      "         -1.3689e-03, -1.4439e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight \n",
      "value: tensor([[ 8.7042e-03,  4.7709e-03,  5.0055e-03,  ..., -7.9959e-03,\n",
      "          6.6433e-05, -5.1332e-03],\n",
      "        [ 1.6163e-04, -4.2820e-03, -3.9723e-03,  ..., -1.7481e-03,\n",
      "         -2.5289e-03,  2.3159e-03],\n",
      "        [ 3.0655e-03, -5.2119e-04, -6.9201e-03,  ..., -1.7992e-03,\n",
      "         -2.3286e-03,  1.1717e-03],\n",
      "        ...,\n",
      "        [-2.1950e-03,  9.6600e-05,  9.1859e-04,  ...,  6.6343e-03,\n",
      "          3.0333e-03, -2.5916e-04],\n",
      "        [-3.0813e-04,  6.3145e-03, -3.1252e-03,  ..., -6.2397e-03,\n",
      "          6.1220e-03, -9.2239e-04],\n",
      "        [ 8.0444e-04,  6.4643e-03,  6.1584e-03,  ..., -6.6953e-04,\n",
      "          6.0993e-03, -4.6140e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight \n",
      "value: tensor([[-1.0884e-04,  2.4101e-03, -1.5702e-03,  ...,  2.5948e-03,\n",
      "          3.3731e-04,  6.3120e-03],\n",
      "        [ 3.3286e-03, -5.9573e-03, -2.1557e-03,  ..., -2.2378e-04,\n",
      "         -1.9266e-03, -9.7018e-03],\n",
      "        [-3.1776e-03, -5.6968e-03,  7.7391e-04,  ..., -3.9411e-03,\n",
      "         -3.2928e-03, -1.6454e-03],\n",
      "        ...,\n",
      "        [ 3.5491e-03,  1.4715e-03,  1.4322e-03,  ..., -2.8620e-03,\n",
      "          1.5852e-03, -8.2123e-03],\n",
      "        [-1.5065e-03, -7.4344e-04, -9.5900e-04,  ...,  2.2171e-03,\n",
      "          2.7402e-03,  8.1941e-03],\n",
      "        [ 4.4927e-03, -3.6292e-03,  3.2234e-03,  ..., -5.7700e-04,\n",
      "         -4.8392e-05, -8.4044e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight \n",
      "value: tensor([[ 2.6809e-03, -1.7986e-03, -2.7891e-03,  ..., -1.0608e-03,\n",
      "          7.2954e-03,  5.3737e-03],\n",
      "        [ 2.4756e-03,  3.4733e-03, -5.0564e-03,  ...,  4.5661e-03,\n",
      "          6.5379e-04, -5.4419e-04],\n",
      "        [-1.7304e-03,  1.0444e-03, -5.5578e-03,  ...,  1.0114e-03,\n",
      "          4.9420e-03, -1.7853e-03],\n",
      "        ...,\n",
      "        [-3.7014e-03,  5.1273e-03, -7.4750e-03,  ..., -8.1288e-03,\n",
      "          3.8504e-03, -1.8188e-03],\n",
      "        [-5.9137e-05,  7.9138e-03,  1.5449e-02,  ...,  8.6958e-03,\n",
      "         -1.0678e-02, -1.0748e-02],\n",
      "        [-2.9388e-03, -7.4255e-04,  5.1210e-03,  ..., -5.7272e-03,\n",
      "         -1.8259e-04, -4.7578e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight \n",
      "value: tensor([[-0.0055,  0.0026,  0.0082,  ..., -0.0030, -0.0038, -0.0002],\n",
      "        [ 0.0022,  0.0034,  0.0008,  ...,  0.0060,  0.0021, -0.0115],\n",
      "        [-0.0117,  0.0006, -0.0010,  ..., -0.0032, -0.0020,  0.0061],\n",
      "        ...,\n",
      "        [-0.0096, -0.0018,  0.0129,  ..., -0.0042,  0.0029, -0.0008],\n",
      "        [-0.0026,  0.0088,  0.0044,  ..., -0.0016,  0.0070,  0.0046],\n",
      "        [ 0.0069, -0.0023, -0.0020,  ..., -0.0037,  0.0077,  0.0014]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight \n",
      "value: tensor([[-4.4797e-03, -9.1964e-03, -7.0116e-03,  ...,  3.4480e-03,\n",
      "         -3.1542e-03, -1.0674e-04],\n",
      "        [ 7.6662e-03, -1.9903e-03, -7.5369e-03,  ..., -1.4904e-03,\n",
      "         -2.0251e-03,  5.4383e-03],\n",
      "        [-2.4120e-03,  5.7017e-03, -4.3734e-03,  ..., -5.6766e-03,\n",
      "          9.2469e-03, -3.8296e-03],\n",
      "        ...,\n",
      "        [ 3.4032e-03,  3.7415e-03,  3.7542e-03,  ..., -7.1071e-03,\n",
      "          9.4555e-04,  1.6320e-03],\n",
      "        [-4.6268e-03,  7.4836e-05,  7.5734e-03,  ...,  8.6360e-03,\n",
      "          9.5646e-03, -1.4303e-02],\n",
      "        [ 1.6684e-03, -2.0834e-03, -1.2869e-03,  ..., -6.3176e-03,\n",
      "          1.5185e-03, -6.2647e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight \n",
      "value: tensor([[-3.4883e-03,  2.9606e-03, -2.3537e-03,  ..., -7.5070e-04,\n",
      "          6.0038e-03,  4.7903e-03],\n",
      "        [ 8.8052e-03, -3.6535e-03, -4.6155e-03,  ...,  5.3479e-04,\n",
      "         -2.8817e-03,  2.5259e-03],\n",
      "        [-1.4022e-02,  2.0488e-03,  5.3988e-03,  ..., -3.7074e-03,\n",
      "          5.8015e-03, -1.6217e-03],\n",
      "        ...,\n",
      "        [-1.0033e-03, -1.1392e-03,  6.0815e-03,  ..., -2.0814e-03,\n",
      "         -7.5129e-04,  2.0306e-03],\n",
      "        [-3.4079e-03, -7.7488e-04, -4.1569e-03,  ...,  1.5920e-03,\n",
      "         -2.3740e-03, -3.8097e-03],\n",
      "        [ 7.2690e-03,  4.8144e-03,  4.5555e-03,  ..., -3.0494e-03,\n",
      "         -2.9228e-05, -1.3353e-04]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.proj_in.lora.down.weight \n",
      "value: tensor([[[[ 0.0023]],\n",
      "\n",
      "         [[ 0.0002]],\n",
      "\n",
      "         [[ 0.0022]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0027]],\n",
      "\n",
      "         [[-0.0105]],\n",
      "\n",
      "         [[ 0.0088]]],\n",
      "\n",
      "\n",
      "        [[[-0.0028]],\n",
      "\n",
      "         [[-0.0061]],\n",
      "\n",
      "         [[ 0.0083]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0064]],\n",
      "\n",
      "         [[ 0.0024]],\n",
      "\n",
      "         [[ 0.0029]]],\n",
      "\n",
      "\n",
      "        [[[-0.0098]],\n",
      "\n",
      "         [[ 0.0017]],\n",
      "\n",
      "         [[-0.0049]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0024]],\n",
      "\n",
      "         [[-0.0061]],\n",
      "\n",
      "         [[ 0.0025]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0054]],\n",
      "\n",
      "         [[-0.0046]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0111]],\n",
      "\n",
      "         [[-0.0036]],\n",
      "\n",
      "         [[-0.0005]]],\n",
      "\n",
      "\n",
      "        [[[-0.0013]],\n",
      "\n",
      "         [[-0.0085]],\n",
      "\n",
      "         [[-0.0103]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0032]],\n",
      "\n",
      "         [[ 0.0157]],\n",
      "\n",
      "         [[ 0.0047]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0028]],\n",
      "\n",
      "         [[-0.0023]],\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0076]],\n",
      "\n",
      "         [[-0.0073]],\n",
      "\n",
      "         [[ 0.0063]]]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.proj_in.lora.up.weight \n",
      "value: tensor([[[[-0.0030]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         [[-0.0035]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0021]],\n",
      "\n",
      "         [[-0.0027]],\n",
      "\n",
      "         [[-0.0138]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0035]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[-0.0042]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0061]],\n",
      "\n",
      "         [[-0.0054]],\n",
      "\n",
      "         [[-0.0021]]],\n",
      "\n",
      "\n",
      "        [[[-0.0147]],\n",
      "\n",
      "         [[-0.0047]],\n",
      "\n",
      "         [[-0.0013]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0014]],\n",
      "\n",
      "         [[ 0.0086]],\n",
      "\n",
      "         [[-0.0030]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0027]],\n",
      "\n",
      "         [[-0.0005]],\n",
      "\n",
      "         [[ 0.0011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0026]],\n",
      "\n",
      "         [[-0.0068]],\n",
      "\n",
      "         [[-0.0006]]],\n",
      "\n",
      "\n",
      "        [[[-0.0014]],\n",
      "\n",
      "         [[ 0.0061]],\n",
      "\n",
      "         [[ 0.0028]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0033]],\n",
      "\n",
      "         [[ 0.0001]],\n",
      "\n",
      "         [[ 0.0064]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0045]],\n",
      "\n",
      "         [[ 0.0001]],\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0045]],\n",
      "\n",
      "         [[ 0.0061]],\n",
      "\n",
      "         [[-0.0026]]]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.proj_out.lora.down.weight \n",
      "value: tensor([[[[ 0.0009]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[ 0.0036]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0127]],\n",
      "\n",
      "         [[-0.0005]],\n",
      "\n",
      "         [[-0.0015]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0051]],\n",
      "\n",
      "         [[ 0.0066]],\n",
      "\n",
      "         [[-0.0047]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[ 0.0013]],\n",
      "\n",
      "         [[-0.0074]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0113]],\n",
      "\n",
      "         [[-0.0131]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0116]],\n",
      "\n",
      "         [[-0.0019]],\n",
      "\n",
      "         [[ 0.0093]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0093]],\n",
      "\n",
      "         [[-0.0053]],\n",
      "\n",
      "         [[ 0.0089]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0034]],\n",
      "\n",
      "         [[ 0.0033]],\n",
      "\n",
      "         [[-0.0002]]],\n",
      "\n",
      "\n",
      "        [[[-0.0110]],\n",
      "\n",
      "         [[ 0.0070]],\n",
      "\n",
      "         [[ 0.0025]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0056]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[ 0.0041]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0059]],\n",
      "\n",
      "         [[ 0.0101]],\n",
      "\n",
      "         [[-0.0106]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0007]],\n",
      "\n",
      "         [[-0.0016]],\n",
      "\n",
      "         [[-0.0051]]]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.proj_out.lora.up.weight \n",
      "value: tensor([[[[ 0.0030]],\n",
      "\n",
      "         [[-0.0033]],\n",
      "\n",
      "         [[-0.0020]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[ 0.0006]],\n",
      "\n",
      "         [[ 0.0008]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0020]],\n",
      "\n",
      "         [[ 0.0016]],\n",
      "\n",
      "         [[-0.0009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0007]],\n",
      "\n",
      "         [[-0.0084]],\n",
      "\n",
      "         [[-0.0025]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0011]],\n",
      "\n",
      "         [[-0.0025]],\n",
      "\n",
      "         [[ 0.0078]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0069]],\n",
      "\n",
      "         [[-0.0052]],\n",
      "\n",
      "         [[-0.0127]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0102]],\n",
      "\n",
      "         [[-0.0034]],\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0025]],\n",
      "\n",
      "         [[ 0.0059]],\n",
      "\n",
      "         [[-0.0048]]],\n",
      "\n",
      "\n",
      "        [[[-0.0087]],\n",
      "\n",
      "         [[ 0.0019]],\n",
      "\n",
      "         [[ 0.0021]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         [[ 0.0033]],\n",
      "\n",
      "         [[-0.0033]]],\n",
      "\n",
      "\n",
      "        [[[-0.0015]],\n",
      "\n",
      "         [[ 0.0015]],\n",
      "\n",
      "         [[-0.0012]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0017]],\n",
      "\n",
      "         [[-0.0006]],\n",
      "\n",
      "         [[ 0.0074]]]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight \n",
      "value: tensor([[-1.1501e-02,  4.6599e-03, -8.4691e-03,  ..., -8.0671e-03,\n",
      "          2.6083e-03,  1.0436e-02],\n",
      "        [-2.2247e-03, -5.2271e-04, -3.1257e-03,  ...,  6.1278e-03,\n",
      "          1.9312e-05,  3.3557e-03],\n",
      "        [ 1.3795e-03,  4.5980e-03, -4.7815e-04,  ..., -1.1014e-03,\n",
      "         -5.3300e-03, -6.8703e-04],\n",
      "        ...,\n",
      "        [ 4.7809e-03,  3.0415e-03,  4.0229e-03,  ...,  2.6011e-03,\n",
      "          4.2565e-03,  1.9440e-03],\n",
      "        [ 1.7449e-02,  4.5284e-03,  2.2543e-02,  ...,  1.0145e-03,\n",
      "          5.7773e-03, -7.0368e-03],\n",
      "        [-3.5621e-03,  1.8062e-03,  2.9987e-03,  ..., -2.5221e-03,\n",
      "         -7.3173e-04,  2.1426e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight \n",
      "value: tensor([[-0.0032, -0.0052, -0.0044,  ..., -0.0025,  0.0086,  0.0030],\n",
      "        [-0.0057,  0.0009, -0.0024,  ...,  0.0039,  0.0066,  0.0031],\n",
      "        [ 0.0038,  0.0033,  0.0017,  ...,  0.0036, -0.0023, -0.0011],\n",
      "        ...,\n",
      "        [ 0.0081, -0.0023,  0.0094,  ..., -0.0013, -0.0123, -0.0005],\n",
      "        [ 0.0161, -0.0018, -0.0054,  ...,  0.0011, -0.0102, -0.0003],\n",
      "        [ 0.0102,  0.0020, -0.0033,  ...,  0.0001, -0.0020, -0.0041]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight \n",
      "value: tensor([[-1.0149e-03, -5.9922e-05, -5.1109e-03,  ...,  3.3771e-03,\n",
      "          3.4904e-03, -3.9408e-03],\n",
      "        [ 2.4863e-03, -4.2888e-03,  1.8979e-04,  ...,  2.2842e-03,\n",
      "          3.4428e-03,  6.3558e-03],\n",
      "        [-5.3846e-03, -4.3274e-03,  3.3226e-04,  ...,  3.7067e-04,\n",
      "         -3.0004e-03,  4.4144e-03],\n",
      "        ...,\n",
      "        [ 4.4156e-03,  1.1263e-03, -4.0529e-03,  ...,  4.5421e-03,\n",
      "          6.4808e-03,  1.8291e-03],\n",
      "        [-3.1438e-03, -1.9718e-03, -3.8360e-03,  ...,  9.0137e-03,\n",
      "         -7.2295e-03, -1.1943e-02],\n",
      "        [-5.1606e-03, -1.1144e-02,  2.1431e-03,  ..., -1.0169e-02,\n",
      "         -5.9295e-03,  5.1828e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight \n",
      "value: tensor([[-2.0823e-03, -1.3469e-03, -3.9183e-03,  ...,  3.6854e-03,\n",
      "         -2.9839e-03,  5.7737e-04],\n",
      "        [-5.4694e-03,  2.1175e-03,  2.8917e-03,  ..., -3.1564e-03,\n",
      "          7.4614e-03, -3.0183e-04],\n",
      "        [-7.0550e-03,  2.9351e-03, -1.0188e-02,  ..., -2.1452e-03,\n",
      "          6.2066e-03, -3.9257e-03],\n",
      "        ...,\n",
      "        [ 4.2001e-03, -3.6018e-03, -1.4432e-03,  ...,  3.9741e-03,\n",
      "         -1.1546e-02, -9.7750e-05],\n",
      "        [-2.2302e-03,  1.0548e-03,  1.4855e-03,  ...,  3.9419e-03,\n",
      "         -6.0635e-04,  3.3684e-03],\n",
      "        [ 1.0337e-03, -5.2451e-04,  5.3304e-04,  ..., -1.9084e-03,\n",
      "         -9.3352e-05, -1.8332e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight \n",
      "value: tensor([[-1.5699e-05,  1.2515e-02,  1.3706e-03,  ...,  2.8563e-03,\n",
      "         -2.6118e-03, -1.1797e-02],\n",
      "        [-6.6509e-04,  1.3970e-03, -2.2155e-04,  ...,  2.5193e-03,\n",
      "         -4.7863e-03,  6.7851e-03],\n",
      "        [ 1.7443e-03, -1.3263e-05, -3.5282e-03,  ...,  1.4845e-03,\n",
      "         -2.0748e-03, -4.7681e-03],\n",
      "        ...,\n",
      "        [ 1.1098e-03, -7.2646e-07, -9.5286e-04,  ..., -5.0768e-03,\n",
      "         -3.8745e-03,  2.6250e-04],\n",
      "        [-6.2024e-03, -8.0943e-03, -1.9012e-02,  ...,  1.2639e-03,\n",
      "         -3.9748e-03,  5.9877e-03],\n",
      "        [-1.5261e-03,  2.2338e-03,  1.1233e-03,  ...,  1.2418e-03,\n",
      "         -3.1165e-03,  1.5651e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight \n",
      "value: tensor([[-2.8475e-03,  2.7320e-03,  2.6660e-03,  ..., -1.3747e-03,\n",
      "         -7.5752e-03,  7.4744e-04],\n",
      "        [-1.6268e-03,  1.2677e-03, -8.1228e-03,  ..., -3.8688e-03,\n",
      "          2.6836e-03, -1.6119e-03],\n",
      "        [ 5.3843e-03, -3.7567e-03,  2.9540e-03,  ..., -1.9147e-03,\n",
      "          1.9677e-03, -2.9405e-05],\n",
      "        ...,\n",
      "        [ 2.1459e-02,  3.4110e-03, -4.3535e-06,  ..., -1.9035e-03,\n",
      "         -1.2691e-02,  8.2545e-04],\n",
      "        [ 1.3575e-02, -1.0921e-03, -2.0408e-04,  ..., -9.9137e-05,\n",
      "         -3.1912e-03,  8.4966e-04],\n",
      "        [-3.4545e-03,  1.2714e-03, -3.5189e-03,  ...,  9.3561e-04,\n",
      "          2.1700e-03, -3.1191e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight \n",
      "value: tensor([[ 3.6016e-03, -5.3285e-03, -3.6203e-03,  ..., -1.0156e-02,\n",
      "          4.2075e-03,  9.6790e-04],\n",
      "        [-1.3317e-03, -1.0520e-04,  9.3209e-04,  ..., -3.5616e-03,\n",
      "         -3.0872e-03, -1.5111e-03],\n",
      "        [-4.3971e-04, -2.5096e-03,  1.8131e-03,  ..., -2.5663e-03,\n",
      "          9.3634e-04,  8.3660e-05],\n",
      "        ...,\n",
      "        [ 4.7822e-03,  1.9345e-03, -5.6453e-03,  ..., -2.6875e-04,\n",
      "          6.1559e-03, -2.8006e-03],\n",
      "        [-6.4397e-03, -1.4889e-03,  1.2166e-03,  ...,  7.9974e-03,\n",
      "          1.0623e-02, -1.3592e-03],\n",
      "        [-4.9075e-03,  3.8114e-05, -6.3584e-04,  ...,  4.5154e-03,\n",
      "         -3.1159e-03,  8.6444e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight \n",
      "value: tensor([[-0.0020, -0.0044,  0.0010,  ..., -0.0025, -0.0040,  0.0039],\n",
      "        [-0.0026,  0.0023,  0.0030,  ..., -0.0015,  0.0065,  0.0012],\n",
      "        [-0.0033,  0.0022, -0.0020,  ..., -0.0020,  0.0062, -0.0008],\n",
      "        ...,\n",
      "        [-0.0022,  0.0062, -0.0011,  ...,  0.0068,  0.0016,  0.0030],\n",
      "        [ 0.0012,  0.0007, -0.0022,  ..., -0.0003, -0.0012,  0.0035],\n",
      "        [ 0.0028, -0.0032, -0.0016,  ..., -0.0041,  0.0072, -0.0002]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight \n",
      "value: tensor([[ 0.0030, -0.0004, -0.0019,  ...,  0.0027,  0.0010, -0.0028],\n",
      "        [-0.0085, -0.0001, -0.0029,  ...,  0.0066,  0.0022, -0.0031],\n",
      "        [-0.0025,  0.0012,  0.0013,  ..., -0.0037, -0.0032, -0.0019],\n",
      "        ...,\n",
      "        [-0.0017, -0.0013,  0.0002,  ...,  0.0045,  0.0022, -0.0049],\n",
      "        [-0.0050, -0.0045,  0.0016,  ..., -0.0023,  0.0003,  0.0015],\n",
      "        [ 0.0009,  0.0010,  0.0011,  ...,  0.0019, -0.0021, -0.0004]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight \n",
      "value: tensor([[-6.6858e-04, -5.9906e-04, -4.6822e-04,  ...,  4.8747e-04,\n",
      "          3.1383e-03, -7.7088e-04],\n",
      "        [ 1.0782e-03, -1.1580e-03,  4.8541e-04,  ...,  1.0990e-03,\n",
      "         -4.1008e-03,  1.3431e-04],\n",
      "        [-5.4197e-04,  1.5514e-03,  1.9855e-03,  ...,  4.2114e-04,\n",
      "         -1.8363e-03, -6.3496e-05],\n",
      "        ...,\n",
      "        [ 2.7525e-04, -9.4317e-04, -1.7010e-04,  ...,  5.3184e-04,\n",
      "          1.5178e-03,  5.1968e-04],\n",
      "        [-3.4207e-04, -4.8054e-04, -4.3797e-04,  ...,  8.2605e-04,\n",
      "          3.0459e-04, -6.9493e-05],\n",
      "        [ 6.5314e-04,  1.2091e-04,  2.0173e-04,  ...,  1.2874e-04,\n",
      "         -2.3765e-03,  4.0653e-04]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight \n",
      "value: tensor([[ 1.0571e-05,  4.0692e-03,  2.3763e-03,  ..., -2.6795e-03,\n",
      "         -5.4887e-04, -2.8306e-03],\n",
      "        [ 1.3781e-03, -6.9186e-03,  1.3002e-02,  ...,  3.7137e-03,\n",
      "         -3.6330e-03, -2.4614e-04],\n",
      "        [ 9.7129e-03, -1.3252e-02,  7.5386e-03,  ..., -9.8876e-04,\n",
      "         -6.1511e-03, -4.8192e-05],\n",
      "        ...,\n",
      "        [ 2.5998e-03, -4.1332e-03, -3.3355e-03,  ...,  5.5715e-03,\n",
      "          4.5368e-03,  7.5450e-03],\n",
      "        [ 3.5257e-04,  4.4272e-03,  4.5975e-03,  ..., -5.9819e-03,\n",
      "          7.2087e-04,  7.6657e-03],\n",
      "        [-3.0744e-03, -1.3178e-03, -3.7193e-03,  ...,  6.1644e-03,\n",
      "          6.6911e-03, -2.3699e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight \n",
      "value: tensor([[-8.2817e-03,  6.3013e-03,  1.3496e-03,  ..., -4.6082e-03,\n",
      "         -3.6658e-05, -5.9055e-03],\n",
      "        [ 2.6281e-03,  4.1239e-03, -3.6741e-03,  ..., -6.2006e-04,\n",
      "         -5.1132e-03,  5.6468e-03],\n",
      "        [-3.3407e-03, -8.6874e-04,  1.1570e-03,  ..., -2.4565e-03,\n",
      "         -4.4443e-03, -4.4789e-03],\n",
      "        ...,\n",
      "        [ 1.8367e-03, -1.4928e-03, -4.2063e-03,  ...,  4.0127e-03,\n",
      "          5.1069e-03,  2.6316e-03],\n",
      "        [-4.5386e-03, -1.3300e-03,  1.8540e-03,  ...,  5.8770e-03,\n",
      "          1.5449e-03,  6.5215e-03],\n",
      "        [ 4.1346e-03, -8.2532e-03, -3.5481e-03,  ...,  4.4830e-03,\n",
      "          3.2338e-03,  2.9775e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight \n",
      "value: tensor([[-5.7095e-03,  6.9256e-03,  3.9512e-03,  ..., -1.5124e-03,\n",
      "          2.1249e-03,  5.4298e-03],\n",
      "        [-5.1228e-05, -7.8130e-03, -3.5087e-03,  ..., -2.4910e-03,\n",
      "         -4.8922e-03, -8.3055e-04],\n",
      "        [-1.7213e-03, -5.2084e-03, -3.0483e-03,  ..., -1.2249e-03,\n",
      "          1.4153e-03, -3.7898e-03],\n",
      "        ...,\n",
      "        [-6.8127e-03, -1.6661e-03,  5.4162e-04,  ...,  7.4835e-04,\n",
      "         -5.1776e-03,  6.6156e-03],\n",
      "        [ 9.8717e-03,  6.2738e-03, -3.1377e-03,  ..., -2.8123e-03,\n",
      "         -2.3442e-03,  8.2978e-03],\n",
      "        [ 4.9400e-03,  5.8429e-04,  3.9785e-03,  ..., -5.0130e-03,\n",
      "          9.5419e-03,  2.6320e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight \n",
      "value: tensor([[-3.8951e-03,  1.9812e-03, -6.6149e-04,  ...,  8.3020e-04,\n",
      "          1.4112e-02, -2.0884e-03],\n",
      "        [-5.8379e-03, -8.9394e-04, -2.1316e-03,  ..., -3.6614e-03,\n",
      "         -2.4909e-03, -4.3346e-04],\n",
      "        [-3.4127e-03,  4.1679e-04,  1.1463e-03,  ..., -3.4756e-03,\n",
      "          9.6016e-03, -4.1235e-03],\n",
      "        ...,\n",
      "        [ 6.1124e-05,  6.2410e-04,  1.9523e-03,  ...,  4.9439e-04,\n",
      "          4.5770e-03,  5.4631e-04],\n",
      "        [-2.2576e-03, -1.7433e-04,  1.4069e-05,  ..., -2.6381e-05,\n",
      "          7.3341e-03, -1.7844e-05],\n",
      "        [-1.9051e-03, -9.4476e-04, -1.2819e-03,  ...,  7.8864e-04,\n",
      "         -1.5534e-03,  2.0843e-04]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight \n",
      "value: tensor([[ 0.0041, -0.0062, -0.0005,  ..., -0.0003,  0.0007, -0.0060],\n",
      "        [-0.0017,  0.0004, -0.0010,  ..., -0.0024, -0.0065, -0.0003],\n",
      "        [-0.0022,  0.0013,  0.0019,  ..., -0.0019,  0.0006,  0.0054],\n",
      "        ...,\n",
      "        [-0.0035, -0.0028, -0.0002,  ...,  0.0068,  0.0038, -0.0015],\n",
      "        [-0.0005,  0.0052, -0.0039,  ...,  0.0008,  0.0050,  0.0004],\n",
      "        [-0.0017,  0.0021, -0.0043,  ..., -0.0026,  0.0054,  0.0016]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight \n",
      "value: tensor([[ 3.6488e-04,  1.6495e-04,  3.8142e-03,  ..., -3.0276e-03,\n",
      "          5.0063e-03,  4.3674e-03],\n",
      "        [-7.9133e-04,  4.2631e-03,  1.7498e-03,  ..., -2.2300e-03,\n",
      "         -5.1919e-05, -1.6176e-03],\n",
      "        [-1.9980e-03,  7.2928e-03, -1.1664e-03,  ...,  3.2440e-03,\n",
      "          5.8883e-04, -2.2276e-03],\n",
      "        ...,\n",
      "        [ 1.2151e-03, -8.4818e-04, -2.4992e-05,  ...,  2.9154e-03,\n",
      "          4.7276e-03,  1.8741e-03],\n",
      "        [ 2.5483e-03,  1.6617e-03, -1.9590e-03,  ..., -2.0253e-03,\n",
      "         -2.1329e-04,  1.2711e-04],\n",
      "        [ 2.1770e-03,  9.2128e-04, -3.9039e-03,  ...,  7.6781e-04,\n",
      "          4.0384e-03, -4.3836e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.down.weight \n",
      "value: tensor([[-0.0103,  0.0041, -0.0180,  ...,  0.0082, -0.0130, -0.0052],\n",
      "        [ 0.0039,  0.0016, -0.0005,  ...,  0.0076,  0.0016,  0.0021],\n",
      "        [-0.0057, -0.0032, -0.0023,  ...,  0.0037,  0.0033, -0.0008],\n",
      "        ...,\n",
      "        [-0.0014,  0.0036,  0.0082,  ...,  0.0034, -0.0068,  0.0039],\n",
      "        [ 0.0034, -0.0013,  0.0059,  ..., -0.0005,  0.0009, -0.0005],\n",
      "        [-0.0105,  0.0025,  0.0038,  ...,  0.0066, -0.0022,  0.0009]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.up.weight \n",
      "value: tensor([[ 6.1275e-03,  5.9229e-03,  4.3427e-03,  ..., -3.8681e-03,\n",
      "          6.4429e-03,  3.5410e-03],\n",
      "        [-9.7203e-03,  2.3281e-03,  9.2267e-03,  ..., -3.1186e-03,\n",
      "         -9.2287e-03, -4.1475e-03],\n",
      "        [ 1.9363e-03, -2.3163e-03, -3.7716e-03,  ...,  3.2883e-03,\n",
      "         -9.2036e-03,  8.8139e-05],\n",
      "        ...,\n",
      "        [ 2.1025e-03,  7.9710e-04, -4.5928e-03,  ...,  3.3409e-03,\n",
      "          2.2871e-03,  3.3960e-03],\n",
      "        [-1.6520e-03, -5.8969e-04,  2.0853e-03,  ...,  3.3946e-03,\n",
      "         -1.3715e-03,  9.2843e-04],\n",
      "        [-3.1170e-03,  3.2259e-03,  3.6250e-03,  ..., -4.1552e-03,\n",
      "          2.0688e-03,  1.7544e-03]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.lora.down.weight \n",
      "value: tensor([[-0.0040,  0.0031,  0.0003,  ...,  0.0061, -0.0030, -0.0021],\n",
      "        [-0.0068,  0.0013, -0.0111,  ...,  0.0035,  0.0016, -0.0043],\n",
      "        [-0.0077, -0.0043, -0.0065,  ...,  0.0030, -0.0022, -0.0097],\n",
      "        ...,\n",
      "        [-0.0041,  0.0030, -0.0049,  ..., -0.0094, -0.0062, -0.0053],\n",
      "        [-0.0031,  0.0053, -0.0021,  ..., -0.0035, -0.0092,  0.0080],\n",
      "        [ 0.0048, -0.0060, -0.0081,  ..., -0.0021, -0.0101,  0.0043]])\n",
      "\n",
      "key: unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.lora.up.weight \n",
      "value: tensor([[-3.1945e-03, -6.6349e-03,  1.4938e-03,  ...,  9.6298e-05,\n",
      "         -3.5294e-03, -4.4837e-03],\n",
      "        [ 1.2052e-02, -5.1925e-03, -4.3547e-04,  ..., -4.4873e-03,\n",
      "         -8.0122e-03,  3.2901e-03],\n",
      "        [ 6.0072e-04, -5.0264e-03, -2.4041e-03,  ..., -1.4643e-03,\n",
      "         -7.4649e-03, -2.6245e-03],\n",
      "        ...,\n",
      "        [ 1.2617e-02, -2.9224e-03,  3.6027e-03,  ...,  4.0932e-04,\n",
      "          1.5494e-03, -3.9216e-03],\n",
      "        [-1.7848e-03, -7.1769e-04, -4.1688e-04,  ..., -1.5166e-03,\n",
      "          6.6868e-03,  3.9039e-04],\n",
      "        [-2.8879e-03, -6.1370e-03,  1.2060e-03,  ..., -1.6431e-03,\n",
      "          8.7489e-03, -8.6338e-04]])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T03:14:53.312879Z",
     "start_time": "2024-10-19T03:14:53.298391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 合并水印的lora模型的结构\n",
    "print(\"\\nSummary of LoRA weights:\")\n",
    "lora_num_layers = {}\n",
    "lora_layer_shapes = {}\n",
    "for key, value in state_dict.items():\n",
    "    layer_name = key.split('.')[0]\n",
    "    if layer_name not in lora_num_layers:\n",
    "        lora_num_layers[layer_name] = 0\n",
    "        lora_layer_shapes[layer_name] = []\n",
    "    lora_num_layers[layer_name] += 1\n",
    "    lora_layer_shapes[layer_name].append(value.shape)\n",
    "\n",
    "for layer, count in lora_num_layers.items():\n",
    "    print(f\"Layer: {layer}, Number of parameters: {count}, Shapes: {lora_layer_shapes[layer]}\")\n"
   ],
   "id": "672a1b4ef43343d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of LoRA weights:\n",
      "Layer: unet, Number of parameters: 384, Shapes: [torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([2560, 320]), torch.Size([320, 1280]), torch.Size([320, 320]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([2560, 320]), torch.Size([320, 1280]), torch.Size([320, 320]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([5120, 320]), torch.Size([320, 2560]), torch.Size([640, 320]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([5120, 320]), torch.Size([320, 2560]), torch.Size([640, 320]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([5120, 320]), torch.Size([320, 2560]), torch.Size([640, 320]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([5120, 320]), torch.Size([320, 2560]), torch.Size([640, 320]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([5120, 320]), torch.Size([320, 2560]), torch.Size([640, 320]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([2560, 320]), torch.Size([320, 1280]), torch.Size([320, 320]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([2560, 320]), torch.Size([320, 1280]), torch.Size([320, 320]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([2560, 320]), torch.Size([320, 1280]), torch.Size([320, 320])]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T03:14:54.291788Z",
     "start_time": "2024-10-19T03:14:54.281334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 没有合并水印的lora模型的结构\n",
    "print(\"\\nSummary of no-watermarked LoRA weights:\")\n",
    "no_watermarked_lora_num_layers = {}\n",
    "no_watermarked_lora_layer_shapes = {}\n",
    "for key, value in no_watermarked_state_dict.items():\n",
    "    layer_name = key.split('.')[0]\n",
    "    if layer_name not in no_watermarked_lora_num_layers:\n",
    "        no_watermarked_lora_num_layers[layer_name] = 0\n",
    "        no_watermarked_lora_layer_shapes[layer_name] = []\n",
    "    no_watermarked_lora_num_layers[layer_name] += 1\n",
    "    no_watermarked_lora_layer_shapes[layer_name].append(value.shape)\n",
    "\n",
    "for layer, count in no_watermarked_lora_num_layers.items():\n",
    "    print(f\"Layer: {layer}, Number of parameters: {count}, Shapes: {no_watermarked_lora_layer_shapes[layer]}\")"
   ],
   "id": "e319423c236f9b30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of no-watermarked LoRA weights:\n",
      "Layer: unet, Number of parameters: 384, Shapes: [torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([2560, 320]), torch.Size([320, 1280]), torch.Size([320, 320]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([2560, 320]), torch.Size([320, 1280]), torch.Size([320, 320]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([5120, 320]), torch.Size([320, 2560]), torch.Size([640, 320]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([5120, 320]), torch.Size([320, 2560]), torch.Size([640, 320]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([5120, 320]), torch.Size([320, 2560]), torch.Size([640, 320]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([5120, 320]), torch.Size([320, 2560]), torch.Size([640, 320]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([5120, 320]), torch.Size([320, 2560]), torch.Size([640, 320]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([2560, 320]), torch.Size([320, 1280]), torch.Size([320, 320]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([2560, 320]), torch.Size([320, 1280]), torch.Size([320, 320]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([2560, 320]), torch.Size([320, 1280]), torch.Size([320, 320])]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T03:14:55.906967Z",
     "start_time": "2024-10-19T03:14:55.898374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 比较没有合并水印的lora模型的结构和合并了的的每一个layer参数格式是否一致\n",
    "if no_watermarked_lora_layer_shapes['unet'] == lora_layer_shapes['unet']:\n",
    "    print(\"The layer shapes are the same.\")\n"
   ],
   "id": "76bef55412fff89b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer shapes are the same.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T03:14:57.283973Z",
     "start_time": "2024-10-19T03:14:57.267864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nSummary of LoRA weights:\")\n",
    "num_blocks = {}\n",
    "block_shapes = {}\n",
    "\n",
    "# 遍历 state_dict 中的所有键\n",
    "for key, value in state_dict.items():\n",
    "    # 分割键来提取具体 block 的信息，通常结构是 'layer.block.subblock.weight'\n",
    "    # 我们将前两个部分(layer 和 block)组合起来作为 block 的名称\n",
    "    block_name = '.'.join(key.split('.')[:3])  # 提取 'layer.block.subblock' 名称\n",
    "    if block_name not in num_blocks:\n",
    "        num_blocks[block_name] = 0\n",
    "        block_shapes[block_name] = []\n",
    "\n",
    "    # 记录每个 block 的参数数量和 shape\n",
    "    num_blocks[block_name] += 1\n",
    "    block_shapes[block_name].append(value.shape)\n",
    "\n",
    "# 输出每个 block 的信息\n",
    "for block, count in num_blocks.items():\n",
    "    print(f\"Block: {block}, Number of parameters: {count}, Shapes: {block_shapes[block]}\")\n"
   ],
   "id": "1825b4dfc49c485a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of LoRA weights:\n",
      "Block: unet.down_blocks.0, Number of parameters: 48, Shapes: [torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([2560, 320]), torch.Size([320, 1280]), torch.Size([320, 320]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([2560, 320]), torch.Size([320, 1280]), torch.Size([320, 320])]\n",
      "Block: unet.down_blocks.1, Number of parameters: 48, Shapes: [torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([5120, 320]), torch.Size([320, 2560]), torch.Size([640, 320]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([5120, 320]), torch.Size([320, 2560]), torch.Size([640, 320])]\n",
      "Block: unet.down_blocks.2, Number of parameters: 48, Shapes: [torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320])]\n",
      "Block: unet.mid_block.attentions, Number of parameters: 24, Shapes: [torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320])]\n",
      "Block: unet.up_blocks.1, Number of parameters: 72, Shapes: [torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280, 1, 1]), torch.Size([1280, 320, 1, 1]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([1280, 320]), torch.Size([320, 768]), torch.Size([1280, 320]), torch.Size([320, 1280]), torch.Size([10240, 320]), torch.Size([320, 5120]), torch.Size([1280, 320])]\n",
      "Block: unet.up_blocks.2, Number of parameters: 72, Shapes: [torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([5120, 320]), torch.Size([320, 2560]), torch.Size([640, 320]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([5120, 320]), torch.Size([320, 2560]), torch.Size([640, 320]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640, 1, 1]), torch.Size([640, 320, 1, 1]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([640, 320]), torch.Size([320, 768]), torch.Size([640, 320]), torch.Size([320, 640]), torch.Size([5120, 320]), torch.Size([320, 2560]), torch.Size([640, 320])]\n",
      "Block: unet.up_blocks.3, Number of parameters: 72, Shapes: [torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([2560, 320]), torch.Size([320, 1280]), torch.Size([320, 320]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([2560, 320]), torch.Size([320, 1280]), torch.Size([320, 320]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([320, 768]), torch.Size([320, 320]), torch.Size([320, 320]), torch.Size([2560, 320]), torch.Size([320, 1280]), torch.Size([320, 320])]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T03:14:58.828755Z",
     "start_time": "2024-10-19T03:14:58.802894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nLoRA 权重的结构信息：\")\n",
    "\n",
    "num_blocks = {}  # 用于存储每个 block 的信息\n",
    "layer_type_count = {}  # 用于存储每种层的数量\n",
    "shape_analysis = {}  # 用于存储每种层的 shape 信息\n",
    "# 遍历 state_dict 中的所有键\n",
    "for key, value in state_dict.items():\n",
    "    # 将键按照 '.' 分割\n",
    "    key_parts = key.split('.')\n",
    "\n",
    "    # 定义 block 的层级，例如到 'unet.down_blocks.0.attentions.0'\n",
    "    block_level = 5  # 根据模型的具体结构，选择哪几个.分割的部分作为 block 的名称\n",
    "    block_name = '.'.join(key_parts[:block_level])  # 提取 block 的名称\n",
    "\n",
    "    # 定义 layer_name 为剩余部分\n",
    "    layer_name = '.'.join(key_parts[block_level:])\n",
    "\n",
    "    # 初始化 block 的信息\n",
    "    if block_name not in num_blocks:\n",
    "        num_blocks[block_name] = {}\n",
    "\n",
    "    # 记录每个 block 下的层信息\n",
    "    if layer_name not in num_blocks[block_name]:\n",
    "        num_blocks[block_name][layer_name] = []\n",
    "    num_blocks[block_name][layer_name].append(value.shape)\n",
    "\n",
    "    # 统计每种 layer 类型出现的次数\n",
    "    layer_type = '.'.join(key_parts[block_level:-1])  # 去掉最后的 weight\n",
    "\n",
    "    if layer_type not in layer_type_count:\n",
    "        layer_type_count[layer_type] = 0\n",
    "    layer_type_count[layer_type] += 1\n",
    "\n",
    "    # 记录每种层的 shape 信息\n",
    "    if layer_type not in shape_analysis:\n",
    "        shape_analysis[layer_type] = []\n",
    "    shape_analysis[layer_type].append(value.shape)\n",
    "\n",
    "# 输出每个 block 和其下的层信息\n",
    "for block, layers in num_blocks.items():\n",
    "    print(f\"\\nBlock: {block}\")\n",
    "    for layer, shapes in layers.items():\n",
    "        print(f\"  Layer: {layer}, Shapes: {shapes}\")\n"
   ],
   "id": "c220bd5cb0656abd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LoRA 权重的结构信息：\n",
      "\n",
      "Block: unet.down_blocks.0.attentions.0\n",
      "  Layer: proj_in.lora.down.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: proj_in.lora.up.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.down.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.up.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shapes: [torch.Size([2560, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "\n",
      "Block: unet.down_blocks.0.attentions.1\n",
      "  Layer: proj_in.lora.down.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: proj_in.lora.up.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.down.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.up.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shapes: [torch.Size([2560, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "\n",
      "Block: unet.down_blocks.1.attentions.0\n",
      "  Layer: proj_in.lora.down.weight, Shapes: [torch.Size([320, 640, 1, 1])]\n",
      "  Layer: proj_in.lora.up.weight, Shapes: [torch.Size([640, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.down.weight, Shapes: [torch.Size([320, 640, 1, 1])]\n",
      "  Layer: proj_out.lora.up.weight, Shapes: [torch.Size([640, 320, 1, 1])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shapes: [torch.Size([5120, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.down.weight, Shapes: [torch.Size([320, 2560])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "\n",
      "Block: unet.down_blocks.1.attentions.1\n",
      "  Layer: proj_in.lora.down.weight, Shapes: [torch.Size([320, 640, 1, 1])]\n",
      "  Layer: proj_in.lora.up.weight, Shapes: [torch.Size([640, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.down.weight, Shapes: [torch.Size([320, 640, 1, 1])]\n",
      "  Layer: proj_out.lora.up.weight, Shapes: [torch.Size([640, 320, 1, 1])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shapes: [torch.Size([5120, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.down.weight, Shapes: [torch.Size([320, 2560])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "\n",
      "Block: unet.down_blocks.2.attentions.0\n",
      "  Layer: proj_in.lora.down.weight, Shapes: [torch.Size([320, 1280, 1, 1])]\n",
      "  Layer: proj_in.lora.up.weight, Shapes: [torch.Size([1280, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.down.weight, Shapes: [torch.Size([320, 1280, 1, 1])]\n",
      "  Layer: proj_out.lora.up.weight, Shapes: [torch.Size([1280, 320, 1, 1])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shapes: [torch.Size([10240, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.down.weight, Shapes: [torch.Size([320, 5120])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "\n",
      "Block: unet.down_blocks.2.attentions.1\n",
      "  Layer: proj_in.lora.down.weight, Shapes: [torch.Size([320, 1280, 1, 1])]\n",
      "  Layer: proj_in.lora.up.weight, Shapes: [torch.Size([1280, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.down.weight, Shapes: [torch.Size([320, 1280, 1, 1])]\n",
      "  Layer: proj_out.lora.up.weight, Shapes: [torch.Size([1280, 320, 1, 1])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shapes: [torch.Size([10240, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.down.weight, Shapes: [torch.Size([320, 5120])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "\n",
      "Block: unet.mid_block.attentions.0.proj_in\n",
      "  Layer: lora.down.weight, Shapes: [torch.Size([320, 1280, 1, 1])]\n",
      "  Layer: lora.up.weight, Shapes: [torch.Size([1280, 320, 1, 1])]\n",
      "\n",
      "Block: unet.mid_block.attentions.0.proj_out\n",
      "  Layer: lora.down.weight, Shapes: [torch.Size([320, 1280, 1, 1])]\n",
      "  Layer: lora.up.weight, Shapes: [torch.Size([1280, 320, 1, 1])]\n",
      "\n",
      "Block: unet.mid_block.attentions.0.transformer_blocks\n",
      "  Layer: 0.attn1.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: 0.attn1.processor.to_k_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: 0.attn1.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: 0.attn1.processor.to_out_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: 0.attn1.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: 0.attn1.processor.to_q_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: 0.attn1.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: 0.attn1.processor.to_v_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: 0.attn2.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: 0.attn2.processor.to_k_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: 0.attn2.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: 0.attn2.processor.to_out_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: 0.attn2.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: 0.attn2.processor.to_q_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: 0.attn2.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: 0.attn2.processor.to_v_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: 0.ff.net.0.proj.lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: 0.ff.net.0.proj.lora.up.weight, Shapes: [torch.Size([10240, 320])]\n",
      "  Layer: 0.ff.net.2.lora.down.weight, Shapes: [torch.Size([320, 5120])]\n",
      "  Layer: 0.ff.net.2.lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "\n",
      "Block: unet.up_blocks.1.attentions.0\n",
      "  Layer: proj_in.lora.down.weight, Shapes: [torch.Size([320, 1280, 1, 1])]\n",
      "  Layer: proj_in.lora.up.weight, Shapes: [torch.Size([1280, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.down.weight, Shapes: [torch.Size([320, 1280, 1, 1])]\n",
      "  Layer: proj_out.lora.up.weight, Shapes: [torch.Size([1280, 320, 1, 1])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shapes: [torch.Size([10240, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.down.weight, Shapes: [torch.Size([320, 5120])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "\n",
      "Block: unet.up_blocks.1.attentions.1\n",
      "  Layer: proj_in.lora.down.weight, Shapes: [torch.Size([320, 1280, 1, 1])]\n",
      "  Layer: proj_in.lora.up.weight, Shapes: [torch.Size([1280, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.down.weight, Shapes: [torch.Size([320, 1280, 1, 1])]\n",
      "  Layer: proj_out.lora.up.weight, Shapes: [torch.Size([1280, 320, 1, 1])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shapes: [torch.Size([10240, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.down.weight, Shapes: [torch.Size([320, 5120])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "\n",
      "Block: unet.up_blocks.1.attentions.2\n",
      "  Layer: proj_in.lora.down.weight, Shapes: [torch.Size([320, 1280, 1, 1])]\n",
      "  Layer: proj_in.lora.up.weight, Shapes: [torch.Size([1280, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.down.weight, Shapes: [torch.Size([320, 1280, 1, 1])]\n",
      "  Layer: proj_out.lora.up.weight, Shapes: [torch.Size([1280, 320, 1, 1])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shapes: [torch.Size([10240, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.down.weight, Shapes: [torch.Size([320, 5120])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.up.weight, Shapes: [torch.Size([1280, 320])]\n",
      "\n",
      "Block: unet.up_blocks.2.attentions.0\n",
      "  Layer: proj_in.lora.down.weight, Shapes: [torch.Size([320, 640, 1, 1])]\n",
      "  Layer: proj_in.lora.up.weight, Shapes: [torch.Size([640, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.down.weight, Shapes: [torch.Size([320, 640, 1, 1])]\n",
      "  Layer: proj_out.lora.up.weight, Shapes: [torch.Size([640, 320, 1, 1])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shapes: [torch.Size([5120, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.down.weight, Shapes: [torch.Size([320, 2560])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "\n",
      "Block: unet.up_blocks.2.attentions.1\n",
      "  Layer: proj_in.lora.down.weight, Shapes: [torch.Size([320, 640, 1, 1])]\n",
      "  Layer: proj_in.lora.up.weight, Shapes: [torch.Size([640, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.down.weight, Shapes: [torch.Size([320, 640, 1, 1])]\n",
      "  Layer: proj_out.lora.up.weight, Shapes: [torch.Size([640, 320, 1, 1])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shapes: [torch.Size([5120, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.down.weight, Shapes: [torch.Size([320, 2560])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "\n",
      "Block: unet.up_blocks.2.attentions.2\n",
      "  Layer: proj_in.lora.down.weight, Shapes: [torch.Size([320, 640, 1, 1])]\n",
      "  Layer: proj_in.lora.up.weight, Shapes: [torch.Size([640, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.down.weight, Shapes: [torch.Size([320, 640, 1, 1])]\n",
      "  Layer: proj_out.lora.up.weight, Shapes: [torch.Size([640, 320, 1, 1])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shapes: [torch.Size([320, 640])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shapes: [torch.Size([5120, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.down.weight, Shapes: [torch.Size([320, 2560])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.up.weight, Shapes: [torch.Size([640, 320])]\n",
      "\n",
      "Block: unet.up_blocks.3.attentions.0\n",
      "  Layer: proj_in.lora.down.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: proj_in.lora.up.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.down.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.up.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shapes: [torch.Size([2560, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "\n",
      "Block: unet.up_blocks.3.attentions.1\n",
      "  Layer: proj_in.lora.down.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: proj_in.lora.up.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.down.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.up.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shapes: [torch.Size([2560, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "\n",
      "Block: unet.up_blocks.3.attentions.2\n",
      "  Layer: proj_in.lora.down.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: proj_in.lora.up.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.down.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: proj_out.lora.up.weight, Shapes: [torch.Size([320, 320, 1, 1])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shapes: [torch.Size([320, 768])]\n",
      "  Layer: transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shapes: [torch.Size([320, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shapes: [torch.Size([2560, 320])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.down.weight, Shapes: [torch.Size([320, 1280])]\n",
      "  Layer: transformer_blocks.0.ff.net.2.lora.up.weight, Shapes: [torch.Size([320, 320])]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T03:15:00.464948Z",
     "start_time": "2024-10-19T03:15:00.455317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 输出层类型统计信息\n",
    "print(\"\\nLoRA 层类型统计信息：\")\n",
    "for layer_type, count in layer_type_count.items():\n",
    "    print(f\"  Layer type: {layer_type}, Count: {count}\")"
   ],
   "id": "a40fd40f5ae849d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LoRA 层类型统计信息：\n",
      "  Layer type: proj_in.lora.down, Count: 15\n",
      "  Layer type: proj_in.lora.up, Count: 15\n",
      "  Layer type: proj_out.lora.down, Count: 15\n",
      "  Layer type: proj_out.lora.up, Count: 15\n",
      "  Layer type: transformer_blocks.0.attn1.processor.to_k_lora.down, Count: 15\n",
      "  Layer type: transformer_blocks.0.attn1.processor.to_k_lora.up, Count: 15\n",
      "  Layer type: transformer_blocks.0.attn1.processor.to_out_lora.down, Count: 15\n",
      "  Layer type: transformer_blocks.0.attn1.processor.to_out_lora.up, Count: 15\n",
      "  Layer type: transformer_blocks.0.attn1.processor.to_q_lora.down, Count: 15\n",
      "  Layer type: transformer_blocks.0.attn1.processor.to_q_lora.up, Count: 15\n",
      "  Layer type: transformer_blocks.0.attn1.processor.to_v_lora.down, Count: 15\n",
      "  Layer type: transformer_blocks.0.attn1.processor.to_v_lora.up, Count: 15\n",
      "  Layer type: transformer_blocks.0.attn2.processor.to_k_lora.down, Count: 15\n",
      "  Layer type: transformer_blocks.0.attn2.processor.to_k_lora.up, Count: 15\n",
      "  Layer type: transformer_blocks.0.attn2.processor.to_out_lora.down, Count: 15\n",
      "  Layer type: transformer_blocks.0.attn2.processor.to_out_lora.up, Count: 15\n",
      "  Layer type: transformer_blocks.0.attn2.processor.to_q_lora.down, Count: 15\n",
      "  Layer type: transformer_blocks.0.attn2.processor.to_q_lora.up, Count: 15\n",
      "  Layer type: transformer_blocks.0.attn2.processor.to_v_lora.down, Count: 15\n",
      "  Layer type: transformer_blocks.0.attn2.processor.to_v_lora.up, Count: 15\n",
      "  Layer type: transformer_blocks.0.ff.net.0.proj.lora.down, Count: 15\n",
      "  Layer type: transformer_blocks.0.ff.net.0.proj.lora.up, Count: 15\n",
      "  Layer type: transformer_blocks.0.ff.net.2.lora.down, Count: 15\n",
      "  Layer type: transformer_blocks.0.ff.net.2.lora.up, Count: 15\n",
      "  Layer type: lora.down, Count: 2\n",
      "  Layer type: lora.up, Count: 2\n",
      "  Layer type: 0.attn1.processor.to_k_lora.down, Count: 1\n",
      "  Layer type: 0.attn1.processor.to_k_lora.up, Count: 1\n",
      "  Layer type: 0.attn1.processor.to_out_lora.down, Count: 1\n",
      "  Layer type: 0.attn1.processor.to_out_lora.up, Count: 1\n",
      "  Layer type: 0.attn1.processor.to_q_lora.down, Count: 1\n",
      "  Layer type: 0.attn1.processor.to_q_lora.up, Count: 1\n",
      "  Layer type: 0.attn1.processor.to_v_lora.down, Count: 1\n",
      "  Layer type: 0.attn1.processor.to_v_lora.up, Count: 1\n",
      "  Layer type: 0.attn2.processor.to_k_lora.down, Count: 1\n",
      "  Layer type: 0.attn2.processor.to_k_lora.up, Count: 1\n",
      "  Layer type: 0.attn2.processor.to_out_lora.down, Count: 1\n",
      "  Layer type: 0.attn2.processor.to_out_lora.up, Count: 1\n",
      "  Layer type: 0.attn2.processor.to_q_lora.down, Count: 1\n",
      "  Layer type: 0.attn2.processor.to_q_lora.up, Count: 1\n",
      "  Layer type: 0.attn2.processor.to_v_lora.down, Count: 1\n",
      "  Layer type: 0.attn2.processor.to_v_lora.up, Count: 1\n",
      "  Layer type: 0.ff.net.0.proj.lora.down, Count: 1\n",
      "  Layer type: 0.ff.net.0.proj.lora.up, Count: 1\n",
      "  Layer type: 0.ff.net.2.lora.down, Count: 1\n",
      "  Layer type: 0.ff.net.2.lora.up, Count: 1\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T03:15:01.909503Z",
     "start_time": "2024-10-19T03:15:01.899387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 输出层形状分析信息\n",
    "print(\"\\nLoRA 层的形状分析：\")\n",
    "for layer_type, shapes in shape_analysis.items():\n",
    "    unique_shapes = set(shapes)  # 统计唯一的 shape\n",
    "    print(f\"  Layer type: {layer_type}, Unique Shapes: {unique_shapes}\")"
   ],
   "id": "99db63a12b15e55b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LoRA 层的形状分析：\n",
      "  Layer type: proj_in.lora.down, Unique Shapes: {torch.Size([320, 1280, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 640, 1, 1])}\n",
      "  Layer type: proj_in.lora.up, Unique Shapes: {torch.Size([640, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([1280, 320, 1, 1])}\n",
      "  Layer type: proj_out.lora.down, Unique Shapes: {torch.Size([320, 1280, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([320, 640, 1, 1])}\n",
      "  Layer type: proj_out.lora.up, Unique Shapes: {torch.Size([640, 320, 1, 1]), torch.Size([320, 320, 1, 1]), torch.Size([1280, 320, 1, 1])}\n",
      "  Layer type: transformer_blocks.0.attn1.processor.to_k_lora.down, Unique Shapes: {torch.Size([320, 1280]), torch.Size([320, 640]), torch.Size([320, 320])}\n",
      "  Layer type: transformer_blocks.0.attn1.processor.to_k_lora.up, Unique Shapes: {torch.Size([1280, 320]), torch.Size([640, 320]), torch.Size([320, 320])}\n",
      "  Layer type: transformer_blocks.0.attn1.processor.to_out_lora.down, Unique Shapes: {torch.Size([320, 1280]), torch.Size([320, 640]), torch.Size([320, 320])}\n",
      "  Layer type: transformer_blocks.0.attn1.processor.to_out_lora.up, Unique Shapes: {torch.Size([1280, 320]), torch.Size([640, 320]), torch.Size([320, 320])}\n",
      "  Layer type: transformer_blocks.0.attn1.processor.to_q_lora.down, Unique Shapes: {torch.Size([320, 1280]), torch.Size([320, 640]), torch.Size([320, 320])}\n",
      "  Layer type: transformer_blocks.0.attn1.processor.to_q_lora.up, Unique Shapes: {torch.Size([1280, 320]), torch.Size([640, 320]), torch.Size([320, 320])}\n",
      "  Layer type: transformer_blocks.0.attn1.processor.to_v_lora.down, Unique Shapes: {torch.Size([320, 1280]), torch.Size([320, 640]), torch.Size([320, 320])}\n",
      "  Layer type: transformer_blocks.0.attn1.processor.to_v_lora.up, Unique Shapes: {torch.Size([1280, 320]), torch.Size([640, 320]), torch.Size([320, 320])}\n",
      "  Layer type: transformer_blocks.0.attn2.processor.to_k_lora.down, Unique Shapes: {torch.Size([320, 768])}\n",
      "  Layer type: transformer_blocks.0.attn2.processor.to_k_lora.up, Unique Shapes: {torch.Size([1280, 320]), torch.Size([640, 320]), torch.Size([320, 320])}\n",
      "  Layer type: transformer_blocks.0.attn2.processor.to_out_lora.down, Unique Shapes: {torch.Size([320, 1280]), torch.Size([320, 640]), torch.Size([320, 320])}\n",
      "  Layer type: transformer_blocks.0.attn2.processor.to_out_lora.up, Unique Shapes: {torch.Size([1280, 320]), torch.Size([640, 320]), torch.Size([320, 320])}\n",
      "  Layer type: transformer_blocks.0.attn2.processor.to_q_lora.down, Unique Shapes: {torch.Size([320, 1280]), torch.Size([320, 640]), torch.Size([320, 320])}\n",
      "  Layer type: transformer_blocks.0.attn2.processor.to_q_lora.up, Unique Shapes: {torch.Size([1280, 320]), torch.Size([640, 320]), torch.Size([320, 320])}\n",
      "  Layer type: transformer_blocks.0.attn2.processor.to_v_lora.down, Unique Shapes: {torch.Size([320, 768])}\n",
      "  Layer type: transformer_blocks.0.attn2.processor.to_v_lora.up, Unique Shapes: {torch.Size([1280, 320]), torch.Size([640, 320]), torch.Size([320, 320])}\n",
      "  Layer type: transformer_blocks.0.ff.net.0.proj.lora.down, Unique Shapes: {torch.Size([320, 1280]), torch.Size([320, 640]), torch.Size([320, 320])}\n",
      "  Layer type: transformer_blocks.0.ff.net.0.proj.lora.up, Unique Shapes: {torch.Size([5120, 320]), torch.Size([10240, 320]), torch.Size([2560, 320])}\n",
      "  Layer type: transformer_blocks.0.ff.net.2.lora.down, Unique Shapes: {torch.Size([320, 1280]), torch.Size([320, 2560]), torch.Size([320, 5120])}\n",
      "  Layer type: transformer_blocks.0.ff.net.2.lora.up, Unique Shapes: {torch.Size([1280, 320]), torch.Size([640, 320]), torch.Size([320, 320])}\n",
      "  Layer type: lora.down, Unique Shapes: {torch.Size([320, 1280, 1, 1])}\n",
      "  Layer type: lora.up, Unique Shapes: {torch.Size([1280, 320, 1, 1])}\n",
      "  Layer type: 0.attn1.processor.to_k_lora.down, Unique Shapes: {torch.Size([320, 1280])}\n",
      "  Layer type: 0.attn1.processor.to_k_lora.up, Unique Shapes: {torch.Size([1280, 320])}\n",
      "  Layer type: 0.attn1.processor.to_out_lora.down, Unique Shapes: {torch.Size([320, 1280])}\n",
      "  Layer type: 0.attn1.processor.to_out_lora.up, Unique Shapes: {torch.Size([1280, 320])}\n",
      "  Layer type: 0.attn1.processor.to_q_lora.down, Unique Shapes: {torch.Size([320, 1280])}\n",
      "  Layer type: 0.attn1.processor.to_q_lora.up, Unique Shapes: {torch.Size([1280, 320])}\n",
      "  Layer type: 0.attn1.processor.to_v_lora.down, Unique Shapes: {torch.Size([320, 1280])}\n",
      "  Layer type: 0.attn1.processor.to_v_lora.up, Unique Shapes: {torch.Size([1280, 320])}\n",
      "  Layer type: 0.attn2.processor.to_k_lora.down, Unique Shapes: {torch.Size([320, 768])}\n",
      "  Layer type: 0.attn2.processor.to_k_lora.up, Unique Shapes: {torch.Size([1280, 320])}\n",
      "  Layer type: 0.attn2.processor.to_out_lora.down, Unique Shapes: {torch.Size([320, 1280])}\n",
      "  Layer type: 0.attn2.processor.to_out_lora.up, Unique Shapes: {torch.Size([1280, 320])}\n",
      "  Layer type: 0.attn2.processor.to_q_lora.down, Unique Shapes: {torch.Size([320, 1280])}\n",
      "  Layer type: 0.attn2.processor.to_q_lora.up, Unique Shapes: {torch.Size([1280, 320])}\n",
      "  Layer type: 0.attn2.processor.to_v_lora.down, Unique Shapes: {torch.Size([320, 768])}\n",
      "  Layer type: 0.attn2.processor.to_v_lora.up, Unique Shapes: {torch.Size([1280, 320])}\n",
      "  Layer type: 0.ff.net.0.proj.lora.down, Unique Shapes: {torch.Size([320, 1280])}\n",
      "  Layer type: 0.ff.net.0.proj.lora.up, Unique Shapes: {torch.Size([10240, 320])}\n",
      "  Layer type: 0.ff.net.2.lora.down, Unique Shapes: {torch.Size([320, 5120])}\n",
      "  Layer type: 0.ff.net.2.lora.up, Unique Shapes: {torch.Size([1280, 320])}\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "  Layer type: lora.down, Unique Shapes: {torch.Size([320, 1280, 1, 1])}\n",
    "  Layer type: lora.up, Unique Shapes: {torch.Size([1280, 320, 1, 1])}\n",
    "是什么?\n",
    "是mid_block的proj_in和proj_out的shape"
   ],
   "id": "a4d595d38bc5247b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T14:05:56.551611Z",
     "start_time": "2024-10-22T14:05:56.533365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 训练获取一定数量的lora模型作为我们的训练数据\n",
    "dataset_path = \"./checkpoints/lora_weights_dataset\"\n",
    "\n",
    "# TODO\n"
   ],
   "id": "333f011abc015276",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T14:10:22.277795Z",
     "start_time": "2024-10-22T14:08:02.771923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = \"./checkpoints/lora_weights_dataset/rank320_batchsz8_gpu4\"\n",
    "original_lora_param_info = {}\n",
    "# original_lora_data_lengths = []\n",
    "for file in glob.glob(os.path.join(dataset_path, \"*.safetensors\")):\n",
    "    model = load_file(file)\n",
    "    for key, value in model.items():\n",
    "        param_info = {\n",
    "            'shape': value.shape,\n",
    "            'length': value.numel()\n",
    "        }\n",
    "        original_lora_param_info[key] = param_info"
   ],
   "id": "dccdd98d453d3abc",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T14:12:46.365462Z",
     "start_time": "2024-10-22T14:12:46.354479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "idx = 0\n",
    "for layer_name, param_info in original_lora_param_info.items():\n",
    "    idx += 1\n",
    "    print(f\"Layer: {layer_name}, Shape: {param_info['shape']}, Length: {param_info['length']}\\n\")\n",
    "    if idx == 20:\n",
    "        break\n"
   ],
   "id": "bb205024f0ef7d02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: unet.down_blocks.0.attentions.0.proj_in.lora.down.weight, Shape: torch.Size([320, 320, 1, 1]), Length: 102400\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.proj_in.lora.up.weight, Shape: torch.Size([320, 320, 1, 1]), Length: 102400\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.proj_out.lora.down.weight, Shape: torch.Size([320, 320, 1, 1]), Length: 102400\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.proj_out.lora.up.weight, Shape: torch.Size([320, 320, 1, 1]), Length: 102400\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shape: torch.Size([320, 320]), Length: 102400\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shape: torch.Size([320, 320]), Length: 102400\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shape: torch.Size([320, 320]), Length: 102400\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shape: torch.Size([320, 320]), Length: 102400\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shape: torch.Size([320, 320]), Length: 102400\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shape: torch.Size([320, 320]), Length: 102400\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shape: torch.Size([320, 320]), Length: 102400\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shape: torch.Size([320, 320]), Length: 102400\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shape: torch.Size([320, 768]), Length: 245760\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shape: torch.Size([320, 320]), Length: 102400\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shape: torch.Size([320, 320]), Length: 102400\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shape: torch.Size([320, 320]), Length: 102400\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shape: torch.Size([320, 320]), Length: 102400\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shape: torch.Size([320, 320]), Length: 102400\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shape: torch.Size([320, 768]), Length: 245760\n",
      "\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shape: torch.Size([320, 320]), Length: 102400\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-22T14:39:34.762826Z",
     "start_time": "2024-10-22T14:16:45.523550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 对获取的lora模型权重进行预处理，将其转换为我们需要的格式\n",
    "# TODO: 之后可能需要改成batch的形式\n",
    "total_lora_data = []\n",
    "for file in glob.glob(os.path.join(dataset_path, \"*.safetensors\")):\n",
    "    single_lora_weights = []\n",
    "    model = load_file(file)\n",
    "    for key, value in model.items():\n",
    "        # 临时代码，打印观察 TODO:删除\n",
    "        # print(\"value: \")\n",
    "        # print(value)\n",
    "        # 对每个value进行Z-score标准化的策略\n",
    "        mean = value.mean()\n",
    "        std = value.std()\n",
    "        value = (value - mean) / std\n",
    "\n",
    "        # 将value展平为一维\n",
    "        flattened_value = value.flatten()\n",
    "        single_lora_weights.append(flattened_value)\n",
    "        # print(\"flattened_value：\")\n",
    "        # print(flattened_value)\n",
    "    single_lora_weights = torch.cat(single_lora_weights, dim=0)\n",
    "    # total_lora_data.append(single_lora_weights)\n",
    "    torch.save(single_lora_weights,\n",
    "               os.path.join(dataset_path, \"normalized_{}.pth\".format(os.path.basename(file).split(\".\")[0])))\n",
    "    # torch.save(total_lora_data, \"./checkpoints/lora_data.pth\")"
   ],
   "id": "2b3d1bc4d48c874f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T03:15:27.789277Z",
     "start_time": "2024-10-19T03:15:27.772003Z"
    }
   },
   "cell_type": "code",
   "source": "total_lora_data[0].shape",
   "id": "7eab056ef78f5b43",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([135659520])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T03:15:31.648850Z",
     "start_time": "2024-10-19T03:15:31.633128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 试着重建展平的lora权重回原来的形状\n",
    "# 重建模型的字典\n",
    "start = 0\n",
    "reconstructed_lora_weights = {}\n",
    "for layer_name, param_info in original_lora_param_info.items():\n",
    "    # 在lora权重中，key为layer_name代表某一层的名字，param_info['length']代表这一层的参数个数, param_info['shape']代表这一层参数的shape\n",
    "    # 从total_lora_data中取出这一层的参数\n",
    "    end = start + param_info['length']\n",
    "    layer_weight_vector = total_lora_data[0][start:end]\n",
    "    layer_weight_matrix = layer_weight_vector.view(param_info['shape'])\n",
    "    reconstructed_lora_weights[layer_name] = layer_weight_matrix\n",
    "    start = end\n",
    "\n"
   ],
   "id": "7fb1d08859863239",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T03:15:32.532668Z",
     "start_time": "2024-10-19T03:15:32.199764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 打印观察重建的lora权重\n",
    "for layer_name, layer_weight in reconstructed_lora_weights.items():\n",
    "    print(f\"Layer: {layer_name}, Shape: {layer_weight.shape}\")\n",
    "    print(layer_weight)"
   ],
   "id": "5b3636437c38f4ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: unet.down_blocks.0.attentions.0.proj_in.lora.down.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[ 0.4963]],\n",
      "\n",
      "         [[-0.1690]],\n",
      "\n",
      "         [[ 0.8896]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2209]],\n",
      "\n",
      "         [[ 0.6395]],\n",
      "\n",
      "         [[-0.1154]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1196]],\n",
      "\n",
      "         [[ 0.5286]],\n",
      "\n",
      "         [[-1.4252]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4536]],\n",
      "\n",
      "         [[ 0.3694]],\n",
      "\n",
      "         [[ 0.3576]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3573]],\n",
      "\n",
      "         [[-0.2872]],\n",
      "\n",
      "         [[ 0.9560]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2905]],\n",
      "\n",
      "         [[ 0.2464]],\n",
      "\n",
      "         [[-0.1850]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0818]],\n",
      "\n",
      "         [[ 0.2669]],\n",
      "\n",
      "         [[ 0.3448]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2788]],\n",
      "\n",
      "         [[ 0.6142]],\n",
      "\n",
      "         [[-0.8195]]],\n",
      "\n",
      "\n",
      "        [[[-0.2262]],\n",
      "\n",
      "         [[-0.7471]],\n",
      "\n",
      "         [[ 0.3485]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7929]],\n",
      "\n",
      "         [[ 1.4945]],\n",
      "\n",
      "         [[ 0.6226]]],\n",
      "\n",
      "\n",
      "        [[[-0.7336]],\n",
      "\n",
      "         [[ 0.8656]],\n",
      "\n",
      "         [[-0.5490]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4564]],\n",
      "\n",
      "         [[ 0.3103]],\n",
      "\n",
      "         [[ 0.3889]]]])\n",
      "Layer: unet.down_blocks.0.attentions.0.proj_in.lora.up.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[-0.4912]],\n",
      "\n",
      "         [[ 0.4757]],\n",
      "\n",
      "         [[-2.0527]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8600]],\n",
      "\n",
      "         [[ 0.0632]],\n",
      "\n",
      "         [[-0.4042]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5410]],\n",
      "\n",
      "         [[ 0.6179]],\n",
      "\n",
      "         [[ 0.0728]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0616]],\n",
      "\n",
      "         [[-0.8817]],\n",
      "\n",
      "         [[ 1.5022]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4679]],\n",
      "\n",
      "         [[ 1.4099]],\n",
      "\n",
      "         [[-0.1644]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0644]],\n",
      "\n",
      "         [[-0.4427]],\n",
      "\n",
      "         [[-0.7096]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.7343]],\n",
      "\n",
      "         [[-1.1746]],\n",
      "\n",
      "         [[ 1.5202]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6549]],\n",
      "\n",
      "         [[ 0.8950]],\n",
      "\n",
      "         [[-2.1522]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3506]],\n",
      "\n",
      "         [[ 1.1318]],\n",
      "\n",
      "         [[-0.1238]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2015]],\n",
      "\n",
      "         [[ 0.0365]],\n",
      "\n",
      "         [[ 0.2805]]],\n",
      "\n",
      "\n",
      "        [[[-0.8911]],\n",
      "\n",
      "         [[ 0.5984]],\n",
      "\n",
      "         [[-0.9376]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5085]],\n",
      "\n",
      "         [[ 0.1231]],\n",
      "\n",
      "         [[ 1.7712]]]])\n",
      "Layer: unet.down_blocks.0.attentions.0.proj_out.lora.down.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[ 1.0034e+00]],\n",
      "\n",
      "         [[ 1.2527e-03]],\n",
      "\n",
      "         [[ 1.4510e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2489e+00]],\n",
      "\n",
      "         [[ 1.1375e+00]],\n",
      "\n",
      "         [[-1.2089e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.7104e+00]],\n",
      "\n",
      "         [[-7.5098e-01]],\n",
      "\n",
      "         [[-8.0712e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0388e-01]],\n",
      "\n",
      "         [[-1.1354e+00]],\n",
      "\n",
      "         [[ 2.3652e+00]]],\n",
      "\n",
      "\n",
      "        [[[-5.3746e-01]],\n",
      "\n",
      "         [[-5.3160e-01]],\n",
      "\n",
      "         [[-2.2019e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4416e+00]],\n",
      "\n",
      "         [[ 9.7027e-01]],\n",
      "\n",
      "         [[ 6.8000e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-4.2131e-01]],\n",
      "\n",
      "         [[-1.8199e+00]],\n",
      "\n",
      "         [[ 3.2118e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.0429e+00]],\n",
      "\n",
      "         [[-1.6403e+00]],\n",
      "\n",
      "         [[ 3.1924e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0380e-02]],\n",
      "\n",
      "         [[-1.9389e+00]],\n",
      "\n",
      "         [[ 3.8790e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.1273e+00]],\n",
      "\n",
      "         [[ 1.5256e-01]],\n",
      "\n",
      "         [[-1.9380e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 9.8097e-01]],\n",
      "\n",
      "         [[ 4.0045e-01]],\n",
      "\n",
      "         [[ 1.3716e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.2838e-01]],\n",
      "\n",
      "         [[-2.4325e-01]],\n",
      "\n",
      "         [[-8.3923e-01]]]])\n",
      "Layer: unet.down_blocks.0.attentions.0.proj_out.lora.up.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[-0.5878]],\n",
      "\n",
      "         [[-0.2064]],\n",
      "\n",
      "         [[ 1.0378]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2954]],\n",
      "\n",
      "         [[-1.6564]],\n",
      "\n",
      "         [[-0.5673]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5660]],\n",
      "\n",
      "         [[ 0.2324]],\n",
      "\n",
      "         [[-0.4924]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5638]],\n",
      "\n",
      "         [[ 0.5164]],\n",
      "\n",
      "         [[-0.2955]]],\n",
      "\n",
      "\n",
      "        [[[-1.7800]],\n",
      "\n",
      "         [[-0.4699]],\n",
      "\n",
      "         [[-0.8168]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9833]],\n",
      "\n",
      "         [[-0.6345]],\n",
      "\n",
      "         [[ 0.8687]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.0445]],\n",
      "\n",
      "         [[ 2.2272]],\n",
      "\n",
      "         [[ 0.4715]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1932]],\n",
      "\n",
      "         [[-0.2139]],\n",
      "\n",
      "         [[ 1.9978]]],\n",
      "\n",
      "\n",
      "        [[[-0.5517]],\n",
      "\n",
      "         [[-2.1831]],\n",
      "\n",
      "         [[-1.1349]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7062]],\n",
      "\n",
      "         [[ 0.2759]],\n",
      "\n",
      "         [[-0.3902]]],\n",
      "\n",
      "\n",
      "        [[[-0.4821]],\n",
      "\n",
      "         [[ 1.4223]],\n",
      "\n",
      "         [[ 1.0101]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4060]],\n",
      "\n",
      "         [[ 0.0318]],\n",
      "\n",
      "         [[ 0.2872]]]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 1.0827, -0.2125,  0.3366,  ...,  1.6791,  0.9789,  0.5109],\n",
      "        [-1.6087, -0.4766, -0.3059,  ..., -0.7197,  1.7080,  0.3411],\n",
      "        [ 1.1662, -1.1714,  0.6482,  ..., -0.5135, -1.1364, -2.1290],\n",
      "        ...,\n",
      "        [-1.7913, -1.7044,  0.1301,  ...,  0.1022, -0.6977,  0.9593],\n",
      "        [ 0.1829,  2.2572,  0.0781,  ..., -0.3835, -0.1820,  0.6579],\n",
      "        [-0.7553, -0.7618,  0.7467,  ..., -0.4400, -1.5460, -1.5306]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 6.5273e-01, -2.3017e-01, -2.3948e-01,  ...,  1.2400e+00,\n",
      "          1.3694e+00, -6.4641e-01],\n",
      "        [ 5.6580e-01,  2.0399e-01, -1.6955e+00,  ..., -4.1656e-01,\n",
      "          1.3066e+00, -1.3623e+00],\n",
      "        [-3.1042e+00,  7.4468e-01, -2.6628e-01,  ...,  4.5288e-01,\n",
      "          2.3280e+00, -6.9629e-01],\n",
      "        ...,\n",
      "        [ 1.0061e+00, -6.1067e-01, -3.8938e-01,  ...,  1.2025e-01,\n",
      "         -7.6580e-01, -4.2338e-01],\n",
      "        [-2.7681e-01, -3.9597e-01, -4.9430e-01,  ...,  3.7564e-01,\n",
      "          1.9361e-03,  6.9532e-01],\n",
      "        [-2.4356e+00, -1.3047e+00, -1.0395e+00,  ...,  8.2899e-01,\n",
      "          1.9874e+00,  1.5907e+00]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 1.8857,  2.2123, -0.7476,  ...,  0.5949, -0.7438,  1.0634],\n",
      "        [ 0.1989, -0.6504, -0.3729,  ..., -0.0896,  1.2572, -1.0375],\n",
      "        [-0.0893, -0.9682,  1.0688,  ...,  0.8042, -0.4747,  1.6198],\n",
      "        ...,\n",
      "        [-0.2447,  0.1618, -1.1499,  ..., -0.4357,  0.2070, -2.0710],\n",
      "        [ 0.3056, -0.0805, -0.6821,  ...,  1.2319,  0.4450,  1.1679],\n",
      "        [-0.1156,  0.1590, -0.3604,  ..., -1.3474, -1.2790,  0.3105]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.0376, -0.9047,  0.8480,  ...,  0.6331,  2.3538, -1.1749],\n",
      "        [-0.7617,  0.1552,  1.5809,  ...,  0.4573, -1.0257, -0.7132],\n",
      "        [-0.5723,  1.6409,  1.3900,  ...,  0.8739, -0.6413, -0.2112],\n",
      "        ...,\n",
      "        [-0.4228,  0.3031,  0.7677,  ...,  0.9057, -1.2388, -2.0203],\n",
      "        [-1.3730,  1.0373,  1.0465,  ...,  0.8128, -0.3059, -0.1023],\n",
      "        [ 0.3804,  2.9654,  0.9242,  ...,  1.5918, -0.7915, -2.2625]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.5577,  0.8535, -0.6412,  ..., -0.2455,  0.5166,  0.8076],\n",
      "        [ 0.2901, -1.1297,  0.2838,  ..., -1.5329,  0.2024,  0.9074],\n",
      "        [ 0.9985,  2.4054, -0.6662,  ..., -0.0911,  0.7721,  0.3320],\n",
      "        ...,\n",
      "        [-2.1663, -1.3339, -0.4576,  ...,  1.4314, -0.1502, -0.4859],\n",
      "        [-0.3835,  0.2684,  0.7189,  ...,  1.1721,  1.6887,  0.8815],\n",
      "        [-1.2873,  1.1874,  1.7194,  ..., -0.4723, -0.0281, -1.2380]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.8453,  0.3042,  0.0382,  ...,  0.5426,  0.8428, -1.3689],\n",
      "        [ 0.6798, -0.5385, -0.0968,  ...,  1.1080,  1.4919,  1.9424],\n",
      "        [-0.3159,  0.6022,  1.4410,  ..., -0.5259, -0.5892,  2.0835],\n",
      "        ...,\n",
      "        [ 0.6921,  0.9626, -1.5752,  ..., -1.3328,  0.2324,  0.3739],\n",
      "        [ 0.1090, -0.4061,  0.1321,  ...,  0.3090, -0.0981, -0.1359],\n",
      "        [-0.3035,  0.3654, -1.0384,  ...,  0.6419,  1.3546,  0.0174]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.7680,  0.4661,  0.1288,  ..., -0.9854,  1.0521,  0.0251],\n",
      "        [-1.4180, -2.3793, -1.2185,  ..., -2.1776,  1.8538, -2.1397],\n",
      "        [-2.0473,  0.0687, -0.0172,  ..., -1.1470,  0.2786, -0.4701],\n",
      "        ...,\n",
      "        [ 0.2719,  1.8119, -0.9567,  ..., -1.1962,  1.2854, -0.5371],\n",
      "        [ 0.9549,  0.8844,  1.1230,  ...,  1.4325, -2.0189, -1.6267],\n",
      "        [ 1.4723, -2.1986,  0.4121,  ...,  1.1857, -0.7099,  3.0593]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 1.0401, -2.9565, -1.9792,  ...,  1.7423,  0.9071,  2.4058],\n",
      "        [ 0.5020, -0.3308,  1.2497,  ..., -1.0700, -0.2882, -1.9025],\n",
      "        [ 1.1800, -3.5962,  0.3306,  ..., -0.9669,  0.7454, -0.9929],\n",
      "        ...,\n",
      "        [ 0.9099, -2.4421, -1.4202,  ...,  0.7798,  1.2947, -1.2178],\n",
      "        [ 0.4411, -0.8909,  0.3899,  ..., -3.5526, -1.0817,  0.1141],\n",
      "        [ 2.3624,  3.4800, -1.2444,  ..., -1.1790,  0.7945,  0.2411]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 0.0353,  0.5770, -0.3742,  ...,  0.3761, -0.8061, -0.1636],\n",
      "        [ 0.2437, -0.5179,  0.1805,  ...,  1.6436,  0.0090,  1.1821],\n",
      "        [-0.1893, -0.1102, -0.0970,  ..., -0.0394, -0.3379, -0.0556],\n",
      "        ...,\n",
      "        [ 1.2312,  0.2985,  1.2874,  ...,  0.2818,  0.5545, -2.1263],\n",
      "        [ 1.1490, -0.7386, -0.2257,  ..., -0.7480, -1.1332,  1.8979],\n",
      "        [ 0.3412, -1.0487, -0.1097,  ...,  1.0305, -0.3274, -0.5941]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.8655,  1.0915, -0.6563,  ...,  1.4695,  0.2641,  0.2060],\n",
      "        [ 2.1339,  1.0228, -1.0646,  ..., -1.7936,  0.3706,  1.1581],\n",
      "        [ 0.6617, -0.5413,  1.5418,  ..., -0.2583,  0.3209,  0.7605],\n",
      "        ...,\n",
      "        [ 0.7212, -0.9025,  1.1065,  ...,  0.6216, -0.8351, -0.8595],\n",
      "        [ 0.4838, -0.0285,  0.9609,  ..., -0.4755, -0.8275,  1.5897],\n",
      "        [-0.6808, -0.4314,  0.2898,  ...,  0.6441,  1.1680, -0.8594]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.7423,  0.0554, -1.1063,  ..., -0.5166,  0.4963,  0.9686],\n",
      "        [-1.1167,  0.5091,  0.5665,  ..., -0.3882, -1.3481, -0.1911],\n",
      "        [-0.3987, -0.5393, -1.6264,  ..., -0.7331,  0.3580, -1.1563],\n",
      "        ...,\n",
      "        [-0.9165,  1.5456, -0.1541,  ..., -0.9939, -1.6164,  0.0825],\n",
      "        [ 0.2558,  0.4615,  0.1132,  ...,  0.3368,  0.7375,  0.4605],\n",
      "        [-3.0691,  1.8027, -0.5915,  ...,  0.5666, -0.7051,  1.2534]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.4854, -0.2600, -1.0182,  ...,  1.8910, -0.5126,  1.6605],\n",
      "        [-1.9482,  1.1467, -0.3554,  ...,  3.5033, -1.1503, -0.7410],\n",
      "        [ 1.8836, -0.0623,  1.0804,  ..., -1.9174,  0.1817,  2.3242],\n",
      "        ...,\n",
      "        [ 0.6528, -0.4578, -0.1364,  ..., -0.1531, -1.1097,  1.6617],\n",
      "        [ 0.0211,  0.8568, -1.2813,  ...,  0.7648, -1.4780,  3.1428],\n",
      "        [ 0.7702,  1.2319, -1.0317,  ...,  1.3640,  1.7745,  3.2691]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.6177, -1.4241, -0.5843,  ...,  0.7673,  0.0422,  0.0725],\n",
      "        [ 0.6202, -0.3544, -1.9865,  ...,  0.6364,  1.1808,  0.2748],\n",
      "        [-2.2911, -1.5659,  0.2946,  ...,  0.6656, -2.0058,  2.1868],\n",
      "        ...,\n",
      "        [-0.9136, -2.1519,  0.4225,  ..., -3.8806,  0.3034, -1.2994],\n",
      "        [-0.9190,  0.0704, -0.5573,  ..., -0.8007, -1.7490, -0.9291],\n",
      "        [-2.0740,  1.2992,  0.0635,  ...,  0.3379, -0.3361, -0.3408]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.5245,  1.8313, -1.0567,  ...,  1.7945, -1.4132, -1.8912],\n",
      "        [ 0.9186,  0.1052, -0.5317,  ...,  0.3930, -0.8234, -0.3377],\n",
      "        [ 0.3705, -0.3553, -0.4706,  ...,  0.7129, -0.3343,  0.9880],\n",
      "        ...,\n",
      "        [ 0.2403,  0.4403,  1.1155,  ...,  0.1323,  0.3475, -0.0273],\n",
      "        [-0.1618,  0.5789, -1.0333,  ..., -0.2354,  0.4477, -0.0456],\n",
      "        [-0.0456,  0.1142,  0.5725,  ...,  1.0469,  0.8366,  0.6549]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[-0.9403, -0.9747,  0.3896,  ..., -0.2068,  0.4170,  0.8953],\n",
      "        [-1.6984, -0.8486,  0.6504,  ..., -1.3818, -0.6965, -0.8717],\n",
      "        [ 0.3014,  0.2278, -1.2548,  ...,  0.3180,  0.0114, -0.2658],\n",
      "        ...,\n",
      "        [-0.0058,  0.6097, -0.8845,  ...,  0.9028,  0.7087,  0.0897],\n",
      "        [ 0.8264, -2.4443,  0.9369,  ...,  0.4232,  0.8717,  1.6465],\n",
      "        [ 0.3788,  0.4887, -0.0053,  ..., -1.0389,  0.5388, -1.0252]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-1.2130,  1.3614, -0.3991,  ...,  1.0881,  0.2637, -1.3249],\n",
      "        [-0.9573,  0.3149, -0.3819,  ...,  0.9212,  0.0074,  0.2333],\n",
      "        [-1.4728,  0.3461,  0.1588,  ...,  1.1894, -0.9837,  0.8032],\n",
      "        ...,\n",
      "        [-0.4077,  0.6121,  0.2625,  ...,  0.5676, -0.2726, -1.0253],\n",
      "        [ 0.2373,  0.8264, -0.5621,  ..., -1.9231,  0.4977, -0.0393],\n",
      "        [-0.6162,  0.7299, -2.0413,  ..., -1.6417, -1.5013, -0.8677]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-1.3465, -0.4065, -0.0502,  ...,  0.4778, -0.3113,  0.1552],\n",
      "        [ 1.1412,  0.1253, -0.4933,  ...,  1.7091, -0.5894,  0.7314],\n",
      "        [-0.0986, -0.8223,  0.0269,  ..., -0.7524,  0.4135, -0.8539],\n",
      "        ...,\n",
      "        [ 0.8446, -1.4215, -0.7316,  ...,  1.0594,  0.2190,  0.3059],\n",
      "        [-0.6768,  1.0905, -0.8951,  ...,  2.2206, -0.5553, -0.6101],\n",
      "        [-0.2322,  1.1787, -0.4304,  ...,  0.8243,  1.0671, -1.8962]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shape: torch.Size([2560, 320])\n",
      "tensor([[-0.1711,  0.6324, -1.3761,  ..., -0.0924, -0.2619, -0.3303],\n",
      "        [-1.6694, -3.0234, -1.4030,  ..., -1.0655,  1.6238, -0.0484],\n",
      "        [ 0.2386, -0.5273,  0.1365,  ..., -0.9340, -1.2460, -1.4259],\n",
      "        ...,\n",
      "        [ 0.6979, -0.7507,  0.2589,  ..., -1.8603,  0.4166, -0.1724],\n",
      "        [-0.7497, -1.1117,  1.0992,  ...,  0.7147,  0.7381, -0.9008],\n",
      "        [-0.1971, -1.8196,  1.7056,  ...,  0.9232,  0.7062, -0.1681]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[ 0.0104, -0.6568, -0.4386,  ..., -0.7646,  0.7394, -1.0918],\n",
      "        [ 1.6452, -1.9169,  0.0554,  ..., -0.9911,  0.0487,  1.6791],\n",
      "        [ 1.2251,  3.0033, -0.6751,  ...,  0.4019, -0.2154,  1.5135],\n",
      "        ...,\n",
      "        [-0.6436,  2.5980,  0.1084,  ...,  0.4725, -1.1863,  0.8501],\n",
      "        [-0.8794, -0.7074, -0.4062,  ..., -0.1906,  1.6504, -0.1845],\n",
      "        [-1.4153,  2.4417, -0.1465,  ...,  0.4617, -2.0629,  1.5548]])\n",
      "Layer: unet.down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.4898,  0.0298, -1.1789,  ..., -1.0170, -0.6260,  0.0585],\n",
      "        [-0.2099,  0.3426,  0.8854,  ...,  0.6710,  1.9174, -0.2897],\n",
      "        [-0.2726, -0.6477,  1.3553,  ..., -0.8449, -0.0525,  0.2414],\n",
      "        ...,\n",
      "        [-1.0712,  0.8679, -1.0180,  ..., -1.4089,  0.0585, -0.1610],\n",
      "        [ 1.5241,  2.2987, -0.6148,  ..., -1.1218,  0.4502, -2.0130],\n",
      "        [ 1.3771, -0.5563,  0.0769,  ...,  0.9204,  2.3194, -0.1383]])\n",
      "Layer: unet.down_blocks.0.attentions.1.proj_in.lora.down.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[-0.0289]],\n",
      "\n",
      "         [[-0.1792]],\n",
      "\n",
      "         [[ 1.3172]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8691]],\n",
      "\n",
      "         [[ 0.3181]],\n",
      "\n",
      "         [[ 1.8812]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3237]],\n",
      "\n",
      "         [[-0.3894]],\n",
      "\n",
      "         [[ 1.2627]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0406]],\n",
      "\n",
      "         [[-1.3099]],\n",
      "\n",
      "         [[ 1.5886]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3676]],\n",
      "\n",
      "         [[-0.1454]],\n",
      "\n",
      "         [[-0.9377]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2851]],\n",
      "\n",
      "         [[ 0.5039]],\n",
      "\n",
      "         [[ 0.6465]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.3615]],\n",
      "\n",
      "         [[ 0.4722]],\n",
      "\n",
      "         [[-1.1245]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2783]],\n",
      "\n",
      "         [[ 1.0317]],\n",
      "\n",
      "         [[-0.7263]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5360]],\n",
      "\n",
      "         [[ 0.0639]],\n",
      "\n",
      "         [[ 0.0413]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4980]],\n",
      "\n",
      "         [[-1.0097]],\n",
      "\n",
      "         [[ 1.6918]]],\n",
      "\n",
      "\n",
      "        [[[-0.2860]],\n",
      "\n",
      "         [[ 0.2479]],\n",
      "\n",
      "         [[ 0.6456]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7400]],\n",
      "\n",
      "         [[ 0.1839]],\n",
      "\n",
      "         [[-0.0305]]]])\n",
      "Layer: unet.down_blocks.0.attentions.1.proj_in.lora.up.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[-1.3713]],\n",
      "\n",
      "         [[-0.9582]],\n",
      "\n",
      "         [[-0.4480]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3856]],\n",
      "\n",
      "         [[ 0.7180]],\n",
      "\n",
      "         [[ 1.0270]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1452]],\n",
      "\n",
      "         [[ 0.0560]],\n",
      "\n",
      "         [[-0.0832]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0517]],\n",
      "\n",
      "         [[ 0.6525]],\n",
      "\n",
      "         [[ 0.6468]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7477]],\n",
      "\n",
      "         [[ 0.4937]],\n",
      "\n",
      "         [[-0.7435]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3141]],\n",
      "\n",
      "         [[ 0.9535]],\n",
      "\n",
      "         [[-0.6853]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0039]],\n",
      "\n",
      "         [[ 0.0599]],\n",
      "\n",
      "         [[-2.1593]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8404]],\n",
      "\n",
      "         [[-1.0432]],\n",
      "\n",
      "         [[-2.3673]]],\n",
      "\n",
      "\n",
      "        [[[-1.4849]],\n",
      "\n",
      "         [[ 0.6428]],\n",
      "\n",
      "         [[ 1.5721]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1451]],\n",
      "\n",
      "         [[ 0.1084]],\n",
      "\n",
      "         [[-0.6142]]],\n",
      "\n",
      "\n",
      "        [[[-2.2545]],\n",
      "\n",
      "         [[-0.0819]],\n",
      "\n",
      "         [[-0.0228]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2870]],\n",
      "\n",
      "         [[ 1.1223]],\n",
      "\n",
      "         [[ 1.1053]]]])\n",
      "Layer: unet.down_blocks.0.attentions.1.proj_out.lora.down.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[ 0.4950]],\n",
      "\n",
      "         [[-0.7855]],\n",
      "\n",
      "         [[-1.1614]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4690]],\n",
      "\n",
      "         [[ 1.9521]],\n",
      "\n",
      "         [[-0.2914]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6072]],\n",
      "\n",
      "         [[ 0.5846]],\n",
      "\n",
      "         [[-0.8737]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.9318]],\n",
      "\n",
      "         [[-1.3849]],\n",
      "\n",
      "         [[-0.7817]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9684]],\n",
      "\n",
      "         [[-2.0818]],\n",
      "\n",
      "         [[ 0.6669]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.4910]],\n",
      "\n",
      "         [[-0.2727]],\n",
      "\n",
      "         [[-0.7562]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.3178]],\n",
      "\n",
      "         [[-0.9726]],\n",
      "\n",
      "         [[-0.2896]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0092]],\n",
      "\n",
      "         [[ 1.4818]],\n",
      "\n",
      "         [[-1.1177]]],\n",
      "\n",
      "\n",
      "        [[[-0.0575]],\n",
      "\n",
      "         [[ 1.3879]],\n",
      "\n",
      "         [[-0.7626]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7197]],\n",
      "\n",
      "         [[-1.2882]],\n",
      "\n",
      "         [[ 0.3223]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1492]],\n",
      "\n",
      "         [[ 0.7976]],\n",
      "\n",
      "         [[ 0.8372]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.3496]],\n",
      "\n",
      "         [[ 0.4618]],\n",
      "\n",
      "         [[ 1.4799]]]])\n",
      "Layer: unet.down_blocks.0.attentions.1.proj_out.lora.up.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[ 0.3232]],\n",
      "\n",
      "         [[-1.3279]],\n",
      "\n",
      "         [[ 0.2143]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6458]],\n",
      "\n",
      "         [[-0.3800]],\n",
      "\n",
      "         [[-0.3057]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2188]],\n",
      "\n",
      "         [[ 0.1102]],\n",
      "\n",
      "         [[-1.0547]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3950]],\n",
      "\n",
      "         [[-0.3790]],\n",
      "\n",
      "         [[-0.7597]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8019]],\n",
      "\n",
      "         [[ 0.5441]],\n",
      "\n",
      "         [[-1.1613]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4294]],\n",
      "\n",
      "         [[-0.8889]],\n",
      "\n",
      "         [[ 1.6517]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.6376]],\n",
      "\n",
      "         [[ 0.7372]],\n",
      "\n",
      "         [[ 0.9324]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.8505]],\n",
      "\n",
      "         [[ 1.9148]],\n",
      "\n",
      "         [[ 0.5644]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3577]],\n",
      "\n",
      "         [[-0.8819]],\n",
      "\n",
      "         [[ 1.6536]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2494]],\n",
      "\n",
      "         [[ 0.3142]],\n",
      "\n",
      "         [[-0.7636]]],\n",
      "\n",
      "\n",
      "        [[[-0.7250]],\n",
      "\n",
      "         [[-0.4322]],\n",
      "\n",
      "         [[ 0.9298]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8517]],\n",
      "\n",
      "         [[-0.3779]],\n",
      "\n",
      "         [[ 0.5346]]]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.3939, -2.2485,  1.7030,  ...,  0.4782, -1.0099, -1.9751],\n",
      "        [ 1.2493, -2.9324, -0.6678,  ..., -0.3705, -1.2731, -1.4843],\n",
      "        [ 3.3346, -0.4578, -0.3111,  ...,  0.7253,  0.1293,  0.0635],\n",
      "        ...,\n",
      "        [-1.6221,  0.1717,  0.3712,  ..., -1.3745,  2.8487, -1.2092],\n",
      "        [-0.5170, -1.1615,  0.0904,  ...,  1.1799, -0.5881, -2.2117],\n",
      "        [ 0.9498, -0.4430,  0.1871,  ...,  0.5059, -1.8614, -0.0362]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.5833, -2.2130, -1.1198,  ...,  1.6997, -1.5530, -0.3845],\n",
      "        [ 1.5910,  1.4163,  0.5360,  ..., -0.4114,  1.5529,  1.5263],\n",
      "        [-0.7407,  0.7307,  0.0805,  ..., -0.8085,  1.8445, -1.6528],\n",
      "        ...,\n",
      "        [-0.0707, -1.0778, -0.6825,  ..., -0.4202,  0.1773,  0.3463],\n",
      "        [ 0.9518, -0.9442, -0.9129,  ..., -0.2708,  0.6853, -0.6187],\n",
      "        [ 0.4033,  0.3638,  1.4660,  ..., -1.1517, -1.5725, -0.9101]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.7830,  0.4320,  0.4742,  ..., -0.2685, -0.2961,  0.2953],\n",
      "        [ 1.2951,  0.4823,  0.4543,  ..., -1.2379,  0.4931, -0.0969],\n",
      "        [-1.6100,  0.5719,  0.1150,  ..., -1.1677, -0.1355, -0.7239],\n",
      "        ...,\n",
      "        [ 0.4439,  1.2732, -0.9758,  ...,  0.0553,  0.5546, -1.3783],\n",
      "        [-0.1526,  1.2995,  0.4904,  ..., -0.3282,  0.2084, -0.0030],\n",
      "        [ 2.0085, -1.2252,  1.1210,  ..., -1.4708,  0.2103, -0.3166]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.8869,  1.6123, -0.8518,  ...,  3.2349,  0.0493, -1.5586],\n",
      "        [ 2.4388, -1.8818, -0.0991,  ..., -1.0811,  0.1097,  0.5052],\n",
      "        [-1.0338,  0.5623,  0.7834,  ..., -0.2823,  1.4450,  0.9057],\n",
      "        ...,\n",
      "        [ 0.1870, -0.4479,  1.6539,  ...,  0.3182,  0.0244,  1.6888],\n",
      "        [ 0.4634,  0.6914,  1.4039,  ...,  0.4987,  0.4575, -1.3528],\n",
      "        [-0.0820, -0.1605, -0.2997,  ...,  0.6406,  0.8779, -1.8035]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.3412, -0.2741, -0.8835,  ...,  0.3123, -0.3379, -0.3059],\n",
      "        [-2.0079, -0.4865,  1.1127,  ...,  1.3867, -0.8192,  0.1399],\n",
      "        [ 0.9251,  0.7635,  1.7932,  ...,  1.1801, -0.9280,  1.8996],\n",
      "        ...,\n",
      "        [ 0.6574,  0.8993,  0.2198,  ...,  1.0956,  0.4381,  0.3672],\n",
      "        [ 1.3510, -0.8124,  1.2930,  ..., -0.0861, -1.1265, -0.9111],\n",
      "        [-1.9550, -2.5664,  0.2358,  ...,  0.2697, -1.4031, -0.8163]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 3.1711,  0.0183,  1.4200,  ..., -1.2424,  1.2153,  1.3028],\n",
      "        [-0.4337,  2.0904,  0.1366,  ..., -0.1816, -0.6306,  0.4178],\n",
      "        [-0.8251,  0.4088, -1.7692,  ..., -0.5337,  1.1230,  0.3854],\n",
      "        ...,\n",
      "        [-2.6567, -0.2112, -0.6835,  ...,  0.8025, -0.1052, -0.6659],\n",
      "        [-0.1973, -0.2850, -0.1407,  ..., -0.0102,  0.4393, -0.1047],\n",
      "        [ 1.1866, -0.5387, -0.5995,  ..., -0.7513,  0.3987,  0.4407]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.5346, -1.4816,  0.9665,  ...,  0.0485,  0.5618,  1.3586],\n",
      "        [-0.3717, -1.1565,  0.7759,  ..., -0.3407, -1.2694,  0.1697],\n",
      "        [ 1.1555,  0.1335, -0.4229,  ...,  1.0907, -2.8147, -0.2731],\n",
      "        ...,\n",
      "        [-0.3613,  0.9251, -0.9381,  ..., -0.6339, -0.1523,  0.0216],\n",
      "        [ 0.4377, -0.0724, -2.1855,  ...,  2.1659,  0.3311, -2.1203],\n",
      "        [-0.4624, -0.3614, -2.9029,  ..., -0.6280,  3.4372,  0.7062]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 2.0811,  1.0868, -0.0987,  ...,  0.1931,  2.2979, -0.8708],\n",
      "        [-0.7648, -0.3603, -0.4184,  ..., -1.2786,  0.2680,  2.1725],\n",
      "        [ 0.2964, -1.5911, -1.9471,  ..., -2.3564,  0.9469, -1.0995],\n",
      "        ...,\n",
      "        [ 1.0230, -0.3463,  1.6300,  ...,  0.9850, -1.5628,  0.4038],\n",
      "        [ 0.9392, -2.7325, -0.9026,  ..., -0.8029,  0.4257,  0.0576],\n",
      "        [-0.1415, -1.2184,  0.3793,  ..., -0.3778,  1.1199,  0.2169]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[-1.0560,  0.4174, -0.4649,  ..., -0.8663, -0.5942,  0.7716],\n",
      "        [ 1.0428,  0.6930, -1.0934,  ...,  0.4787,  0.3896, -0.6650],\n",
      "        [-1.0541,  0.0814,  0.7658,  ..., -0.1995, -1.3884,  0.0548],\n",
      "        ...,\n",
      "        [ 0.1044,  1.0277, -1.7128,  ...,  1.9840,  0.0919,  0.7770],\n",
      "        [-1.1555, -1.5365, -0.6657,  ..., -0.2755, -0.9653,  0.4103],\n",
      "        [ 1.3760, -0.1007,  0.6862,  ...,  0.2441, -1.0429,  0.2386]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.2482,  0.9854,  0.7470,  ..., -2.1798, -0.1302, -0.4268],\n",
      "        [-0.3607,  0.4679,  0.4317,  ...,  0.1727,  0.4287, -1.3704],\n",
      "        [ 1.2831, -0.8119, -1.1412,  ..., -2.3126,  0.3495,  1.8504],\n",
      "        ...,\n",
      "        [ 1.1967,  0.0533,  0.2120,  ...,  1.4050, -0.1844, -0.3803],\n",
      "        [ 0.2965,  0.0532,  0.9062,  ...,  0.3364,  0.7212,  0.6092],\n",
      "        [-0.5340, -0.8465, -0.7253,  ..., -0.4552, -0.7857,  0.6627]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.7218,  1.7467,  1.4785,  ..., -0.2292,  1.7118,  0.0743],\n",
      "        [-0.6351, -1.6005, -0.9093,  ...,  0.3254, -0.0838,  0.2081],\n",
      "        [ 0.4890, -2.7698,  0.0124,  ..., -2.4424,  1.6814, -0.6801],\n",
      "        ...,\n",
      "        [ 1.2678,  0.2617,  1.7691,  ..., -0.9188, -0.7486,  0.1233],\n",
      "        [ 0.2321,  0.6612,  0.4649,  ...,  0.0227, -0.6348,  1.7730],\n",
      "        [ 2.3728,  0.1854, -2.0024,  ...,  0.0373, -1.3837, -0.9567]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.3292, -3.2061,  2.3133,  ..., -1.8794, -0.0777, -1.1142],\n",
      "        [ 1.0386, -0.8406,  0.0602,  ...,  1.9540, -1.1570, -0.7217],\n",
      "        [-0.6820,  0.5530,  0.4506,  ..., -0.1142,  0.3862, -0.0678],\n",
      "        ...,\n",
      "        [-1.7615, -0.3242, -1.0142,  ...,  0.3589,  0.5936,  0.3516],\n",
      "        [-1.2604,  1.0399, -0.7717,  ..., -0.0389,  1.0618, -0.5985],\n",
      "        [ 0.0469, -0.9415, -0.1414,  ...,  1.1807, -1.3198, -0.0626]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.4585, -1.1766,  0.3874,  ...,  0.4469, -0.6524, -0.9164],\n",
      "        [ 0.8158,  0.9401,  0.4878,  ...,  1.1663, -0.7288,  0.4358],\n",
      "        [ 0.2391, -0.4159,  0.0139,  ...,  0.7125,  0.4595, -0.0750],\n",
      "        ...,\n",
      "        [ 0.1819,  0.4154, -0.4165,  ..., -0.9966,  1.5831, -0.3390],\n",
      "        [ 0.3892,  0.3684, -1.6335,  ..., -1.0335,  0.5015,  0.7224],\n",
      "        [-0.6417,  0.1313,  0.6993,  ..., -1.1440, -0.7806, -3.0270]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.4543,  0.8852,  1.1004,  ..., -0.3451, -1.8050,  0.3385],\n",
      "        [ 0.8379,  1.5082, -0.3867,  ..., -1.4146, -1.4172, -0.3781],\n",
      "        [ 0.2794, -2.3148, -0.3925,  ...,  0.4513, -0.1805, -0.3861],\n",
      "        ...,\n",
      "        [ 1.8024,  0.4833,  0.3727,  ..., -0.7110, -0.8002,  0.3733],\n",
      "        [ 0.5419,  1.6806,  1.6276,  ...,  0.3392, -1.5301,  1.2902],\n",
      "        [-0.2699, -1.3021,  0.2773,  ...,  0.1156, -1.0125,  0.3090]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 0.6952, -0.1676, -1.3928,  ..., -0.7696, -0.3050, -0.4768],\n",
      "        [ 1.2374,  0.4256, -0.3664,  ...,  0.3971, -1.3117,  1.1519],\n",
      "        [ 0.2879,  1.9735,  0.4968,  ...,  0.3734, -0.8885,  0.0060],\n",
      "        ...,\n",
      "        [ 0.5929,  1.3884, -1.0932,  ...,  1.6011, -1.0104,  0.3040],\n",
      "        [-0.1418,  0.4046, -0.6568,  ...,  0.2683, -0.8379, -0.1911],\n",
      "        [ 0.3829, -1.5420,  0.9852,  ...,  0.8516, -0.4523, -0.7772]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.1448,  1.7675,  0.3519,  ...,  1.7287,  0.0584, -0.7504],\n",
      "        [ 0.1917, -1.8464,  0.8415,  ..., -3.7393,  0.1668,  0.4670],\n",
      "        [ 0.9146, -1.7353,  0.4553,  ...,  1.3208,  0.0280, -0.0857],\n",
      "        ...,\n",
      "        [ 0.6692, -0.5693,  2.3474,  ...,  0.5130,  0.1654, -0.6639],\n",
      "        [ 0.2716, -1.5355,  0.9498,  ...,  1.6436, -0.5710, -0.9580],\n",
      "        [-0.5455,  0.2085,  0.0958,  ...,  0.5982,  0.9975, -1.2617]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.9810, -0.3102, -1.9668,  ..., -2.6857,  1.1643,  0.0358],\n",
      "        [-2.6454,  0.9507,  0.8566,  ...,  0.0086, -0.1001, -2.5676],\n",
      "        [ 0.3303,  0.7805, -0.9871,  ..., -0.7019,  0.2430,  1.6876],\n",
      "        ...,\n",
      "        [-1.4155, -0.5127, -1.0737,  ...,  2.1463, -1.1201,  0.9591],\n",
      "        [-0.0857, -0.8600,  1.8127,  ...,  1.1594, -0.9195, -1.6447],\n",
      "        [-0.7582,  0.3342,  0.2336,  ..., -1.0458,  1.5868,  0.1749]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shape: torch.Size([2560, 320])\n",
      "tensor([[ 1.1788,  0.2060,  0.2875,  ..., -0.0413, -0.0535, -0.2007],\n",
      "        [ 1.8057, -0.8242, -0.1554,  ...,  0.1986, -0.4880, -0.3036],\n",
      "        [-0.3932, -0.9556,  1.4138,  ...,  0.3803, -0.1754,  1.0361],\n",
      "        ...,\n",
      "        [ 1.4902,  0.5224,  0.1104,  ...,  1.3364, -0.3364,  0.3908],\n",
      "        [-0.6430,  0.4809, -0.2550,  ...,  0.4992, -0.4388,  0.9432],\n",
      "        [-0.8339,  1.8295,  0.8964,  ...,  1.5554, -1.0179,  0.2099]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.0357, -0.7118,  0.1200,  ...,  0.1929, -0.0715, -0.1472],\n",
      "        [-0.3043,  0.6359, -1.1544,  ...,  0.5058,  0.4082, -2.0458],\n",
      "        [-0.1734,  0.2552, -1.0377,  ...,  0.4415,  0.6961, -1.6326],\n",
      "        ...,\n",
      "        [ 0.8213, -0.4512, -0.4271,  ...,  0.1928, -0.2797, -1.1473],\n",
      "        [ 0.6229, -0.3014, -0.1753,  ..., -1.0656,  0.9856, -0.8874],\n",
      "        [ 0.0295, -2.2609,  0.8262,  ...,  0.8664,  0.4286, -0.4548]])\n",
      "Layer: unet.down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-2.4100, -1.0382,  1.7956,  ...,  0.1676,  1.3036, -0.1913],\n",
      "        [ 2.3186, -0.3705, -1.4154,  ..., -1.8732,  2.3589,  0.3449],\n",
      "        [-0.9040,  1.1611,  1.3834,  ...,  1.0762,  1.4578,  1.1161],\n",
      "        ...,\n",
      "        [-0.9909,  1.1507,  1.5656,  ...,  0.3666, -0.0246, -1.1931],\n",
      "        [ 0.2020, -0.8647,  0.4900,  ..., -0.4565, -1.1068,  0.2953],\n",
      "        [-0.7194,  1.8900,  0.8134,  ..., -0.3249, -1.1921,  0.0024]])\n",
      "Layer: unet.down_blocks.1.attentions.0.proj_in.lora.down.weight, Shape: torch.Size([320, 640, 1, 1])\n",
      "tensor([[[[-0.1660]],\n",
      "\n",
      "         [[-1.0376]],\n",
      "\n",
      "         [[ 0.1902]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9757]],\n",
      "\n",
      "         [[-0.5139]],\n",
      "\n",
      "         [[ 1.3117]]],\n",
      "\n",
      "\n",
      "        [[[-0.2599]],\n",
      "\n",
      "         [[ 0.9593]],\n",
      "\n",
      "         [[ 1.6810]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9100]],\n",
      "\n",
      "         [[-1.3028]],\n",
      "\n",
      "         [[ 1.1814]]],\n",
      "\n",
      "\n",
      "        [[[-0.3727]],\n",
      "\n",
      "         [[-0.5248]],\n",
      "\n",
      "         [[ 0.1594]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3374]],\n",
      "\n",
      "         [[-0.3825]],\n",
      "\n",
      "         [[ 0.9745]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.1664]],\n",
      "\n",
      "         [[ 0.0699]],\n",
      "\n",
      "         [[ 0.5183]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2934]],\n",
      "\n",
      "         [[-0.0451]],\n",
      "\n",
      "         [[ 0.2961]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5740]],\n",
      "\n",
      "         [[ 1.1942]],\n",
      "\n",
      "         [[-2.2032]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2140]],\n",
      "\n",
      "         [[-0.0588]],\n",
      "\n",
      "         [[-0.1343]]],\n",
      "\n",
      "\n",
      "        [[[-0.5395]],\n",
      "\n",
      "         [[ 2.5327]],\n",
      "\n",
      "         [[ 0.1355]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0339]],\n",
      "\n",
      "         [[-0.3029]],\n",
      "\n",
      "         [[-0.0666]]]])\n",
      "Layer: unet.down_blocks.1.attentions.0.proj_in.lora.up.weight, Shape: torch.Size([640, 320, 1, 1])\n",
      "tensor([[[[ 1.3140]],\n",
      "\n",
      "         [[ 1.5820]],\n",
      "\n",
      "         [[ 2.1203]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1490]],\n",
      "\n",
      "         [[-0.0058]],\n",
      "\n",
      "         [[ 1.0287]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0724]],\n",
      "\n",
      "         [[-2.6567]],\n",
      "\n",
      "         [[-1.6823]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1518]],\n",
      "\n",
      "         [[-0.8182]],\n",
      "\n",
      "         [[-0.5263]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6281]],\n",
      "\n",
      "         [[ 0.5083]],\n",
      "\n",
      "         [[-0.8983]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5786]],\n",
      "\n",
      "         [[ 0.3135]],\n",
      "\n",
      "         [[-0.5964]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.0753]],\n",
      "\n",
      "         [[-1.6632]],\n",
      "\n",
      "         [[ 2.5479]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1598]],\n",
      "\n",
      "         [[-0.3388]],\n",
      "\n",
      "         [[-1.1184]]],\n",
      "\n",
      "\n",
      "        [[[-1.4186]],\n",
      "\n",
      "         [[ 2.1104]],\n",
      "\n",
      "         [[ 0.5023]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0107]],\n",
      "\n",
      "         [[ 0.1334]],\n",
      "\n",
      "         [[-0.5229]]],\n",
      "\n",
      "\n",
      "        [[[-2.0929]],\n",
      "\n",
      "         [[ 0.1548]],\n",
      "\n",
      "         [[ 1.5459]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3645]],\n",
      "\n",
      "         [[-0.2473]],\n",
      "\n",
      "         [[-0.5266]]]])\n",
      "Layer: unet.down_blocks.1.attentions.0.proj_out.lora.down.weight, Shape: torch.Size([320, 640, 1, 1])\n",
      "tensor([[[[ 0.3796]],\n",
      "\n",
      "         [[ 0.0515]],\n",
      "\n",
      "         [[-0.1844]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9907]],\n",
      "\n",
      "         [[-0.9898]],\n",
      "\n",
      "         [[ 0.5484]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7164]],\n",
      "\n",
      "         [[-0.5869]],\n",
      "\n",
      "         [[-2.0599]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6348]],\n",
      "\n",
      "         [[-0.2446]],\n",
      "\n",
      "         [[-0.0386]]],\n",
      "\n",
      "\n",
      "        [[[-0.2641]],\n",
      "\n",
      "         [[ 0.1432]],\n",
      "\n",
      "         [[ 0.6857]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.5265]],\n",
      "\n",
      "         [[ 0.5558]],\n",
      "\n",
      "         [[ 0.0962]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.5864]],\n",
      "\n",
      "         [[-1.4942]],\n",
      "\n",
      "         [[ 0.9896]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.2379]],\n",
      "\n",
      "         [[ 0.1501]],\n",
      "\n",
      "         [[-1.0626]]],\n",
      "\n",
      "\n",
      "        [[[-0.4433]],\n",
      "\n",
      "         [[-0.4292]],\n",
      "\n",
      "         [[ 0.9130]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2353]],\n",
      "\n",
      "         [[-1.1337]],\n",
      "\n",
      "         [[-0.2003]]],\n",
      "\n",
      "\n",
      "        [[[-0.4772]],\n",
      "\n",
      "         [[ 0.1639]],\n",
      "\n",
      "         [[ 0.9544]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5290]],\n",
      "\n",
      "         [[-1.2915]],\n",
      "\n",
      "         [[ 0.4774]]]])\n",
      "Layer: unet.down_blocks.1.attentions.0.proj_out.lora.up.weight, Shape: torch.Size([640, 320, 1, 1])\n",
      "tensor([[[[ 0.1737]],\n",
      "\n",
      "         [[ 0.4622]],\n",
      "\n",
      "         [[ 1.5272]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6204]],\n",
      "\n",
      "         [[-0.3106]],\n",
      "\n",
      "         [[ 0.7018]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9315]],\n",
      "\n",
      "         [[ 0.4788]],\n",
      "\n",
      "         [[-0.6354]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6195]],\n",
      "\n",
      "         [[-2.2328]],\n",
      "\n",
      "         [[ 1.0256]]],\n",
      "\n",
      "\n",
      "        [[[-1.4972]],\n",
      "\n",
      "         [[ 0.3282]],\n",
      "\n",
      "         [[ 0.3511]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4938]],\n",
      "\n",
      "         [[ 1.5101]],\n",
      "\n",
      "         [[-0.8186]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.9700]],\n",
      "\n",
      "         [[ 0.3172]],\n",
      "\n",
      "         [[-0.2716]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1979]],\n",
      "\n",
      "         [[ 0.4257]],\n",
      "\n",
      "         [[ 0.9299]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7589]],\n",
      "\n",
      "         [[-1.4062]],\n",
      "\n",
      "         [[ 0.9538]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1216]],\n",
      "\n",
      "         [[-0.5263]],\n",
      "\n",
      "         [[ 0.8679]]],\n",
      "\n",
      "\n",
      "        [[[-0.0296]],\n",
      "\n",
      "         [[-0.4005]],\n",
      "\n",
      "         [[ 0.5818]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9798]],\n",
      "\n",
      "         [[ 0.4561]],\n",
      "\n",
      "         [[-0.9535]]]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[-0.1490, -0.4631,  1.5453,  ...,  0.7754,  1.5444, -0.2457],\n",
      "        [ 0.3644,  0.4786, -0.3204,  ..., -1.9825,  0.8274,  1.6821],\n",
      "        [-0.8652,  0.1449,  0.7122,  ...,  0.6405, -2.0094, -1.4205],\n",
      "        ...,\n",
      "        [-0.5993,  1.1677,  0.9008,  ..., -0.4258, -0.5799, -0.8133],\n",
      "        [ 0.0657, -1.4395,  1.5309,  ...,  0.8440,  2.1107,  0.6232],\n",
      "        [-0.2224,  0.3172, -1.2952,  ...,  0.5991,  0.0357,  0.1080]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-3.3163e-01, -1.1929e+00,  8.1469e-01,  ..., -6.7174e-01,\n",
      "          3.8291e-01,  1.4581e+00],\n",
      "        [ 4.6252e-01, -1.9228e+00,  1.1563e-01,  ...,  1.8314e-03,\n",
      "         -2.3517e-01,  8.8919e-02],\n",
      "        [ 6.3021e-01, -9.6305e-01,  5.7974e-01,  ..., -5.2253e-01,\n",
      "          1.5827e+00, -6.1039e-01],\n",
      "        ...,\n",
      "        [ 3.0079e-01,  1.8048e-01, -4.2544e-01,  ..., -1.2638e+00,\n",
      "         -1.5440e+00,  4.9654e-01],\n",
      "        [-5.8533e-01, -1.4666e+00, -5.3046e-01,  ...,  2.2715e+00,\n",
      "          7.3984e-01, -8.6774e-02],\n",
      "        [ 5.9393e-02,  7.8952e-01, -6.4153e-01,  ..., -5.2517e-01,\n",
      "          1.2556e-02,  1.8382e-01]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 0.1365, -0.0126, -0.5857,  ...,  0.3373, -0.5552, -0.0482],\n",
      "        [ 1.5023,  0.3111,  0.8613,  ..., -0.5112,  1.0258,  1.2208],\n",
      "        [-0.6539, -0.5276, -2.1693,  ...,  0.2074,  1.8371,  0.1334],\n",
      "        ...,\n",
      "        [-0.4068,  1.0931,  0.8283,  ...,  1.3627,  0.8141,  1.2604],\n",
      "        [ 0.9499,  1.3309,  0.5874,  ...,  0.1706,  0.0704, -0.7519],\n",
      "        [ 1.1520, -0.7768, -0.3936,  ..., -0.5371,  0.5554,  0.5308]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.5798, -0.5658, -2.2484,  ...,  1.3597,  0.8500, -1.4845],\n",
      "        [-0.1948, -1.8166,  1.0431,  ..., -2.2375, -0.1510,  2.1793],\n",
      "        [ 0.2283,  2.2087,  1.5403,  ...,  1.5691,  0.8156, -1.0480],\n",
      "        ...,\n",
      "        [ 0.0928,  0.4066, -1.5282,  ...,  2.0542, -0.9232, -2.7701],\n",
      "        [ 0.4516,  1.1837, -0.0160,  ..., -1.4207, -0.8451,  0.7247],\n",
      "        [ 1.1272,  0.9473,  0.4548,  ...,  0.1526,  0.3051,  0.0989]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 0.6892,  1.2386, -1.8814,  ..., -1.3619, -0.2830,  0.8011],\n",
      "        [ 1.8491,  2.2685, -0.5660,  ...,  1.9191,  2.1643,  1.2276],\n",
      "        [ 1.1262, -0.0382,  0.1385,  ..., -2.2151,  0.4027, -0.6225],\n",
      "        ...,\n",
      "        [-0.1413, -0.3849,  1.1155,  ...,  1.1953, -0.3490,  0.3053],\n",
      "        [ 0.9891, -0.1411, -1.5829,  ...,  0.2248, -0.3471, -0.4941],\n",
      "        [ 1.1022, -0.6306, -0.8928,  ...,  1.4499, -0.9932, -0.3152]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.5777,  2.0481,  0.3899,  ..., -0.4787, -1.6418, -0.3129],\n",
      "        [-0.4083,  1.2202,  0.1185,  ...,  0.1286, -0.7177,  1.9564],\n",
      "        [-1.1190, -0.0915,  0.1521,  ..., -0.4186, -1.8733,  0.0078],\n",
      "        ...,\n",
      "        [-0.2116, -0.1650,  0.1033,  ...,  0.0179, -0.6825,  0.7825],\n",
      "        [-0.6716, -0.2165, -0.6956,  ...,  1.6652, -0.1295, -0.4870],\n",
      "        [ 1.2044,  0.3670,  0.8956,  ...,  2.0530, -1.6118,  1.6607]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[-0.8811, -1.3572, -1.3244,  ...,  0.0570, -1.1360,  0.0338],\n",
      "        [-0.0415, -0.1530,  1.9173,  ..., -2.2037, -0.9415,  1.5065],\n",
      "        [ 1.3558, -1.1626,  0.6332,  ..., -0.4955,  0.3251,  1.0316],\n",
      "        ...,\n",
      "        [ 0.3658,  0.4577,  1.8187,  ..., -1.5604, -0.3077,  0.3403],\n",
      "        [ 0.3309, -0.0572,  0.7452,  ...,  0.8898,  0.6670,  0.2851],\n",
      "        [ 1.0497,  1.4561, -1.3560,  ..., -0.9825,  0.3605,  1.4794]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-0.6166, -0.8551,  0.7025,  ...,  0.7605, -0.9496, -1.7647],\n",
      "        [-0.9134,  1.8765, -1.1330,  ...,  0.7387,  0.0483, -0.7359],\n",
      "        [ 0.6543,  0.3076, -0.6151,  ..., -0.1976, -0.5410,  1.3539],\n",
      "        ...,\n",
      "        [ 0.8693,  0.2712,  0.2567,  ..., -2.2754, -0.1141,  0.6770],\n",
      "        [ 0.3642, -1.3615,  1.0770,  ..., -0.9514,  0.7875, -0.6087],\n",
      "        [-0.6728,  0.9844, -1.2568,  ...,  0.4724, -0.1693, -0.4881]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[-0.5288, -0.3335, -0.4683,  ..., -0.2348, -0.8064,  0.2939],\n",
      "        [-2.2713,  0.3150, -0.5973,  ..., -0.4212, -0.6934,  0.1190],\n",
      "        [-1.8810,  0.1931, -0.2705,  ...,  0.2125,  1.3482,  1.3613],\n",
      "        ...,\n",
      "        [-1.3219,  1.5898, -0.0854,  ..., -2.1841, -2.3650, -0.5381],\n",
      "        [-0.0796, -0.1886,  0.0721,  ..., -0.1565, -1.3504,  1.3895],\n",
      "        [ 0.5015, -0.7079, -1.4246,  ..., -1.0183, -0.1063, -0.6086]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-0.0565, -0.1508,  0.8969,  ...,  0.0661,  0.5572,  0.7117],\n",
      "        [-0.4594,  0.7866, -0.1524,  ...,  0.7439, -1.5361,  0.2020],\n",
      "        [-0.1426,  0.8821,  0.2001,  ...,  0.0333,  0.3443, -1.3381],\n",
      "        ...,\n",
      "        [-0.1558,  0.3591,  0.2668,  ...,  1.0792,  1.2592,  0.1237],\n",
      "        [-1.0392,  0.6429, -1.5563,  ...,  0.7182, -0.3413,  0.6517],\n",
      "        [ 0.9938, -0.3363,  0.1572,  ..., -1.6187,  0.9545, -0.8098]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[-1.2520, -0.0823, -1.5614,  ..., -0.4597, -0.2376, -0.1529],\n",
      "        [-0.1223, -0.5655,  0.1203,  ..., -0.7391,  0.3960,  1.9184],\n",
      "        [-0.4660, -1.0434, -1.7613,  ...,  0.4420, -1.4698, -1.6952],\n",
      "        ...,\n",
      "        [ 1.0118, -0.1796,  0.5201,  ...,  0.3894,  0.2298,  0.3807],\n",
      "        [-0.7043, -1.3378,  0.4179,  ...,  0.3794,  0.9515, -0.0512],\n",
      "        [-1.0058,  1.1914,  0.4077,  ...,  0.1406, -0.8313,  1.2307]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-1.6956, -0.7808,  0.9515,  ...,  1.8125, -0.5248,  0.1395],\n",
      "        [ 0.5070, -1.5465, -2.5148,  ...,  0.7942, -2.1658, -1.2042],\n",
      "        [ 0.7473, -2.1886,  0.3985,  ...,  1.1848, -0.7169,  0.1633],\n",
      "        ...,\n",
      "        [ 0.0089, -1.0412,  0.6928,  ..., -1.8661, -0.2297,  0.2647],\n",
      "        [-0.3086,  1.6053, -1.7666,  ...,  1.1991,  0.2296,  0.2587],\n",
      "        [-1.3557, -0.8132, -0.2069,  ...,  0.4370,  0.6544,  0.7374]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 0.1675, -0.9587,  0.4050,  ..., -0.0632, -2.0083,  0.4432],\n",
      "        [ 0.8835,  0.3596, -2.5948,  ...,  2.0647,  0.7627,  0.5538],\n",
      "        [ 1.2000, -0.1405, -1.6597,  ..., -0.0928, -0.4087,  0.2318],\n",
      "        ...,\n",
      "        [ 0.2373,  0.7002, -0.7744,  ..., -0.7343,  1.5962,  1.1735],\n",
      "        [ 0.4374, -1.2221,  1.0819,  ..., -0.9597,  1.1030,  1.3980],\n",
      "        [-0.4573,  0.2637, -0.6984,  ...,  0.3832,  0.6148,  0.9682]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.3140,  0.1336, -0.1334,  ..., -0.4123,  0.7933, -0.0839],\n",
      "        [ 0.8320,  0.9209, -1.4761,  ..., -1.1776, -1.8907,  0.1350],\n",
      "        [ 0.1926,  0.8914,  0.4417,  ...,  0.7466,  0.8495, -0.1929],\n",
      "        ...,\n",
      "        [ 1.4019,  1.0043, -2.8097,  ...,  0.0774, -1.1247, -0.5114],\n",
      "        [ 1.9823,  1.0819,  1.1281,  ..., -0.5192, -0.7946, -0.5557],\n",
      "        [ 0.9355, -0.8191, -0.9990,  ..., -0.0663,  0.0418,  2.0030]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[-0.2816, -1.7320, -0.1325,  ..., -0.3218,  0.1176, -1.6491],\n",
      "        [ 0.6991,  0.8534, -0.0373,  ..., -0.8651,  0.2013,  2.0762],\n",
      "        [ 0.3983, -0.9383,  0.2975,  ...,  0.5395,  0.0073,  1.8476],\n",
      "        ...,\n",
      "        [ 0.0638, -1.5312, -0.1115,  ..., -0.7430, -0.5971,  2.0150],\n",
      "        [ 0.9528, -0.5375,  2.4082,  ..., -1.0629, -0.8901, -0.7491],\n",
      "        [-0.7066, -0.6897,  0.4135,  ...,  0.0755,  0.5557,  1.7077]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.0945, -1.6645,  2.9408,  ...,  1.1901, -0.1200, -0.2576],\n",
      "        [ 1.2643, -0.1179, -0.5741,  ..., -0.4230, -1.2218,  0.2878],\n",
      "        [-0.9926, -0.1425,  1.5532,  ..., -1.4475,  0.9693, -0.1172],\n",
      "        ...,\n",
      "        [ 0.7929, -1.2368,  1.7058,  ..., -0.5684,  0.1447, -1.3118],\n",
      "        [-0.4061,  0.2377, -0.5065,  ..., -0.4585,  0.3162,  3.2221],\n",
      "        [ 1.6797, -1.1172, -1.2758,  ..., -0.6194, -1.1503, -1.2810]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[-9.6275e-01, -9.8250e-01, -1.3912e+00,  ...,  1.0398e+00,\n",
      "          3.5454e-01, -1.2022e+00],\n",
      "        [-2.0581e-01, -1.8258e+00, -7.9584e-01,  ..., -1.1066e+00,\n",
      "         -9.8705e-01,  1.2319e-01],\n",
      "        [ 8.2827e-02, -6.8405e-01,  1.2482e-01,  ..., -7.6820e-01,\n",
      "         -6.2560e-01, -9.9974e-01],\n",
      "        ...,\n",
      "        [-4.7581e-01,  1.1648e+00, -1.0660e-01,  ...,  1.5811e-02,\n",
      "          2.7113e+00,  1.4037e-01],\n",
      "        [-7.7988e-01,  1.0399e+00, -6.3757e-01,  ..., -1.7271e+00,\n",
      "          5.6224e-01, -6.4343e-01],\n",
      "        [ 1.9647e-03,  6.2921e-01,  3.9968e-02,  ...,  1.7427e-01,\n",
      "          3.0355e-02,  2.1524e+00]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shape: torch.Size([5120, 320])\n",
      "tensor([[ 0.5540,  1.2291, -1.8384,  ...,  0.7845,  0.0420, -1.2628],\n",
      "        [-2.1736,  1.3544,  0.5942,  ...,  0.5489, -0.7382, -0.6659],\n",
      "        [-0.7332, -0.2712, -0.9318,  ..., -0.3767,  0.5702, -0.8902],\n",
      "        ...,\n",
      "        [-0.4761, -0.8771, -0.0441,  ..., -2.0404, -0.8084,  0.0755],\n",
      "        [ 0.3073, -0.9860, -1.9460,  ...,  1.7790, -1.0945,  0.5717],\n",
      "        [-0.0074,  1.3640, -0.1716,  ...,  0.5383,  0.0282,  1.7686]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight, Shape: torch.Size([320, 2560])\n",
      "tensor([[-0.1245, -1.7369,  0.3088,  ...,  0.3769,  0.1377,  0.2456],\n",
      "        [-0.1752, -0.1231,  0.0409,  ...,  0.0922,  0.6384,  0.1132],\n",
      "        [ 0.4346,  0.3951,  1.1942,  ..., -0.8410, -0.2709,  0.5310],\n",
      "        ...,\n",
      "        [-0.1475,  0.6103, -0.9946,  ...,  0.1282, -0.3190,  1.0220],\n",
      "        [ 1.1299,  0.2761, -0.2816,  ..., -0.0863, -0.2713, -1.0777],\n",
      "        [ 0.5387,  0.4861,  0.1066,  ...,  0.6664, -1.3997,  0.5779]])\n",
      "Layer: unet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 1.2667,  0.7688,  0.8050,  ..., -1.2551,  0.8095,  0.5330],\n",
      "        [ 0.3155,  1.9780, -0.6718,  ..., -0.4250,  0.5932,  1.2292],\n",
      "        [-0.6481,  0.0311, -0.7996,  ...,  1.7430, -1.5502,  0.9451],\n",
      "        ...,\n",
      "        [-0.1033, -0.1114, -0.4444,  ..., -1.0698,  0.0794, -0.2712],\n",
      "        [-0.0827, -0.1686, -0.0158,  ...,  0.0744,  0.5002, -0.4594],\n",
      "        [ 0.2861, -0.8765,  1.1621,  ...,  0.2796,  1.5182, -0.5753]])\n",
      "Layer: unet.down_blocks.1.attentions.1.proj_in.lora.down.weight, Shape: torch.Size([320, 640, 1, 1])\n",
      "tensor([[[[-0.4838]],\n",
      "\n",
      "         [[-1.1403]],\n",
      "\n",
      "         [[-0.8110]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.2137]],\n",
      "\n",
      "         [[ 0.0693]],\n",
      "\n",
      "         [[ 0.8194]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1209]],\n",
      "\n",
      "         [[-1.4996]],\n",
      "\n",
      "         [[ 0.3262]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7135]],\n",
      "\n",
      "         [[-0.2215]],\n",
      "\n",
      "         [[ 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[-0.8517]],\n",
      "\n",
      "         [[ 2.1947]],\n",
      "\n",
      "         [[ 1.0374]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.2935]],\n",
      "\n",
      "         [[-0.4087]],\n",
      "\n",
      "         [[-0.1040]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.3860]],\n",
      "\n",
      "         [[ 2.6574]],\n",
      "\n",
      "         [[ 0.4254]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0428]],\n",
      "\n",
      "         [[-0.1422]],\n",
      "\n",
      "         [[-0.5533]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0190]],\n",
      "\n",
      "         [[ 0.0659]],\n",
      "\n",
      "         [[-0.2808]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5334]],\n",
      "\n",
      "         [[-1.3120]],\n",
      "\n",
      "         [[ 0.9979]]],\n",
      "\n",
      "\n",
      "        [[[-0.5487]],\n",
      "\n",
      "         [[ 0.2789]],\n",
      "\n",
      "         [[ 0.1662]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8851]],\n",
      "\n",
      "         [[-2.3047]],\n",
      "\n",
      "         [[-0.3836]]]])\n",
      "Layer: unet.down_blocks.1.attentions.1.proj_in.lora.up.weight, Shape: torch.Size([640, 320, 1, 1])\n",
      "tensor([[[[-0.8140]],\n",
      "\n",
      "         [[ 1.0079]],\n",
      "\n",
      "         [[-0.3378]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7668]],\n",
      "\n",
      "         [[-0.1742]],\n",
      "\n",
      "         [[-2.2054]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9470]],\n",
      "\n",
      "         [[-1.6844]],\n",
      "\n",
      "         [[-0.5537]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9036]],\n",
      "\n",
      "         [[ 0.4663]],\n",
      "\n",
      "         [[-0.8097]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2932]],\n",
      "\n",
      "         [[ 1.6679]],\n",
      "\n",
      "         [[ 1.1218]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4178]],\n",
      "\n",
      "         [[-0.7526]],\n",
      "\n",
      "         [[ 0.2743]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.6452]],\n",
      "\n",
      "         [[-2.3749]],\n",
      "\n",
      "         [[ 1.0054]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7598]],\n",
      "\n",
      "         [[-1.3398]],\n",
      "\n",
      "         [[-0.6586]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4425]],\n",
      "\n",
      "         [[-1.3440]],\n",
      "\n",
      "         [[ 0.0564]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4787]],\n",
      "\n",
      "         [[-1.5713]],\n",
      "\n",
      "         [[ 0.2814]]],\n",
      "\n",
      "\n",
      "        [[[-1.4684]],\n",
      "\n",
      "         [[-0.1717]],\n",
      "\n",
      "         [[-0.6233]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.6116]],\n",
      "\n",
      "         [[-1.1415]],\n",
      "\n",
      "         [[-0.1178]]]])\n",
      "Layer: unet.down_blocks.1.attentions.1.proj_out.lora.down.weight, Shape: torch.Size([320, 640, 1, 1])\n",
      "tensor([[[[ 0.8904]],\n",
      "\n",
      "         [[-0.8426]],\n",
      "\n",
      "         [[-0.5798]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0222]],\n",
      "\n",
      "         [[-0.4030]],\n",
      "\n",
      "         [[ 0.3673]]],\n",
      "\n",
      "\n",
      "        [[[ 2.7665]],\n",
      "\n",
      "         [[-0.6240]],\n",
      "\n",
      "         [[-0.4891]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6357]],\n",
      "\n",
      "         [[ 0.0103]],\n",
      "\n",
      "         [[ 0.5550]]],\n",
      "\n",
      "\n",
      "        [[[-0.5873]],\n",
      "\n",
      "         [[ 0.6631]],\n",
      "\n",
      "         [[ 1.3595]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.1396]],\n",
      "\n",
      "         [[ 0.9575]],\n",
      "\n",
      "         [[-0.5797]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.1546]],\n",
      "\n",
      "         [[ 0.2724]],\n",
      "\n",
      "         [[ 0.4726]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0921]],\n",
      "\n",
      "         [[ 0.0284]],\n",
      "\n",
      "         [[-0.5206]]],\n",
      "\n",
      "\n",
      "        [[[-0.8425]],\n",
      "\n",
      "         [[-1.0343]],\n",
      "\n",
      "         [[-0.2920]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0588]],\n",
      "\n",
      "         [[ 1.1037]],\n",
      "\n",
      "         [[ 0.2650]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5826]],\n",
      "\n",
      "         [[ 1.5631]],\n",
      "\n",
      "         [[-0.5201]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2725]],\n",
      "\n",
      "         [[ 0.6116]],\n",
      "\n",
      "         [[ 0.8657]]]])\n",
      "Layer: unet.down_blocks.1.attentions.1.proj_out.lora.up.weight, Shape: torch.Size([640, 320, 1, 1])\n",
      "tensor([[[[-1.6465]],\n",
      "\n",
      "         [[-1.1339]],\n",
      "\n",
      "         [[-1.0264]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2443]],\n",
      "\n",
      "         [[ 0.6996]],\n",
      "\n",
      "         [[ 0.6619]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8334]],\n",
      "\n",
      "         [[-2.4450]],\n",
      "\n",
      "         [[-4.1786]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0699]],\n",
      "\n",
      "         [[ 0.0437]],\n",
      "\n",
      "         [[ 0.6295]]],\n",
      "\n",
      "\n",
      "        [[[-1.2472]],\n",
      "\n",
      "         [[-0.4834]],\n",
      "\n",
      "         [[ 1.0151]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0290]],\n",
      "\n",
      "         [[ 0.4194]],\n",
      "\n",
      "         [[ 0.5435]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0346]],\n",
      "\n",
      "         [[-0.8788]],\n",
      "\n",
      "         [[ 0.2255]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1811]],\n",
      "\n",
      "         [[-0.4391]],\n",
      "\n",
      "         [[ 1.7854]]],\n",
      "\n",
      "\n",
      "        [[[-0.4220]],\n",
      "\n",
      "         [[ 0.5004]],\n",
      "\n",
      "         [[-0.6284]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1722]],\n",
      "\n",
      "         [[-1.5089]],\n",
      "\n",
      "         [[ 0.2316]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2002]],\n",
      "\n",
      "         [[ 2.4202]],\n",
      "\n",
      "         [[ 1.4516]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2261]],\n",
      "\n",
      "         [[-0.8972]],\n",
      "\n",
      "         [[ 0.5603]]]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 0.6083, -0.1030, -0.9247,  ..., -0.7286,  1.0222, -0.8321],\n",
      "        [ 0.5156, -1.2737, -0.0490,  ...,  0.8335,  0.6859, -2.1489],\n",
      "        [ 0.1202, -0.4841,  0.1461,  ...,  1.1778, -0.0117,  1.8098],\n",
      "        ...,\n",
      "        [-0.5992,  2.5519, -0.8302,  ..., -0.5876, -0.6706,  0.6268],\n",
      "        [-0.6573, -0.5528, -0.3583,  ...,  0.5565, -1.8640, -0.7974],\n",
      "        [-0.3454, -1.0657,  0.2526,  ..., -1.0989,  1.3772, -0.6517]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.0376, -0.1850, -0.4272,  ...,  0.1497,  0.8761, -0.4765],\n",
      "        [ 1.6312, -0.6176,  0.4584,  ..., -1.3552, -0.3354, -0.0410],\n",
      "        [-0.1981,  0.0270, -0.6243,  ...,  0.5115,  0.8032, -1.1268],\n",
      "        ...,\n",
      "        [ 0.8159, -0.9811,  0.4617,  ..., -0.2691,  0.3904,  0.3137],\n",
      "        [ 0.7868,  0.4598, -1.7117,  ...,  1.4325,  0.9060,  0.0553],\n",
      "        [ 1.3083, -0.2723, -1.3206,  ..., -0.2798, -0.1091, -2.0764]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 0.2692, -0.1378,  1.4304,  ..., -0.4087, -0.5860,  0.6786],\n",
      "        [-0.5211, -0.2096,  0.3461,  ...,  1.2378, -0.3782, -2.1807],\n",
      "        [ 0.1459, -1.2703, -0.1580,  ...,  0.9288,  0.0514,  0.9237],\n",
      "        ...,\n",
      "        [-1.8692,  0.0031, -0.2721,  ..., -1.0335,  0.0587, -1.5261],\n",
      "        [ 0.6791, -0.7819,  1.3635,  ..., -0.3140,  0.0893, -0.5795],\n",
      "        [ 1.5824, -1.7242, -0.0483,  ..., -0.9396,  1.6057,  0.3939]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.7642,  0.7329,  0.1435,  ...,  0.5969, -0.6693, -0.0795],\n",
      "        [ 0.5618, -1.7246,  0.0308,  ..., -0.1197,  0.2815, -0.9977],\n",
      "        [-1.4180, -0.2150, -0.3828,  ..., -0.3360,  0.2328,  1.7581],\n",
      "        ...,\n",
      "        [-2.6805,  0.5356, -1.4130,  ...,  1.3784,  0.8940, -0.2723],\n",
      "        [-0.6944, -2.1774,  1.0905,  ...,  0.7260, -0.3412, -0.0385],\n",
      "        [-0.7404,  0.1446, -0.6487,  ...,  1.2001,  1.1702, -0.1184]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[-0.2311, -2.6368,  0.3934,  ..., -0.8047,  0.0156, -0.9074],\n",
      "        [ 1.4581,  1.3341, -1.2851,  ...,  0.1804,  0.8696,  1.6021],\n",
      "        [-0.9584, -0.1797, -0.3466,  ...,  0.7943, -0.9071, -2.4905],\n",
      "        ...,\n",
      "        [ 3.0901,  1.5382, -2.4063,  ..., -0.6725, -0.2299,  1.1952],\n",
      "        [ 0.8795,  1.6571, -1.5447,  ...,  1.1056, -0.5593, -0.6555],\n",
      "        [ 0.8603, -1.3710,  1.2087,  ..., -0.4876,  0.9255, -0.9408]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.9247,  0.8184,  0.3878,  ...,  0.5488,  0.0596,  0.8801],\n",
      "        [-1.6932, -0.1040, -1.3329,  ...,  0.2121,  0.2742,  1.6211],\n",
      "        [-0.5474,  1.0134,  1.5312,  ..., -0.2791, -0.3132,  1.0814],\n",
      "        ...,\n",
      "        [ 0.2876, -0.6754,  0.3522,  ...,  2.0578, -1.2770,  0.0355],\n",
      "        [ 1.5225,  0.1495,  2.8269,  ..., -1.5577,  1.8736,  0.5557],\n",
      "        [ 0.7709,  0.4694,  2.3463,  ...,  0.2727, -0.8673, -2.4876]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[-0.3785,  0.6446, -1.1121,  ...,  0.1623, -0.9007,  0.1692],\n",
      "        [-1.5719, -0.4708,  1.9523,  ...,  1.0369,  0.3112, -0.1851],\n",
      "        [ 0.3272,  0.8158,  0.8392,  ..., -1.5053,  2.1023, -1.3438],\n",
      "        ...,\n",
      "        [ 0.6889, -1.5812, -0.0160,  ..., -2.6209, -1.9561,  2.6268],\n",
      "        [ 1.2332,  0.4331, -1.3401,  ...,  1.2860,  1.5028, -0.4261],\n",
      "        [ 2.1331,  1.2081, -2.1112,  ..., -1.3247,  1.0319,  0.7195]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-0.2375,  1.4536,  0.4311,  ..., -0.8027, -1.5563,  1.3792],\n",
      "        [ 1.0355,  1.2464, -0.9594,  ...,  0.1351, -0.1291, -1.8810],\n",
      "        [ 0.1775,  0.4326,  0.7621,  ..., -0.2211, -0.0323, -1.6610],\n",
      "        ...,\n",
      "        [ 0.8477, -1.0575, -0.3525,  ...,  0.7846,  0.0023,  0.5800],\n",
      "        [ 0.2142, -0.4520, -0.2112,  ..., -1.9621, -0.1744, -1.4279],\n",
      "        [ 0.4362, -1.3438,  0.7277,  ...,  0.9887,  0.1741,  0.5323]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[-0.7325,  0.4186, -0.7719,  ..., -0.3036, -0.8109, -0.4180],\n",
      "        [ 0.4041, -0.0462,  0.4069,  ..., -0.5825,  0.8729, -1.2931],\n",
      "        [ 1.4625,  0.4293, -1.2297,  ...,  0.5943, -1.3744, -0.5455],\n",
      "        ...,\n",
      "        [-0.7253, -0.2457,  1.5296,  ...,  0.1534,  1.4812, -0.3456],\n",
      "        [-0.3457, -0.3655, -0.0609,  ...,  0.3901,  0.1274,  0.9347],\n",
      "        [ 0.0085,  0.6506,  0.3050,  ...,  0.1446,  0.2864, -0.3347]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-0.4120, -0.9883,  1.0035,  ..., -0.0662,  1.0518,  1.6125],\n",
      "        [ 0.2407,  0.0681, -1.6272,  ...,  2.4013,  0.4553, -0.1826],\n",
      "        [ 0.3810,  1.1486, -0.4794,  ...,  0.6061,  0.7109, -0.0417],\n",
      "        ...,\n",
      "        [-0.6648, -0.4109,  2.0436,  ...,  1.7599,  0.8511,  0.9209],\n",
      "        [-0.0822, -0.8255, -1.3766,  ..., -1.1439, -1.2213,  0.8372],\n",
      "        [ 0.3358, -0.6837, -1.9424,  ..., -0.7311, -0.3508,  2.6970]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[-5.8064e-01, -8.4482e-01,  1.3959e+00,  ..., -2.8081e-01,\n",
      "         -8.8372e-01, -4.7937e-04],\n",
      "        [-1.0576e+00,  1.6945e-01, -7.4654e-01,  ..., -1.7062e+00,\n",
      "          1.6594e+00, -3.1581e-03],\n",
      "        [-1.7247e-01, -8.3723e-01, -1.2272e+00,  ...,  7.5852e-01,\n",
      "          1.4161e-01,  1.7582e+00],\n",
      "        ...,\n",
      "        [ 4.4320e-01, -3.4613e-01, -8.4346e-01,  ...,  7.9957e-01,\n",
      "          6.7748e-01,  4.9019e-03],\n",
      "        [-1.0995e+00, -2.2886e-01, -1.7067e+00,  ..., -1.0036e+00,\n",
      "         -6.5871e-01,  3.7134e-01],\n",
      "        [ 6.1774e-01,  6.0556e-01,  7.1180e-01,  ...,  2.0979e+00,\n",
      "         -1.5740e-01,  7.2802e-01]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-0.1076, -1.9733, -0.5929,  ..., -0.4785, -1.3055,  0.8060],\n",
      "        [ 0.3450, -0.1001,  0.0095,  ...,  0.4637,  1.0048,  0.7340],\n",
      "        [ 0.0728,  0.3181,  0.8795,  ...,  0.6980,  0.5297, -0.0359],\n",
      "        ...,\n",
      "        [-0.1208, -0.2364,  1.5479,  ..., -0.5627, -0.3190,  0.7613],\n",
      "        [-0.3405, -1.0896,  0.6507,  ...,  0.8291,  1.3995, -0.9667],\n",
      "        [-0.3595,  1.0872,  0.0647,  ...,  1.4715, -0.2553, -0.7610]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 1.7556e-01, -4.1313e-01,  2.6006e-02,  ..., -1.3893e+00,\n",
      "          3.7389e-01,  1.6633e-01],\n",
      "        [-7.9284e-02, -4.6565e-02,  4.1945e-01,  ..., -1.4626e-02,\n",
      "          4.6485e-01, -1.4200e+00],\n",
      "        [ 1.0335e-01, -3.0773e-01, -3.7951e-01,  ..., -2.0228e-01,\n",
      "          5.5525e-01, -9.7658e-01],\n",
      "        ...,\n",
      "        [-5.4944e-01, -1.3923e-02,  3.2048e-01,  ..., -7.0380e-01,\n",
      "          2.0724e+00, -9.6683e-01],\n",
      "        [-4.6653e-01, -1.2282e+00,  1.4968e+00,  ..., -1.6773e+00,\n",
      "         -1.6322e+00, -7.2673e-01],\n",
      "        [-1.5591e+00, -5.8829e-01, -1.8299e-02,  ..., -4.7070e-04,\n",
      "         -2.6508e-01, -9.0438e-02]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.0692,  0.0708,  0.1081,  ..., -0.4964,  1.0891, -0.8798],\n",
      "        [ 1.0094, -0.5002, -0.3241,  ..., -1.7705,  1.3164,  0.0683],\n",
      "        [ 1.5151, -0.5971, -0.2227,  ...,  1.0002,  0.3876, -0.2468],\n",
      "        ...,\n",
      "        [ 0.4759,  0.3155,  0.5264,  ...,  2.9218, -1.0116,  0.8374],\n",
      "        [-2.6823, -0.7977,  0.6376,  ...,  1.1037, -0.8080, -1.6045],\n",
      "        [-0.0592,  0.1111, -0.7723,  ..., -0.2955, -2.1076, -0.6079]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 1.2407e-01, -2.0499e+00,  1.8505e-01,  ..., -2.1176e-04,\n",
      "         -1.1997e+00,  1.2502e+00],\n",
      "        [ 6.9882e-01,  2.7573e-01, -2.0633e+00,  ..., -1.1701e+00,\n",
      "         -2.1984e-02, -2.9218e-01],\n",
      "        [-3.8697e-01,  1.1010e+00,  4.6394e-01,  ...,  4.7678e-01,\n",
      "          1.3261e+00, -4.1615e-01],\n",
      "        ...,\n",
      "        [ 5.5132e-01,  6.0527e-01, -6.5185e-01,  ...,  7.4149e-02,\n",
      "          4.4022e-01, -1.1298e+00],\n",
      "        [ 5.5846e-01,  7.1086e-01, -1.5759e+00,  ...,  8.0372e-01,\n",
      "         -4.9051e-01,  2.9556e-01],\n",
      "        [ 7.6664e-01, -2.6459e-01,  1.3612e+00,  ...,  8.1197e-01,\n",
      "          7.6562e-01, -5.2546e-01]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.9769, -1.2083, -0.5679,  ...,  0.9480,  0.7168,  0.8184],\n",
      "        [-0.4530,  1.1919,  0.1992,  ..., -0.1139,  0.3971,  3.0323],\n",
      "        [-0.8042, -0.0194,  0.1291,  ...,  2.0325, -0.1671, -0.4721],\n",
      "        ...,\n",
      "        [ 0.3924, -0.0147, -0.5143,  ...,  0.4478, -0.3022,  0.8415],\n",
      "        [-0.4632,  1.0687,  1.7890,  ..., -0.3190, -0.2149, -0.2722],\n",
      "        [-1.1447,  1.8489, -0.5315,  ..., -2.2834, -0.6857,  0.3845]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[-1.1268, -1.4208,  0.2695,  ..., -1.3804,  1.3645, -0.3518],\n",
      "        [-1.2634,  1.3970,  1.4962,  ..., -0.2673, -1.0364,  0.4626],\n",
      "        [ 0.1140, -1.3489, -0.9528,  ..., -0.4267, -0.2114, -0.1778],\n",
      "        ...,\n",
      "        [ 2.3138, -1.1293,  0.6739,  ...,  1.2552,  0.3781, -1.7673],\n",
      "        [ 1.1069,  0.9216, -1.1140,  ..., -0.8813,  0.0535, -0.1665],\n",
      "        [ 0.2227, -1.0030, -0.0502,  ...,  0.4314,  0.5913,  1.1033]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shape: torch.Size([5120, 320])\n",
      "tensor([[-0.2980, -2.4415, -0.6419,  ..., -0.2140, -1.2943, -0.4470],\n",
      "        [ 0.0275,  0.0910, -0.9875,  ...,  1.1759, -1.1251,  1.0364],\n",
      "        [ 0.3402, -0.0623, -0.2396,  ..., -0.8831, -0.8362,  0.7370],\n",
      "        ...,\n",
      "        [-0.5366, -0.6930,  0.8254,  ...,  0.9999,  0.2093,  1.5362],\n",
      "        [-1.4314,  0.1060, -0.6952,  ...,  0.2955,  1.9231, -0.5458],\n",
      "        [-0.3361, -1.4595,  0.5765,  ..., -1.0482, -1.4810,  0.3795]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight, Shape: torch.Size([320, 2560])\n",
      "tensor([[-1.1587,  0.2943,  0.0589,  ...,  1.0435,  0.9468,  1.5186],\n",
      "        [-0.1042, -0.4468, -0.5294,  ...,  0.1266, -1.0868,  0.2736],\n",
      "        [-1.0349,  0.2401,  0.3823,  ..., -0.1613, -1.0661, -1.1514],\n",
      "        ...,\n",
      "        [-0.6139, -1.3048,  0.5752,  ...,  0.4336,  1.9775, -0.5118],\n",
      "        [-0.9000,  0.7698,  0.6921,  ..., -2.3292, -1.4643,  0.6191],\n",
      "        [-1.1861, -0.3474, -1.0372,  ..., -0.5427, -0.7867,  0.3782]])\n",
      "Layer: unet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.3023,  0.8350, -0.7273,  ...,  0.0481, -1.1532, -0.5483],\n",
      "        [ 0.6578,  0.0382, -1.0649,  ..., -0.3903, -0.7589, -0.0966],\n",
      "        [ 0.1467, -0.0637,  1.8501,  ...,  0.7902,  0.9166,  0.1112],\n",
      "        ...,\n",
      "        [-0.4874,  0.3003, -0.4993,  ...,  1.4807, -0.2321,  1.0864],\n",
      "        [-1.0941,  0.3982,  1.1215,  ...,  0.3374, -1.6570, -1.9215],\n",
      "        [-0.1712,  0.2113,  1.8472,  ...,  1.0350, -0.4576,  1.5034]])\n",
      "Layer: unet.down_blocks.2.attentions.0.proj_in.lora.down.weight, Shape: torch.Size([320, 1280, 1, 1])\n",
      "tensor([[[[ 0.3003]],\n",
      "\n",
      "         [[-0.7191]],\n",
      "\n",
      "         [[ 1.0597]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2947]],\n",
      "\n",
      "         [[-1.1151]],\n",
      "\n",
      "         [[-0.2985]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4766]],\n",
      "\n",
      "         [[ 0.0333]],\n",
      "\n",
      "         [[-0.0475]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3984]],\n",
      "\n",
      "         [[-0.1605]],\n",
      "\n",
      "         [[-0.1032]]],\n",
      "\n",
      "\n",
      "        [[[-0.8213]],\n",
      "\n",
      "         [[ 0.7583]],\n",
      "\n",
      "         [[ 0.8648]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5733]],\n",
      "\n",
      "         [[ 0.3661]],\n",
      "\n",
      "         [[ 0.3505]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.0369]],\n",
      "\n",
      "         [[ 0.9528]],\n",
      "\n",
      "         [[ 2.1482]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1535]],\n",
      "\n",
      "         [[-0.6012]],\n",
      "\n",
      "         [[-0.1392]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8938]],\n",
      "\n",
      "         [[ 0.7292]],\n",
      "\n",
      "         [[ 0.9011]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2848]],\n",
      "\n",
      "         [[ 0.1607]],\n",
      "\n",
      "         [[-0.0696]]],\n",
      "\n",
      "\n",
      "        [[[-0.3696]],\n",
      "\n",
      "         [[-1.0227]],\n",
      "\n",
      "         [[-1.1378]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0100]],\n",
      "\n",
      "         [[ 0.4071]],\n",
      "\n",
      "         [[ 0.1837]]]])\n",
      "Layer: unet.down_blocks.2.attentions.0.proj_in.lora.up.weight, Shape: torch.Size([1280, 320, 1, 1])\n",
      "tensor([[[[ 1.7994]],\n",
      "\n",
      "         [[ 0.0452]],\n",
      "\n",
      "         [[ 0.0244]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2299]],\n",
      "\n",
      "         [[-0.0488]],\n",
      "\n",
      "         [[ 0.5946]]],\n",
      "\n",
      "\n",
      "        [[[-1.2724]],\n",
      "\n",
      "         [[-1.1772]],\n",
      "\n",
      "         [[ 1.1685]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0429]],\n",
      "\n",
      "         [[ 0.0993]],\n",
      "\n",
      "         [[-1.7312]]],\n",
      "\n",
      "\n",
      "        [[[-0.3569]],\n",
      "\n",
      "         [[-1.9772]],\n",
      "\n",
      "         [[-0.2605]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8086]],\n",
      "\n",
      "         [[ 1.6549]],\n",
      "\n",
      "         [[ 1.5952]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.9718]],\n",
      "\n",
      "         [[ 0.9991]],\n",
      "\n",
      "         [[ 0.9962]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2795]],\n",
      "\n",
      "         [[ 0.5279]],\n",
      "\n",
      "         [[ 1.5295]]],\n",
      "\n",
      "\n",
      "        [[[-0.3875]],\n",
      "\n",
      "         [[ 0.9472]],\n",
      "\n",
      "         [[-0.1244]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5076]],\n",
      "\n",
      "         [[ 0.0364]],\n",
      "\n",
      "         [[-0.1291]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8459]],\n",
      "\n",
      "         [[ 0.0491]],\n",
      "\n",
      "         [[-0.3748]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4653]],\n",
      "\n",
      "         [[-1.6173]],\n",
      "\n",
      "         [[-0.4452]]]])\n",
      "Layer: unet.down_blocks.2.attentions.0.proj_out.lora.down.weight, Shape: torch.Size([320, 1280, 1, 1])\n",
      "tensor([[[[-0.5009]],\n",
      "\n",
      "         [[-0.0233]],\n",
      "\n",
      "         [[ 1.0096]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0668]],\n",
      "\n",
      "         [[-1.4593]],\n",
      "\n",
      "         [[-0.0637]]],\n",
      "\n",
      "\n",
      "        [[[-0.8326]],\n",
      "\n",
      "         [[ 0.4437]],\n",
      "\n",
      "         [[-0.0914]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2859]],\n",
      "\n",
      "         [[ 0.3215]],\n",
      "\n",
      "         [[ 1.5724]]],\n",
      "\n",
      "\n",
      "        [[[-1.0945]],\n",
      "\n",
      "         [[ 1.1443]],\n",
      "\n",
      "         [[-0.1107]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6264]],\n",
      "\n",
      "         [[ 0.0441]],\n",
      "\n",
      "         [[ 1.6167]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.1340]],\n",
      "\n",
      "         [[ 1.1787]],\n",
      "\n",
      "         [[ 1.0978]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3721]],\n",
      "\n",
      "         [[ 0.4365]],\n",
      "\n",
      "         [[ 1.5016]]],\n",
      "\n",
      "\n",
      "        [[[-0.5978]],\n",
      "\n",
      "         [[ 1.7396]],\n",
      "\n",
      "         [[-0.9241]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2371]],\n",
      "\n",
      "         [[ 2.8266]],\n",
      "\n",
      "         [[ 0.3914]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0263]],\n",
      "\n",
      "         [[ 1.4713]],\n",
      "\n",
      "         [[-0.1151]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7209]],\n",
      "\n",
      "         [[-0.1495]],\n",
      "\n",
      "         [[ 0.2863]]]])\n",
      "Layer: unet.down_blocks.2.attentions.0.proj_out.lora.up.weight, Shape: torch.Size([1280, 320, 1, 1])\n",
      "tensor([[[[ 0.1918]],\n",
      "\n",
      "         [[-0.8666]],\n",
      "\n",
      "         [[ 1.2504]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0992]],\n",
      "\n",
      "         [[ 0.6239]],\n",
      "\n",
      "         [[ 1.4980]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3781]],\n",
      "\n",
      "         [[-0.5208]],\n",
      "\n",
      "         [[-2.2097]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1813]],\n",
      "\n",
      "         [[ 1.0484]],\n",
      "\n",
      "         [[ 0.8746]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0326]],\n",
      "\n",
      "         [[-1.1402]],\n",
      "\n",
      "         [[ 0.8693]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0384]],\n",
      "\n",
      "         [[-0.4784]],\n",
      "\n",
      "         [[-0.0036]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.2138]],\n",
      "\n",
      "         [[ 0.1443]],\n",
      "\n",
      "         [[-1.2884]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7734]],\n",
      "\n",
      "         [[ 0.2267]],\n",
      "\n",
      "         [[ 0.5802]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5556]],\n",
      "\n",
      "         [[ 0.1507]],\n",
      "\n",
      "         [[ 1.4509]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2253]],\n",
      "\n",
      "         [[-0.9362]],\n",
      "\n",
      "         [[ 0.1501]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5279]],\n",
      "\n",
      "         [[-0.8931]],\n",
      "\n",
      "         [[ 1.3483]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5473]],\n",
      "\n",
      "         [[-1.7193]],\n",
      "\n",
      "         [[ 0.2272]]]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.7753, -1.2171,  0.3998,  ..., -1.4700, -0.0451, -0.4546],\n",
      "        [ 0.9241, -0.1770,  0.5626,  ..., -1.4794, -0.1885,  0.1251],\n",
      "        [ 0.5150,  0.6281,  0.3042,  ...,  0.3037,  0.0945, -0.3114],\n",
      "        ...,\n",
      "        [-0.8496, -1.8056, -0.9562,  ...,  0.6656, -0.4388, -0.1147],\n",
      "        [-2.0458,  1.9160, -0.7292,  ..., -0.1499,  0.1143, -1.8629],\n",
      "        [ 1.9363, -0.0207,  1.2819,  ..., -1.8333, -1.3145,  1.1903]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 0.3475,  0.1962,  0.5954,  ..., -0.9707,  1.4021,  0.8754],\n",
      "        [ 0.4448, -0.8971, -1.1576,  ..., -0.5096,  1.3116, -0.2496],\n",
      "        [ 0.6006, -0.3912, -1.0096,  ..., -0.0956, -0.2611,  0.4537],\n",
      "        ...,\n",
      "        [-1.2862, -0.6494, -0.0881,  ..., -0.0621,  0.0248, -0.3859],\n",
      "        [-1.0063, -0.6786, -0.2830,  ...,  1.0433,  1.3696, -0.7517],\n",
      "        [-0.3565, -0.1559,  0.5048,  ..., -0.1072,  0.9672, -0.2643]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-1.0862,  0.9499, -1.5677,  ...,  0.3231,  1.4669, -1.6257],\n",
      "        [-1.1282,  1.0258, -0.3518,  ..., -0.5770, -1.1070, -2.1282],\n",
      "        [-0.9956, -0.2061, -1.6838,  ...,  0.2150,  1.1814,  0.9146],\n",
      "        ...,\n",
      "        [-1.1676, -0.6064, -0.4543,  ...,  0.0950,  0.1586, -0.2406],\n",
      "        [ 1.6781, -1.9979, -1.7960,  ...,  0.6223, -2.0623,  2.4035],\n",
      "        [ 0.2533,  0.0653, -0.1245,  ..., -0.5380, -0.4523,  0.6231]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-1.0077, -0.7368, -1.7331,  ..., -0.5829,  0.0745,  1.9716],\n",
      "        [-0.7644,  0.0306,  0.3377,  ...,  0.4724,  0.8835,  0.4664],\n",
      "        [-0.9310, -0.3709, -0.4124,  ..., -1.4061, -0.7712, -0.6240],\n",
      "        ...,\n",
      "        [ 1.7849,  1.2390,  2.8921,  ..., -0.1888,  0.1241,  1.1171],\n",
      "        [-0.1843,  0.0197, -1.5144,  ...,  0.5211,  1.2547, -0.4805],\n",
      "        [ 0.2781, -0.9126, -1.5777,  ...,  1.6594, -1.1916,  1.0814]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[ 0.5926,  0.0813, -0.2582,  ..., -0.4677, -0.8521,  0.6220],\n",
      "        [-0.2896,  1.0370, -0.0078,  ..., -0.6133,  0.6135,  0.4980],\n",
      "        [-0.0058,  1.3462,  0.2107,  ..., -0.1226, -0.1196,  0.0641],\n",
      "        ...,\n",
      "        [-0.4212,  0.4903, -1.0142,  ..., -0.8923,  0.7619,  0.2705],\n",
      "        [-1.1969, -1.2994,  0.9251,  ..., -0.1095, -0.5726,  0.8960],\n",
      "        [-1.7655, -0.2244, -0.6587,  ...,  1.2837, -0.3241, -0.4439]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-0.0233,  0.6835,  0.2878,  ..., -0.5979,  1.1876,  1.0931],\n",
      "        [-1.3018,  0.9747, -1.4309,  ...,  0.9365,  0.3382, -0.0979],\n",
      "        [ 0.5063, -0.6272,  0.3294,  ..., -2.1133, -0.3128, -0.4387],\n",
      "        ...,\n",
      "        [-0.4734, -0.2982, -0.9231,  ..., -0.2215,  0.3877,  0.0207],\n",
      "        [ 0.4157, -0.3561,  0.7251,  ...,  0.0300,  0.1334, -0.2643],\n",
      "        [-0.7888, -0.4012, -0.0771,  ...,  0.3182,  0.8723, -0.2943]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.4918, -1.6013, -1.2512,  ..., -1.0220, -1.4323, -0.7664],\n",
      "        [ 0.8944,  0.8802,  0.7043,  ...,  1.0388,  0.8586,  0.2738],\n",
      "        [ 0.9721,  1.3542, -0.3715,  ..., -0.3788, -1.2395, -0.4528],\n",
      "        ...,\n",
      "        [-1.3855,  0.2637, -0.4830,  ...,  0.3459, -0.4011, -0.2079],\n",
      "        [ 0.0211,  0.4673, -1.4962,  ..., -1.2533,  1.7063,  2.1238],\n",
      "        [ 1.9512, -0.7199,  0.5334,  ..., -1.4952,  1.2711, -1.1632]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 2.3154, -1.0418,  0.4297,  ..., -0.6767, -0.4244,  1.7556],\n",
      "        [-1.6701,  0.2463, -0.3936,  ..., -2.0956,  0.1471,  0.2604],\n",
      "        [-1.6228, -1.4491,  0.4045,  ..., -0.8950,  0.0395, -1.3986],\n",
      "        ...,\n",
      "        [ 1.5015, -0.6624, -0.3808,  ..., -1.1620,  2.7595, -1.7280],\n",
      "        [-1.2815, -0.0532, -1.3957,  ..., -0.0480,  0.9667, -1.0983],\n",
      "        [-0.1869, -1.6017,  0.6009,  ..., -0.4006, -0.6592,  1.2694]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 1.0859e+00, -9.7761e-01,  2.5228e+00,  ..., -2.6023e-01,\n",
      "          9.6941e-01,  3.1056e-01],\n",
      "        [-3.1270e-01,  2.9951e-01, -9.7568e-02,  ..., -4.3385e-02,\n",
      "          1.6152e+00,  5.3702e-01],\n",
      "        [-7.4380e-02, -6.9831e-01,  2.9224e-01,  ..., -1.6641e-01,\n",
      "         -9.8269e-01, -6.9274e-01],\n",
      "        ...,\n",
      "        [ 1.4441e+00,  6.6796e-02, -5.4582e-02,  ..., -9.1006e-01,\n",
      "         -9.8719e-01,  2.6100e+00],\n",
      "        [ 1.8996e-01, -1.3826e+00, -1.6341e+00,  ..., -9.0487e-01,\n",
      "          4.2016e-01,  1.2888e+00],\n",
      "        [ 9.5886e-04,  3.7594e-01, -1.3989e+00,  ..., -1.2182e+00,\n",
      "          5.9416e-01,  6.8314e-01]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-0.8403, -0.5007, -0.0290,  ..., -1.8189, -1.6873,  1.7199],\n",
      "        [-0.2760,  0.0987, -0.4498,  ..., -1.6496,  1.0587,  1.7973],\n",
      "        [ 1.6680, -0.7269, -1.0235,  ..., -0.0949,  0.8410,  0.1935],\n",
      "        ...,\n",
      "        [ 0.1558, -0.1350, -0.8710,  ...,  0.9595, -0.3905, -0.6123],\n",
      "        [ 0.2054,  0.4252,  0.5595,  ..., -1.4928, -0.5856,  0.0743],\n",
      "        [-0.0563, -0.1903, -0.3784,  ...,  0.3544,  0.1091,  0.3584]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[ 3.1279e-04, -2.1822e+00,  1.0253e+00,  ...,  1.0970e+00,\n",
      "          7.4039e-01,  1.0670e-01],\n",
      "        [ 1.8815e+00, -1.3140e+00,  7.5869e-01,  ..., -7.5867e-01,\n",
      "          1.5841e+00, -1.9046e-02],\n",
      "        [-5.9345e-01,  6.2166e-01,  1.2417e-01,  ...,  4.5167e-01,\n",
      "          6.9544e-01,  2.4020e-01],\n",
      "        ...,\n",
      "        [ 4.3345e-01,  1.6142e+00,  1.4172e+00,  ...,  1.7525e-01,\n",
      "         -3.2614e-01, -1.0161e+00],\n",
      "        [-2.8689e-01,  1.4800e+00,  2.0679e+00,  ...,  9.7712e-01,\n",
      "          6.5496e-01, -2.4386e-01],\n",
      "        [ 5.9036e-02, -6.7758e-01, -1.0912e+00,  ...,  9.4828e-02,\n",
      "          2.2048e-01,  2.8263e-01]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 0.2122,  1.4989, -1.4716,  ..., -1.8147, -0.7205, -0.6006],\n",
      "        [ 0.1318,  2.1808,  0.4059,  ..., -0.6011,  0.7042,  0.5679],\n",
      "        [ 0.4081,  1.3517, -0.9445,  ...,  0.0917, -0.5591, -0.5073],\n",
      "        ...,\n",
      "        [-0.2960,  0.2260,  0.0742,  ..., -1.4262, -0.1360,  0.7088],\n",
      "        [-0.4342,  2.0198, -0.3024,  ..., -1.3160, -1.2203,  0.9962],\n",
      "        [-0.4715,  0.5776,  1.5622,  ..., -0.5711, -1.0235, -1.1937]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[ 0.5973, -1.0360,  0.3328,  ...,  0.3306, -0.1297,  0.6277],\n",
      "        [ 2.2597, -0.3315, -0.1700,  ...,  0.4653, -0.9112, -0.4163],\n",
      "        [ 1.7443, -1.7244, -0.4056,  ...,  0.6922, -0.2743, -1.4563],\n",
      "        ...,\n",
      "        [ 0.4707,  1.6457,  1.8839,  ...,  0.7685, -0.2733, -0.3339],\n",
      "        [-0.0958, -0.5115,  0.1181,  ..., -0.7552, -1.0470, -1.0157],\n",
      "        [ 1.0866,  1.3541, -0.9793,  ..., -1.2868,  2.2402, -0.8322]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 1.5456,  0.3240, -0.0555,  ...,  0.7565,  0.0986, -1.6867],\n",
      "        [-0.7291,  0.5443, -1.3591,  ..., -1.5441, -0.8122, -0.3603],\n",
      "        [-0.2183, -0.3879,  1.1029,  ...,  1.1329,  0.6075,  0.3236],\n",
      "        ...,\n",
      "        [ 0.4260,  0.6409, -0.1889,  ..., -0.4813,  0.3679, -0.0809],\n",
      "        [ 1.3308, -0.2142,  0.5819,  ..., -0.3664,  2.2981, -0.5193],\n",
      "        [-1.0350,  0.2144, -0.5017,  ...,  0.6512,  0.8502, -0.0787]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[-0.0561,  0.5265, -0.0199,  ...,  0.9928,  0.8614,  1.7797],\n",
      "        [ 0.4129, -0.5194,  0.4573,  ..., -0.3377,  0.9917,  1.4713],\n",
      "        [-0.2496,  0.4970, -0.5728,  ...,  0.1896, -0.1581,  0.8471],\n",
      "        ...,\n",
      "        [-0.5261, -1.0043,  1.4675,  ..., -0.3640, -0.7123, -0.0065],\n",
      "        [-0.5824,  1.1499,  0.0581,  ..., -0.2694, -0.6517, -1.2480],\n",
      "        [-0.3478,  1.2587,  0.3001,  ..., -0.0977,  0.0727,  0.5280]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-0.5386, -0.3370, -1.1517,  ..., -0.9525,  0.9815,  0.4394],\n",
      "        [-0.7080, -0.3960,  0.1708,  ..., -0.5328, -0.9560, -0.6533],\n",
      "        [ 1.2628,  1.0334, -0.1082,  ...,  1.2418,  0.5220,  1.8052],\n",
      "        ...,\n",
      "        [-1.5893,  0.0095,  0.8657,  ...,  0.6719,  0.3414,  1.4910],\n",
      "        [ 0.1500, -1.5452,  1.0203,  ..., -0.9120, -0.7104, -1.1454],\n",
      "        [ 0.6864,  1.7719, -0.6383,  ...,  0.3413,  0.3461, -0.1190]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.5724,  1.7425, -0.3643,  ...,  0.6150, -1.3025,  0.8732],\n",
      "        [-0.6183, -0.2731,  1.1329,  ...,  0.0449,  0.4712, -0.7108],\n",
      "        [ 1.7921,  0.3958, -0.3645,  ..., -0.8763, -0.7936,  0.9422],\n",
      "        ...,\n",
      "        [-1.8028, -2.1165, -0.3553,  ..., -0.3348,  1.0850, -0.3740],\n",
      "        [-1.0700, -1.3001,  2.8916,  ..., -0.7863,  2.8789, -0.0116],\n",
      "        [-0.2005,  1.0920, -0.2896,  ..., -0.4767,  0.4902, -1.5399]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shape: torch.Size([10240, 320])\n",
      "tensor([[-0.4187,  1.0951,  0.0660,  ..., -0.8287, -0.4229, -1.2345],\n",
      "        [ 0.6896, -1.2515,  0.3865,  ...,  0.0897, -0.1509,  0.5643],\n",
      "        [-1.0247, -0.2633,  0.3580,  ..., -0.0423,  0.3022,  0.5978],\n",
      "        ...,\n",
      "        [ 0.8422,  0.3946,  0.8868,  ...,  0.9716, -0.2673, -0.4624],\n",
      "        [ 0.8709,  0.2600,  1.4173,  ..., -0.3134,  1.4116,  1.6430],\n",
      "        [-1.0940,  0.3908,  1.7361,  ...,  0.8747,  0.9138, -1.5579]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight, Shape: torch.Size([320, 5120])\n",
      "tensor([[-0.7020, -1.3906,  0.2165,  ..., -0.8877, -0.4632, -1.0255],\n",
      "        [ 0.7116,  0.9856, -1.2748,  ..., -0.4104,  0.8143,  1.7666],\n",
      "        [ 0.5040,  0.8281,  0.9859,  ..., -0.1792, -2.0344, -0.8176],\n",
      "        ...,\n",
      "        [ 1.1825, -1.0461, -0.5381,  ...,  0.8937, -0.0785,  0.2042],\n",
      "        [ 1.3307, -0.1856, -1.0481,  ...,  0.8923,  0.6441,  0.4071],\n",
      "        [-0.0133,  0.7678, -1.1518,  ...,  0.7147, -0.1772, -0.4415]])\n",
      "Layer: unet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-1.1399e+00, -8.1738e-01,  7.0006e-01,  ...,  1.7816e+00,\n",
      "          2.7456e+00, -1.3412e+00],\n",
      "        [ 1.5187e+00,  1.8095e+00, -1.2633e-01,  ...,  1.0456e+00,\n",
      "         -1.3708e+00, -1.7044e-01],\n",
      "        [ 3.8886e-01, -2.6750e-01,  3.5568e-01,  ...,  1.5251e-01,\n",
      "         -1.4220e-01,  3.0500e-01],\n",
      "        ...,\n",
      "        [-1.6748e+00,  8.3632e-01, -8.0066e-01,  ...,  2.7896e-01,\n",
      "         -1.4565e+00, -4.1784e-01],\n",
      "        [ 1.7290e+00,  1.8057e-03,  4.1122e-01,  ..., -1.5333e+00,\n",
      "         -4.1600e-01, -9.0627e-01],\n",
      "        [ 1.3402e-01,  9.1519e-01, -3.1297e-01,  ...,  1.7171e-01,\n",
      "          1.0967e+00,  1.0171e-01]])\n",
      "Layer: unet.down_blocks.2.attentions.1.proj_in.lora.down.weight, Shape: torch.Size([320, 1280, 1, 1])\n",
      "tensor([[[[-1.5087]],\n",
      "\n",
      "         [[-0.5154]],\n",
      "\n",
      "         [[-0.2920]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1960]],\n",
      "\n",
      "         [[ 1.1420]],\n",
      "\n",
      "         [[ 0.5143]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1563]],\n",
      "\n",
      "         [[-1.2677]],\n",
      "\n",
      "         [[ 0.7434]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6656]],\n",
      "\n",
      "         [[-0.9881]],\n",
      "\n",
      "         [[-1.2921]]],\n",
      "\n",
      "\n",
      "        [[[-1.4085]],\n",
      "\n",
      "         [[-0.7739]],\n",
      "\n",
      "         [[ 1.2389]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4742]],\n",
      "\n",
      "         [[-0.5333]],\n",
      "\n",
      "         [[ 0.7354]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.0000]],\n",
      "\n",
      "         [[ 0.1325]],\n",
      "\n",
      "         [[-0.7085]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8966]],\n",
      "\n",
      "         [[-3.3240]],\n",
      "\n",
      "         [[-0.6873]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9947]],\n",
      "\n",
      "         [[ 0.1569]],\n",
      "\n",
      "         [[-0.9946]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7545]],\n",
      "\n",
      "         [[ 2.3992]],\n",
      "\n",
      "         [[ 0.9201]]],\n",
      "\n",
      "\n",
      "        [[[-0.5154]],\n",
      "\n",
      "         [[ 0.0864]],\n",
      "\n",
      "         [[ 0.9583]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7640]],\n",
      "\n",
      "         [[-0.1785]],\n",
      "\n",
      "         [[ 0.9287]]]])\n",
      "Layer: unet.down_blocks.2.attentions.1.proj_in.lora.up.weight, Shape: torch.Size([1280, 320, 1, 1])\n",
      "tensor([[[[-0.5650]],\n",
      "\n",
      "         [[-1.0915]],\n",
      "\n",
      "         [[ 0.8804]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2370]],\n",
      "\n",
      "         [[-1.2793]],\n",
      "\n",
      "         [[-0.3127]]],\n",
      "\n",
      "\n",
      "        [[[-2.0985]],\n",
      "\n",
      "         [[-0.4972]],\n",
      "\n",
      "         [[ 0.5065]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6212]],\n",
      "\n",
      "         [[-1.5583]],\n",
      "\n",
      "         [[ 0.7326]]],\n",
      "\n",
      "\n",
      "        [[[-0.2136]],\n",
      "\n",
      "         [[-1.1098]],\n",
      "\n",
      "         [[-0.2682]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1923]],\n",
      "\n",
      "         [[-1.2413]],\n",
      "\n",
      "         [[ 0.1494]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.2413]],\n",
      "\n",
      "         [[-0.2869]],\n",
      "\n",
      "         [[-0.6482]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9684]],\n",
      "\n",
      "         [[ 0.1873]],\n",
      "\n",
      "         [[-0.4824]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6391]],\n",
      "\n",
      "         [[-1.0968]],\n",
      "\n",
      "         [[ 1.2381]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9516]],\n",
      "\n",
      "         [[ 0.4834]],\n",
      "\n",
      "         [[ 1.9746]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1634]],\n",
      "\n",
      "         [[-0.6511]],\n",
      "\n",
      "         [[-0.1424]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.8914]],\n",
      "\n",
      "         [[-0.0485]],\n",
      "\n",
      "         [[-2.1569]]]])\n",
      "Layer: unet.down_blocks.2.attentions.1.proj_out.lora.down.weight, Shape: torch.Size([320, 1280, 1, 1])\n",
      "tensor([[[[-0.5504]],\n",
      "\n",
      "         [[ 0.6910]],\n",
      "\n",
      "         [[-0.0546]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4487]],\n",
      "\n",
      "         [[ 0.8420]],\n",
      "\n",
      "         [[ 0.1536]]],\n",
      "\n",
      "\n",
      "        [[[-0.8468]],\n",
      "\n",
      "         [[ 0.9400]],\n",
      "\n",
      "         [[-0.4890]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9354]],\n",
      "\n",
      "         [[ 0.1775]],\n",
      "\n",
      "         [[ 1.1891]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4966]],\n",
      "\n",
      "         [[-0.3507]],\n",
      "\n",
      "         [[ 0.6079]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3363]],\n",
      "\n",
      "         [[ 0.6145]],\n",
      "\n",
      "         [[ 0.1401]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0835]],\n",
      "\n",
      "         [[ 0.4300]],\n",
      "\n",
      "         [[ 1.1791]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0426]],\n",
      "\n",
      "         [[ 1.3978]],\n",
      "\n",
      "         [[-0.4364]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5358]],\n",
      "\n",
      "         [[ 0.0635]],\n",
      "\n",
      "         [[ 0.2152]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6546]],\n",
      "\n",
      "         [[ 0.9980]],\n",
      "\n",
      "         [[ 1.7033]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7077]],\n",
      "\n",
      "         [[-0.8537]],\n",
      "\n",
      "         [[-0.9809]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6378]],\n",
      "\n",
      "         [[-0.9079]],\n",
      "\n",
      "         [[-1.7168]]]])\n",
      "Layer: unet.down_blocks.2.attentions.1.proj_out.lora.up.weight, Shape: torch.Size([1280, 320, 1, 1])\n",
      "tensor([[[[ 0.4521]],\n",
      "\n",
      "         [[ 1.4226]],\n",
      "\n",
      "         [[-1.7529]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3866]],\n",
      "\n",
      "         [[ 0.1787]],\n",
      "\n",
      "         [[-0.0858]]],\n",
      "\n",
      "\n",
      "        [[[-1.6208]],\n",
      "\n",
      "         [[-0.5341]],\n",
      "\n",
      "         [[ 0.9185]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8799]],\n",
      "\n",
      "         [[-0.9592]],\n",
      "\n",
      "         [[ 1.8392]]],\n",
      "\n",
      "\n",
      "        [[[-1.9371]],\n",
      "\n",
      "         [[ 1.3192]],\n",
      "\n",
      "         [[-0.2512]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7315]],\n",
      "\n",
      "         [[ 0.5583]],\n",
      "\n",
      "         [[ 0.4160]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.8490]],\n",
      "\n",
      "         [[ 1.1189]],\n",
      "\n",
      "         [[ 0.4740]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7648]],\n",
      "\n",
      "         [[-0.4379]],\n",
      "\n",
      "         [[-0.6634]]],\n",
      "\n",
      "\n",
      "        [[[-0.0717]],\n",
      "\n",
      "         [[ 2.1335]],\n",
      "\n",
      "         [[-1.9347]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4774]],\n",
      "\n",
      "         [[ 0.1602]],\n",
      "\n",
      "         [[ 1.4267]]],\n",
      "\n",
      "\n",
      "        [[[-1.4975]],\n",
      "\n",
      "         [[-0.0454]],\n",
      "\n",
      "         [[-0.7566]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.4921]],\n",
      "\n",
      "         [[-0.5338]],\n",
      "\n",
      "         [[-0.7040]]]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[ 0.3522, -0.5012, -0.2520,  ...,  0.2557, -0.9464,  2.3209],\n",
      "        [ 1.9084,  0.5342,  0.8101,  ...,  1.1992,  1.2487, -0.4729],\n",
      "        [ 0.4426, -0.1527, -0.7767,  ...,  0.7215, -1.0918,  0.0747],\n",
      "        ...,\n",
      "        [ 1.4705,  0.6478,  1.2279,  ..., -0.9527,  0.8660,  0.2259],\n",
      "        [-0.9445, -0.6870,  0.9478,  ..., -0.2628,  0.9471, -0.7267],\n",
      "        [-0.0108,  0.6571,  0.5322,  ..., -0.9028, -0.8631,  0.4323]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-1.1780, -0.4367, -0.0865,  ...,  0.4257, -1.5705,  0.0206],\n",
      "        [ 0.3454, -0.6850,  0.6373,  ...,  1.0280, -0.5199,  0.2854],\n",
      "        [-0.5791, -0.8626,  0.3067,  ..., -0.7836, -0.0665,  0.9218],\n",
      "        ...,\n",
      "        [-0.8703,  0.5240, -0.1511,  ..., -0.1648,  1.7725,  0.0787],\n",
      "        [-0.1019, -0.6445, -0.6884,  ..., -0.4898,  0.3050,  0.9053],\n",
      "        [-1.0848, -0.6707, -0.1753,  ..., -1.5985,  1.5114, -0.8024]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[ 0.8681,  1.4689,  1.1218,  ..., -0.1052, -1.0535,  1.9887],\n",
      "        [-0.1986,  0.5223,  0.3802,  ..., -0.7312, -0.5937,  0.0154],\n",
      "        [ 1.7439, -1.5541,  0.2553,  ..., -1.2393,  1.3302,  0.4020],\n",
      "        ...,\n",
      "        [-0.0644,  1.0726, -1.4986,  ...,  1.5188, -0.0855,  0.6585],\n",
      "        [-0.1011,  0.9544, -1.4507,  ...,  1.4550,  0.5122, -0.9967],\n",
      "        [-1.7891,  0.1186, -0.8396,  ..., -1.2551,  0.1979,  1.0669]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-0.4995, -2.2891,  0.7383,  ..., -0.5312, -0.0282, -0.7080],\n",
      "        [-0.8300, -0.6809,  1.5439,  ...,  0.8980,  0.1861, -1.9309],\n",
      "        [-0.4919, -0.7223,  0.6935,  ..., -0.6938, -0.7782, -1.7741],\n",
      "        ...,\n",
      "        [-0.9966, -1.3508,  0.2000,  ..., -1.7950, -2.9748,  1.8060],\n",
      "        [ 0.6088,  1.2119,  0.2091,  ..., -2.6441, -0.4996,  2.3660],\n",
      "        [ 0.2316, -0.0306, -0.8574,  ..., -2.1020, -0.7584,  0.4491]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[ 0.4194, -0.7051, -0.0611,  ..., -0.8019, -0.8370, -0.0792],\n",
      "        [-0.1999,  0.0261,  0.3800,  ..., -0.5593, -1.2218, -1.2823],\n",
      "        [-1.5950,  0.7994, -0.0415,  ..., -0.7637, -0.1536,  1.3334],\n",
      "        ...,\n",
      "        [-0.2376,  0.0946,  0.0777,  ..., -0.3072,  1.0426,  0.9048],\n",
      "        [ 0.5363,  1.8374, -0.0698,  ...,  0.0762,  0.3500,  0.4162],\n",
      "        [ 0.5823, -0.1673, -0.1197,  ..., -0.1832, -0.8632, -2.0515]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-0.5615,  0.5024,  0.0287,  ...,  0.2324,  0.3094,  0.9109],\n",
      "        [ 0.5806,  0.2062,  0.6435,  ..., -0.2323,  1.2312, -0.0093],\n",
      "        [ 0.8543, -0.3739,  0.6260,  ...,  1.0767, -0.6416,  0.3847],\n",
      "        ...,\n",
      "        [-0.0061, -0.6407, -0.8166,  ...,  0.6642,  1.1088,  0.0721],\n",
      "        [-1.3129, -0.4653, -0.1845,  ...,  0.2919,  1.3990, -0.7425],\n",
      "        [ 1.2618,  0.2849, -0.7940,  ..., -0.1017, -2.1612, -0.2774]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-2.7353, -1.4471, -1.0024,  ..., -0.7274,  0.7632,  0.3048],\n",
      "        [-0.1692,  1.6579, -0.7078,  ..., -0.6783,  0.6335, -1.6600],\n",
      "        [-0.6303, -0.4949,  0.2739,  ...,  1.4532, -0.6472, -1.0998],\n",
      "        ...,\n",
      "        [-0.4928,  1.2160, -0.8992,  ...,  0.6036,  0.1471,  0.0323],\n",
      "        [ 0.3758, -0.2370,  0.2746,  ...,  1.2153,  3.0589,  0.8434],\n",
      "        [-1.2568, -0.9006, -1.2199,  ...,  1.0391, -1.2305, -0.4557]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 1.2861, -1.5143, -0.2466,  ..., -0.5300,  0.6160, -0.5511],\n",
      "        [-0.3790,  1.8047, -2.5838,  ...,  0.9990,  0.2443, -1.0013],\n",
      "        [-0.5319, -0.7639,  1.1374,  ..., -1.0905,  0.1776, -0.5531],\n",
      "        ...,\n",
      "        [ 1.8385, -0.0350, -0.5003,  ...,  2.2660, -2.4431, -0.6427],\n",
      "        [ 0.1537,  0.7393, -1.3673,  ...,  0.6560,  2.4405,  0.2628],\n",
      "        [ 1.7871,  0.8142, -1.2755,  ...,  0.0272, -0.4570, -0.2350]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 0.7377, -0.3262,  0.6663,  ..., -0.8041, -0.2385,  1.6723],\n",
      "        [ 0.8861, -0.1075, -0.7932,  ..., -1.3193, -0.9832,  2.1968],\n",
      "        [-0.5679, -1.2318, -0.1854,  ..., -1.9172,  0.3086,  1.2013],\n",
      "        ...,\n",
      "        [-0.8207, -1.7483, -0.1020,  ..., -0.6845,  0.1616, -0.8887],\n",
      "        [ 0.1468,  0.6606, -0.4835,  ..., -0.4338,  0.5793, -1.1547],\n",
      "        [-0.7745,  1.2790,  0.6208,  ...,  0.6487, -1.5302, -0.2324]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 3.3873,  0.2948,  3.6183,  ...,  1.5475, -0.2743,  0.2935],\n",
      "        [ 1.1905, -0.9882, -2.6901,  ..., -2.1472, -0.3161,  0.7820],\n",
      "        [-1.4316,  0.8723, -0.8304,  ...,  1.1192, -0.5047, -2.0762],\n",
      "        ...,\n",
      "        [-0.3910, -1.8226,  1.6037,  ..., -0.8818, -0.9640, -0.6196],\n",
      "        [-0.5050,  0.8631,  1.1891,  ...,  0.1680,  0.8615,  1.0189],\n",
      "        [ 1.1797,  2.6753,  1.9107,  ..., -0.3793,  1.7645,  1.2211]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.9409,  1.7121, -1.5699,  ...,  0.2019,  1.2696,  0.1954],\n",
      "        [-0.8066, -2.2053,  0.7833,  ..., -1.3125, -0.4146,  0.5123],\n",
      "        [-0.0670,  0.5052,  0.3513,  ..., -0.2312,  0.1070, -1.3678],\n",
      "        ...,\n",
      "        [-0.7641, -1.5779,  1.1193,  ..., -1.1717,  0.1291, -0.3596],\n",
      "        [-0.0588,  0.8892, -0.0148,  ...,  1.6979, -0.6075,  0.3342],\n",
      "        [-1.4402, -0.4211,  1.3701,  ..., -0.9811, -1.2465, -0.1737]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-2.6354, -0.1114, -0.2277,  ..., -1.0711, -0.0186,  0.0423],\n",
      "        [-1.1169, -0.8548,  0.8945,  ...,  0.4194,  0.0393, -1.7382],\n",
      "        [-0.9080,  0.0837, -0.6783,  ...,  1.2355,  0.9120, -0.7431],\n",
      "        ...,\n",
      "        [-0.8051, -0.5902, -1.9306,  ..., -1.6912, -0.2256, -0.6005],\n",
      "        [-0.3264, -0.7211, -0.0471,  ..., -0.2729,  1.6413, -0.3496],\n",
      "        [-0.2657,  0.4007, -1.5810,  ..., -0.7378, -0.2189,  0.1568]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-1.1259, -0.0917,  1.1202,  ..., -0.1875, -1.0392,  0.2583],\n",
      "        [-0.1900, -1.3064,  1.0586,  ..., -0.5840,  1.3856,  0.0170],\n",
      "        [ 0.0352,  0.8750,  1.2171,  ..., -0.5619, -0.8628, -2.1973],\n",
      "        ...,\n",
      "        [ 0.9280, -0.8162, -1.0237,  ...,  0.3599, -0.2138, -0.1925],\n",
      "        [-0.6436,  0.7389,  0.4462,  ..., -0.1358, -0.4711, -0.0389],\n",
      "        [ 0.5414,  1.0012, -0.4287,  ...,  0.1734, -0.2651,  0.4559]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 0.7367,  0.0516,  0.6006,  ..., -1.0670,  0.3760,  0.1411],\n",
      "        [ 2.9288, -1.1920, -1.2292,  ...,  0.8498,  1.0677, -0.6811],\n",
      "        [ 0.9342,  0.6694, -1.3998,  ..., -2.4392, -0.2878,  0.5780],\n",
      "        ...,\n",
      "        [ 1.9789, -0.3825,  1.0575,  ..., -0.3826,  1.6309,  0.2395],\n",
      "        [-1.3003,  0.5888,  0.1159,  ...,  0.2731,  1.1349,  0.5983],\n",
      "        [ 0.9971,  0.1878, -0.6809,  ...,  0.6834, -0.3125, -0.5099]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[-1.0536,  0.6210,  0.6179,  ...,  1.2673,  0.7065, -1.4807],\n",
      "        [ 0.9755,  1.2301, -1.3227,  ...,  0.7063,  0.0100,  0.9871],\n",
      "        [-2.7037, -2.1663, -1.6100,  ...,  0.3304, -0.1650, -0.4326],\n",
      "        ...,\n",
      "        [-0.4927, -0.3268, -2.2130,  ...,  0.1132, -0.9613,  0.7368],\n",
      "        [-0.8476, -0.2547,  0.4486,  ..., -0.7212,  0.2015, -1.1123],\n",
      "        [ 0.5818, -0.4090, -1.0838,  ..., -0.3612, -0.6234,  0.6235]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 0.5297, -0.7286,  1.2177,  ...,  0.7017,  0.4425, -0.5839],\n",
      "        [-0.1725, -3.3992,  2.7446,  ..., -2.8220,  0.6127,  0.3928],\n",
      "        [ 1.1591,  0.8023,  2.3917,  ...,  1.6545, -1.3499, -1.2382],\n",
      "        ...,\n",
      "        [-0.3161,  0.3712,  0.8506,  ..., -1.2984, -0.7424, -0.6453],\n",
      "        [ 1.1301,  2.3825, -1.7246,  ...,  1.8271,  0.5135,  0.7641],\n",
      "        [-0.6471, -1.0663,  0.2166,  ..., -0.7627,  1.4368, -0.7787]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.7433,  0.0457, -1.2466,  ..., -1.1550, -0.0189, -0.0975],\n",
      "        [-0.3432, -1.3152, -0.9302,  ...,  0.7100, -0.4751, -1.2919],\n",
      "        [ 1.2856,  0.1131,  0.3956,  ..., -0.5555,  0.2615, -1.3598],\n",
      "        ...,\n",
      "        [-0.0318, -1.0514,  0.1048,  ...,  0.0954,  0.9535,  0.1363],\n",
      "        [-0.2429, -0.7280, -0.5250,  ..., -0.0404, -2.3518,  0.1265],\n",
      "        [ 1.2936,  0.7370,  1.5105,  ..., -0.0702,  1.1270, -1.1519]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shape: torch.Size([10240, 320])\n",
      "tensor([[-1.2872, -0.1625, -2.0794,  ...,  0.1903,  0.7450,  0.7598],\n",
      "        [ 0.9361, -0.0977, -0.4036,  ...,  0.4278, -0.5432,  1.2461],\n",
      "        [ 1.0599,  0.1791, -0.5487,  ...,  1.5694, -0.5464,  0.2331],\n",
      "        ...,\n",
      "        [ 0.2258, -0.0180,  0.2645,  ...,  0.1659,  1.3773,  0.4729],\n",
      "        [-1.0543,  0.9741, -0.7658,  ...,  0.5860, -0.8625, -0.1632],\n",
      "        [-1.3375, -0.3082, -0.6002,  ...,  1.7416, -1.0706, -0.0958]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight, Shape: torch.Size([320, 5120])\n",
      "tensor([[-0.1710,  0.6392,  2.7114,  ..., -0.3291,  0.3623, -0.7390],\n",
      "        [-0.3283,  0.2356, -0.2912,  ...,  0.6071, -1.3076, -0.1893],\n",
      "        [-0.3185, -0.1910,  0.3298,  ...,  0.1514,  0.0893, -0.5315],\n",
      "        ...,\n",
      "        [-0.0134,  0.1348,  0.3516,  ..., -0.6677, -0.7263, -1.3603],\n",
      "        [-0.8658, -1.0893,  0.2846,  ...,  1.1218,  0.9407,  1.9221],\n",
      "        [ 0.2682,  1.0956,  0.5794,  ..., -0.8949,  0.2207,  0.3206]])\n",
      "Layer: unet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-1.9012,  0.0671,  1.2603,  ...,  1.0244, -0.2393, -0.7623],\n",
      "        [ 0.9306, -0.4132, -1.8797,  ...,  0.2824, -0.2694,  0.3266],\n",
      "        [-0.8945,  1.2762,  0.8588,  ...,  1.5365, -0.1004,  2.1075],\n",
      "        ...,\n",
      "        [ 0.2486,  0.7193,  1.6720,  ..., -0.2462, -0.9511, -0.9741],\n",
      "        [-0.5499, -2.2286,  0.5632,  ...,  0.0662,  0.4088,  0.1495],\n",
      "        [-0.6017,  0.0208, -0.3682,  ..., -0.5157, -0.0384,  0.2020]])\n",
      "Layer: unet.mid_block.attentions.0.proj_in.lora.down.weight, Shape: torch.Size([320, 1280, 1, 1])\n",
      "tensor([[[[-0.7119]],\n",
      "\n",
      "         [[-0.1485]],\n",
      "\n",
      "         [[ 1.3155]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5389]],\n",
      "\n",
      "         [[ 1.2439]],\n",
      "\n",
      "         [[-0.0171]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5926]],\n",
      "\n",
      "         [[-0.6590]],\n",
      "\n",
      "         [[ 1.6897]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1413]],\n",
      "\n",
      "         [[ 0.1147]],\n",
      "\n",
      "         [[ 0.3440]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5937]],\n",
      "\n",
      "         [[ 0.9691]],\n",
      "\n",
      "         [[ 0.4330]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6206]],\n",
      "\n",
      "         [[-0.2450]],\n",
      "\n",
      "         [[ 0.2656]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.1869]],\n",
      "\n",
      "         [[-0.0443]],\n",
      "\n",
      "         [[-0.5633]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7715]],\n",
      "\n",
      "         [[ 0.2925]],\n",
      "\n",
      "         [[-0.2188]]],\n",
      "\n",
      "\n",
      "        [[[-0.5659]],\n",
      "\n",
      "         [[ 1.6772]],\n",
      "\n",
      "         [[ 1.7355]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2275]],\n",
      "\n",
      "         [[-0.2496]],\n",
      "\n",
      "         [[-1.0907]]],\n",
      "\n",
      "\n",
      "        [[[-1.6143]],\n",
      "\n",
      "         [[-1.2752]],\n",
      "\n",
      "         [[ 2.3824]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9081]],\n",
      "\n",
      "         [[-0.6084]],\n",
      "\n",
      "         [[-0.4728]]]])\n",
      "Layer: unet.mid_block.attentions.0.proj_in.lora.up.weight, Shape: torch.Size([1280, 320, 1, 1])\n",
      "tensor([[[[-1.2049]],\n",
      "\n",
      "         [[ 0.8880]],\n",
      "\n",
      "         [[ 1.3990]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2423]],\n",
      "\n",
      "         [[ 0.4106]],\n",
      "\n",
      "         [[ 1.8272]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4851]],\n",
      "\n",
      "         [[-0.0074]],\n",
      "\n",
      "         [[-0.3696]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6398]],\n",
      "\n",
      "         [[ 0.5341]],\n",
      "\n",
      "         [[ 0.0939]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4502]],\n",
      "\n",
      "         [[ 0.4308]],\n",
      "\n",
      "         [[-0.1163]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0985]],\n",
      "\n",
      "         [[ 1.2826]],\n",
      "\n",
      "         [[-0.0036]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.6110]],\n",
      "\n",
      "         [[ 0.1324]],\n",
      "\n",
      "         [[-0.3395]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9339]],\n",
      "\n",
      "         [[ 0.5229]],\n",
      "\n",
      "         [[-0.9712]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3374]],\n",
      "\n",
      "         [[-2.3520]],\n",
      "\n",
      "         [[ 0.0086]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7367]],\n",
      "\n",
      "         [[ 0.3485]],\n",
      "\n",
      "         [[ 1.2040]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6810]],\n",
      "\n",
      "         [[-0.0797]],\n",
      "\n",
      "         [[-0.1505]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6316]],\n",
      "\n",
      "         [[ 0.2484]],\n",
      "\n",
      "         [[ 0.2136]]]])\n",
      "Layer: unet.mid_block.attentions.0.proj_out.lora.down.weight, Shape: torch.Size([320, 1280, 1, 1])\n",
      "tensor([[[[ 0.3545]],\n",
      "\n",
      "         [[-0.5524]],\n",
      "\n",
      "         [[-2.0294]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.6537]],\n",
      "\n",
      "         [[-1.1637]],\n",
      "\n",
      "         [[-0.7086]]],\n",
      "\n",
      "\n",
      "        [[[-0.1754]],\n",
      "\n",
      "         [[-1.1005]],\n",
      "\n",
      "         [[ 1.1072]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3869]],\n",
      "\n",
      "         [[ 0.7710]],\n",
      "\n",
      "         [[-0.6211]]],\n",
      "\n",
      "\n",
      "        [[[-0.6277]],\n",
      "\n",
      "         [[ 1.5712]],\n",
      "\n",
      "         [[ 0.1983]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9831]],\n",
      "\n",
      "         [[-0.2389]],\n",
      "\n",
      "         [[ 1.2484]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.2841]],\n",
      "\n",
      "         [[ 0.6124]],\n",
      "\n",
      "         [[ 0.6142]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0922]],\n",
      "\n",
      "         [[ 0.4852]],\n",
      "\n",
      "         [[ 1.8599]]],\n",
      "\n",
      "\n",
      "        [[[-1.4909]],\n",
      "\n",
      "         [[ 1.9719]],\n",
      "\n",
      "         [[ 1.1170]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0232]],\n",
      "\n",
      "         [[ 0.5569]],\n",
      "\n",
      "         [[ 0.2317]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5574]],\n",
      "\n",
      "         [[ 0.7677]],\n",
      "\n",
      "         [[ 0.0560]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4971]],\n",
      "\n",
      "         [[ 0.0223]],\n",
      "\n",
      "         [[-0.1804]]]])\n",
      "Layer: unet.mid_block.attentions.0.proj_out.lora.up.weight, Shape: torch.Size([1280, 320, 1, 1])\n",
      "tensor([[[[ 0.1087]],\n",
      "\n",
      "         [[-0.6567]],\n",
      "\n",
      "         [[-1.1382]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3557]],\n",
      "\n",
      "         [[-0.8520]],\n",
      "\n",
      "         [[-0.1505]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7408]],\n",
      "\n",
      "         [[-0.2831]],\n",
      "\n",
      "         [[-0.7778]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.4131]],\n",
      "\n",
      "         [[ 0.4934]],\n",
      "\n",
      "         [[ 0.4880]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4825]],\n",
      "\n",
      "         [[-0.3778]],\n",
      "\n",
      "         [[ 1.0624]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9069]],\n",
      "\n",
      "         [[-0.5408]],\n",
      "\n",
      "         [[-0.0394]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.3564]],\n",
      "\n",
      "         [[ 1.0853]],\n",
      "\n",
      "         [[-0.5851]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6093]],\n",
      "\n",
      "         [[-0.1692]],\n",
      "\n",
      "         [[ 0.7803]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0620]],\n",
      "\n",
      "         [[ 0.1847]],\n",
      "\n",
      "         [[ 1.6286]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1321]],\n",
      "\n",
      "         [[ 1.4548]],\n",
      "\n",
      "         [[-0.5055]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5149]],\n",
      "\n",
      "         [[-0.3617]],\n",
      "\n",
      "         [[-1.6676]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3796]],\n",
      "\n",
      "         [[-0.9652]],\n",
      "\n",
      "         [[ 0.1014]]]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.9336, -0.9576,  0.5110,  ..., -0.3812,  0.0317,  0.4118],\n",
      "        [ 1.1744, -0.3376,  1.1300,  ...,  1.5388, -0.8953, -0.4835],\n",
      "        [-0.1982,  0.8198, -0.8804,  ..., -0.1205, -0.1370, -0.5689],\n",
      "        ...,\n",
      "        [-0.0338,  1.1479, -0.9094,  ..., -0.7906,  0.1973, -0.1955],\n",
      "        [ 0.4338,  1.2464, -0.1656,  ...,  0.5282, -0.5915, -1.8145],\n",
      "        [ 0.2611, -0.9510, -0.5257,  ...,  0.2389,  0.0894, -0.9712]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-0.1875,  0.0326, -0.3578,  ..., -0.8841, -2.7596, -0.2439],\n",
      "        [-0.0751, -0.0779,  0.0063,  ..., -0.3484,  0.6719, -0.1917],\n",
      "        [-0.4948,  0.4031,  0.1190,  ...,  0.7110,  0.6182,  0.2106],\n",
      "        ...,\n",
      "        [-0.0072, -0.3454, -0.0752,  ...,  0.8097,  0.8463, -0.6899],\n",
      "        [ 0.6689,  0.0588, -0.5045,  ..., -0.1067,  1.7652, -0.1093],\n",
      "        [ 0.0296,  0.3860,  0.0221,  ..., -0.4616, -0.2454, -0.0548]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[ 0.5667, -0.9118, -0.2039,  ..., -0.0855,  1.0018, -0.2042],\n",
      "        [-1.4714, -0.2453,  0.2650,  ..., -0.6037,  1.0139,  0.4470],\n",
      "        [-2.2092, -0.4465,  0.7223,  ...,  0.3908,  1.4617,  0.3315],\n",
      "        ...,\n",
      "        [-0.7099, -2.0518,  1.0755,  ...,  0.4612, -0.2500,  0.4912],\n",
      "        [-0.1669,  0.6763,  0.5831,  ..., -0.0315,  0.2190,  0.5643],\n",
      "        [ 1.4928, -1.7180, -0.2205,  ...,  0.9598,  2.0820, -0.3605]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 0.9023, -1.7532,  0.7166,  ..., -0.5930,  0.0977, -1.0902],\n",
      "        [-0.4920,  1.6173, -0.4393,  ...,  0.8224, -0.9607, -0.4268],\n",
      "        [-0.4167,  0.7227,  0.0773,  ...,  0.4828, -0.4566,  0.5859],\n",
      "        ...,\n",
      "        [ 2.0800, -0.4556, -1.3810,  ..., -0.3102,  0.5400, -0.3986],\n",
      "        [ 0.0534, -0.0225,  0.1568,  ...,  2.2297,  0.1477, -0.6918],\n",
      "        [ 0.1320, -0.0987, -0.8023,  ...,  0.6056,  1.4413, -1.0371]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.2805,  0.6346, -0.5952,  ...,  0.7528, -1.6245,  0.5462],\n",
      "        [-0.0461,  0.2727,  0.7827,  ..., -0.2560,  0.9299, -0.0207],\n",
      "        [ 1.0763, -0.0439, -1.0988,  ..., -0.2549,  0.4717, -0.8388],\n",
      "        ...,\n",
      "        [ 1.2138, -0.3975,  0.5076,  ..., -1.2785,  0.1283, -0.4367],\n",
      "        [ 0.5134, -0.1799,  2.4397,  ..., -1.4703,  1.5563,  1.2181],\n",
      "        [-0.0740,  0.1038,  0.2579,  ...,  0.0037, -0.0248, -0.4968]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-0.4096,  0.0746, -0.6148,  ..., -0.3886,  0.1251, -0.2539],\n",
      "        [-0.5772,  0.6466, -0.8734,  ..., -0.1119, -0.1079,  0.0179],\n",
      "        [-0.2039,  0.5251,  0.5184,  ...,  0.1188, -1.4305,  0.5015],\n",
      "        ...,\n",
      "        [ 0.3641,  0.0240, -0.0805,  ..., -0.1497, -0.1237, -0.1432],\n",
      "        [ 0.7388,  0.5422, -0.4081,  ..., -0.4264,  0.7024,  0.4259],\n",
      "        [ 0.5261, -0.0292,  0.9905,  ...,  0.3721,  1.0863,  0.0533]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[ 1.0582,  0.5634, -1.2518,  ..., -2.3885,  1.4533, -1.0245],\n",
      "        [ 1.5661,  1.1517, -0.4360,  ..., -0.3396,  0.6430, -0.1532],\n",
      "        [-0.9404, -0.8750,  0.2596,  ..., -1.4651,  0.4183, -1.0621],\n",
      "        ...,\n",
      "        [-1.0405, -1.1761, -1.5680,  ...,  0.1272,  0.5780, -0.0751],\n",
      "        [-0.7508, -1.6924, -1.5806,  ...,  2.1676,  0.2928,  0.0151],\n",
      "        [ 0.9134, -0.0868,  1.4003,  ..., -0.5941,  0.4289, -0.6597]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-0.8575, -0.3204,  1.5780,  ..., -0.0891, -0.7814,  0.6134],\n",
      "        [-0.1770,  0.1858, -1.0444,  ..., -1.1217, -0.3110,  0.2271],\n",
      "        [ 0.1150,  0.3920, -0.8798,  ..., -0.3103, -0.9359,  0.4476],\n",
      "        ...,\n",
      "        [ 0.1402, -0.5855, -1.0355,  ..., -0.8300, -1.0907,  1.1293],\n",
      "        [-0.0676,  1.0983,  0.5537,  ..., -1.8380,  0.7453,  2.0018],\n",
      "        [-0.6644,  1.4475,  2.0609,  ...,  0.6846,  0.9285, -0.3577]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[-0.3364, -0.9594, -0.5371,  ...,  0.2118,  1.2271,  0.2810],\n",
      "        [ 0.5756, -0.8431,  0.8553,  ..., -0.5116, -0.6931, -0.6621],\n",
      "        [-1.5667,  0.2164, -0.1316,  ...,  0.0654,  0.9703,  0.1395],\n",
      "        ...,\n",
      "        [-0.0831,  1.3793,  0.4933,  ..., -0.6731,  1.4727,  1.4722],\n",
      "        [-0.3163,  0.5906, -0.0814,  ..., -2.7932, -0.6001,  0.2828],\n",
      "        [-0.9329,  0.7284, -1.3620,  ...,  0.8673,  0.6766,  0.5174]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 1.4908,  0.1834,  1.4148,  ..., -0.5413, -1.7643, -1.2889],\n",
      "        [-0.3730,  0.9750,  1.1694,  ...,  1.2980,  0.4514, -0.7648],\n",
      "        [ 0.0074, -0.0905, -0.8105,  ..., -1.9628, -0.4033,  1.0259],\n",
      "        ...,\n",
      "        [-0.9624, -0.6061, -0.7823,  ..., -0.1397,  0.5205,  0.3393],\n",
      "        [-1.8088, -0.5496,  1.5155,  ...,  0.8387, -1.1305, -0.3348],\n",
      "        [ 1.2479, -0.2908,  1.2691,  ...,  0.6194, -0.7043,  0.5012]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.1689,  0.2504, -0.4967,  ...,  0.4300, -0.2285,  0.0740],\n",
      "        [-1.6427,  1.1600, -0.2360,  ...,  0.7028,  1.2973,  0.3325],\n",
      "        [ 0.9302, -0.8495,  1.8258,  ..., -0.4012,  2.4071,  0.2220],\n",
      "        ...,\n",
      "        [-0.3113,  2.2033,  0.1151,  ...,  0.8564, -1.3206,  0.8750],\n",
      "        [-1.8348, -1.2292, -2.0431,  ...,  0.4460,  0.4303, -0.4062],\n",
      "        [-2.3309, -0.2826,  0.7993,  ..., -1.3603,  0.4962,  2.1377]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-1.2950,  1.3182, -0.7406,  ...,  0.5302,  0.0133, -0.8081],\n",
      "        [-0.5577,  0.2748,  1.3956,  ...,  0.8693,  1.6929, -0.5712],\n",
      "        [-0.0590, -0.0865,  1.3297,  ...,  0.7519,  0.8837,  0.8087],\n",
      "        ...,\n",
      "        [-0.0085,  0.2213, -0.3003,  ...,  0.2348, -0.1714, -0.5539],\n",
      "        [ 0.5092,  2.6545, -2.6783,  ...,  0.3885,  0.8766,  0.3018],\n",
      "        [-0.7994, -2.6954,  0.1203,  ...,  0.3863, -0.0868, -1.2765]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[ 0.4541, -0.1165, -0.1953,  ...,  0.6931, -0.5474, -0.9551],\n",
      "        [ 0.4822, -0.7370,  0.6060,  ...,  0.1953, -1.0703,  0.2615],\n",
      "        [ 0.7490, -0.6169,  0.6635,  ...,  1.0202, -0.3005, -0.0383],\n",
      "        ...,\n",
      "        [ 0.2669, -0.4483,  0.5538,  ...,  0.5756,  1.3289,  0.1405],\n",
      "        [ 0.8225, -1.8159,  0.8296,  ...,  0.7889, -1.0604,  0.7320],\n",
      "        [ 0.8516,  0.7151, -0.8238,  ...,  0.6881, -0.4732, -0.0847]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-1.2654,  0.1490,  1.5995,  ..., -0.7355,  1.7001,  0.1354],\n",
      "        [ 0.2733, -0.6977, -0.0689,  ...,  0.9068,  1.2801,  0.2569],\n",
      "        [-1.9653, -0.3016, -0.8122,  ...,  0.0341, -0.9391,  0.4611],\n",
      "        ...,\n",
      "        [-1.3503,  0.0325, -0.0685,  ..., -0.3258,  1.7628,  0.3833],\n",
      "        [ 0.4574,  0.1153,  0.1375,  ...,  0.7490,  1.6622,  0.2460],\n",
      "        [ 1.7972,  0.3280,  0.1893,  ...,  0.4716, -1.5147,  0.3367]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 2.1076, -0.4173, -0.1564,  ..., -0.1674, -0.2772, -0.2795],\n",
      "        [ 0.3820, -0.4505,  0.2431,  ..., -1.4565, -0.2208, -0.7391],\n",
      "        [ 0.2535,  0.9400,  0.8620,  ...,  1.5821,  1.0274,  0.1581],\n",
      "        ...,\n",
      "        [-0.6052,  2.6408, -0.0771,  ..., -1.1897,  0.3067,  0.7334],\n",
      "        [-1.6494, -0.2805,  0.0308,  ...,  0.6542, -0.4533,  1.2517],\n",
      "        [ 0.1453,  0.4518,  1.8135,  ...,  0.0490, -0.3097, -2.3342]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-2.1113, -1.1191, -0.4156,  ..., -1.3265,  1.0547, -1.6317],\n",
      "        [ 0.0274, -1.5678,  0.1358,  ...,  0.1790,  0.6047,  1.1777],\n",
      "        [ 0.1988, -0.1067, -0.1407,  ..., -0.2934,  2.2247,  1.0557],\n",
      "        ...,\n",
      "        [ 0.9681, -0.5373,  0.1121,  ..., -1.4927,  1.8106, -0.3878],\n",
      "        [ 1.6190, -1.2360, -1.7925,  ...,  1.3986,  0.8882, -0.7790],\n",
      "        [ 0.3143, -0.8137, -0.7643,  ...,  1.2373,  1.4073, -1.4100]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.1884,  1.0721,  0.5150,  ...,  0.1812,  0.4485, -0.8304],\n",
      "        [ 0.3169,  0.2723,  0.3874,  ...,  0.1588,  0.1711,  0.5333],\n",
      "        [ 1.3424,  0.4451,  1.0370,  ...,  0.6069, -0.6629, -0.0604],\n",
      "        ...,\n",
      "        [-0.6043,  0.1499, -0.5250,  ..., -0.6601, -2.2156,  0.0702],\n",
      "        [-0.2187,  0.9382, -0.3346,  ..., -1.6246,  0.6863, -1.0699],\n",
      "        [-1.3670,  0.3619,  0.5757,  ..., -0.3734, -0.2676, -0.3279]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shape: torch.Size([10240, 320])\n",
      "tensor([[-8.3207e-01,  4.2576e-01, -1.9735e+00,  ..., -4.3982e-02,\n",
      "          5.1277e-01,  6.5919e-01],\n",
      "        [ 1.2385e-01, -4.3856e-01, -7.7630e-01,  ...,  5.4963e-01,\n",
      "          2.4428e-03,  4.1550e-01],\n",
      "        [ 5.2472e-01,  4.3126e-01, -8.0830e-02,  ..., -1.3014e+00,\n",
      "          9.4502e-01,  2.4229e-01],\n",
      "        ...,\n",
      "        [-3.1525e+00,  1.0842e+00,  1.4432e+00,  ...,  7.1462e-01,\n",
      "          1.4691e+00, -4.7776e-01],\n",
      "        [-4.9423e-01, -1.4077e-01,  1.0863e+00,  ...,  1.1477e+00,\n",
      "          9.0865e-01,  1.0540e-01],\n",
      "        [ 6.9449e-01, -4.7561e-02,  5.5137e-01,  ...,  2.6731e+00,\n",
      "         -5.3715e-01, -2.5303e+00]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight, Shape: torch.Size([320, 5120])\n",
      "tensor([[ 1.0500,  0.0342,  0.1408,  ...,  0.7401,  0.3741,  1.3451],\n",
      "        [-2.4757,  0.5079,  2.5596,  ...,  0.2263,  0.6054,  0.0319],\n",
      "        [-0.3791, -1.2638,  0.5271,  ..., -0.2174,  0.0685,  0.7173],\n",
      "        ...,\n",
      "        [-1.2830, -0.3758,  0.6922,  ...,  0.2167, -0.9702,  0.6984],\n",
      "        [ 1.3521, -1.7434, -1.5683,  ..., -0.1552, -0.8279, -0.7775],\n",
      "        [ 1.4836, -0.9657, -1.9150,  ..., -0.8123, -0.8176, -1.3145]])\n",
      "Layer: unet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 1.1056,  1.2884, -0.4031,  ..., -0.3620,  1.6621,  0.9126],\n",
      "        [-1.2990,  0.8375,  0.8342,  ..., -1.3512,  0.6846, -1.0415],\n",
      "        [ 0.7710,  1.1159, -0.8810,  ..., -1.2285, -0.5224,  0.1356],\n",
      "        ...,\n",
      "        [ 0.9031, -1.1105,  0.7981,  ..., -1.3221,  1.0016, -0.1138],\n",
      "        [ 0.5591,  1.1070,  0.8185,  ..., -2.4060, -1.3934, -1.5221],\n",
      "        [ 0.4200, -0.4328,  0.7578,  ..., -0.4668,  0.4843, -0.7122]])\n",
      "Layer: unet.up_blocks.1.attentions.0.proj_in.lora.down.weight, Shape: torch.Size([320, 1280, 1, 1])\n",
      "tensor([[[[ 0.5091]],\n",
      "\n",
      "         [[-1.5496]],\n",
      "\n",
      "         [[-0.4333]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2876]],\n",
      "\n",
      "         [[-0.2520]],\n",
      "\n",
      "         [[-0.8776]]],\n",
      "\n",
      "\n",
      "        [[[-0.0877]],\n",
      "\n",
      "         [[-0.3996]],\n",
      "\n",
      "         [[-0.0740]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2026]],\n",
      "\n",
      "         [[-0.7564]],\n",
      "\n",
      "         [[ 0.5694]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4556]],\n",
      "\n",
      "         [[ 0.2863]],\n",
      "\n",
      "         [[-0.4909]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2461]],\n",
      "\n",
      "         [[ 0.5596]],\n",
      "\n",
      "         [[-0.1193]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.3188]],\n",
      "\n",
      "         [[-0.5944]],\n",
      "\n",
      "         [[ 0.5024]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4211]],\n",
      "\n",
      "         [[-0.6743]],\n",
      "\n",
      "         [[ 0.1510]]],\n",
      "\n",
      "\n",
      "        [[[-1.3420]],\n",
      "\n",
      "         [[-2.3902]],\n",
      "\n",
      "         [[ 1.1141]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2015]],\n",
      "\n",
      "         [[-0.7343]],\n",
      "\n",
      "         [[-1.3454]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0337]],\n",
      "\n",
      "         [[ 0.6456]],\n",
      "\n",
      "         [[ 0.1964]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1499]],\n",
      "\n",
      "         [[ 0.2618]],\n",
      "\n",
      "         [[ 1.2625]]]])\n",
      "Layer: unet.up_blocks.1.attentions.0.proj_in.lora.up.weight, Shape: torch.Size([1280, 320, 1, 1])\n",
      "tensor([[[[-0.4274]],\n",
      "\n",
      "         [[-0.9500]],\n",
      "\n",
      "         [[ 1.1977]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5269]],\n",
      "\n",
      "         [[-0.3503]],\n",
      "\n",
      "         [[ 1.3550]]],\n",
      "\n",
      "\n",
      "        [[[-1.9837]],\n",
      "\n",
      "         [[-1.8209]],\n",
      "\n",
      "         [[ 1.1408]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.8080]],\n",
      "\n",
      "         [[-0.4839]],\n",
      "\n",
      "         [[ 0.0925]]],\n",
      "\n",
      "\n",
      "        [[[-1.2856]],\n",
      "\n",
      "         [[ 0.0966]],\n",
      "\n",
      "         [[-0.7887]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9594]],\n",
      "\n",
      "         [[-0.9830]],\n",
      "\n",
      "         [[ 0.2053]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.1584]],\n",
      "\n",
      "         [[ 1.3547]],\n",
      "\n",
      "         [[ 0.4136]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3282]],\n",
      "\n",
      "         [[ 1.3261]],\n",
      "\n",
      "         [[-0.1111]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1449]],\n",
      "\n",
      "         [[-1.8115]],\n",
      "\n",
      "         [[ 1.2571]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9380]],\n",
      "\n",
      "         [[ 0.5469]],\n",
      "\n",
      "         [[ 0.7216]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4883]],\n",
      "\n",
      "         [[ 0.3656]],\n",
      "\n",
      "         [[ 0.9556]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9509]],\n",
      "\n",
      "         [[-0.5516]],\n",
      "\n",
      "         [[ 1.0698]]]])\n",
      "Layer: unet.up_blocks.1.attentions.0.proj_out.lora.down.weight, Shape: torch.Size([320, 1280, 1, 1])\n",
      "tensor([[[[ 0.2071]],\n",
      "\n",
      "         [[ 1.0351]],\n",
      "\n",
      "         [[ 1.4230]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6600]],\n",
      "\n",
      "         [[-1.0675]],\n",
      "\n",
      "         [[-2.0083]]],\n",
      "\n",
      "\n",
      "        [[[-0.9563]],\n",
      "\n",
      "         [[ 0.2390]],\n",
      "\n",
      "         [[ 1.6804]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.5724]],\n",
      "\n",
      "         [[-1.5701]],\n",
      "\n",
      "         [[-0.7022]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5721]],\n",
      "\n",
      "         [[-0.4373]],\n",
      "\n",
      "         [[ 0.0206]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6248]],\n",
      "\n",
      "         [[-0.7144]],\n",
      "\n",
      "         [[ 0.5023]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.2780]],\n",
      "\n",
      "         [[ 1.1416]],\n",
      "\n",
      "         [[ 0.2850]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7605]],\n",
      "\n",
      "         [[-0.7435]],\n",
      "\n",
      "         [[-0.4236]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0719]],\n",
      "\n",
      "         [[ 0.4641]],\n",
      "\n",
      "         [[-0.2064]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.2150]],\n",
      "\n",
      "         [[ 0.9148]],\n",
      "\n",
      "         [[-0.9402]]],\n",
      "\n",
      "\n",
      "        [[[-0.3104]],\n",
      "\n",
      "         [[ 0.8380]],\n",
      "\n",
      "         [[ 0.1760]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4119]],\n",
      "\n",
      "         [[ 1.4838]],\n",
      "\n",
      "         [[ 0.8694]]]])\n",
      "Layer: unet.up_blocks.1.attentions.0.proj_out.lora.up.weight, Shape: torch.Size([1280, 320, 1, 1])\n",
      "tensor([[[[ 0.9954]],\n",
      "\n",
      "         [[-1.4814]],\n",
      "\n",
      "         [[ 0.4042]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.0362]],\n",
      "\n",
      "         [[ 0.2238]],\n",
      "\n",
      "         [[-0.8166]]],\n",
      "\n",
      "\n",
      "        [[[-0.0572]],\n",
      "\n",
      "         [[ 1.0605]],\n",
      "\n",
      "         [[ 0.3365]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6467]],\n",
      "\n",
      "         [[-0.6257]],\n",
      "\n",
      "         [[ 0.9763]]],\n",
      "\n",
      "\n",
      "        [[[-2.1467]],\n",
      "\n",
      "         [[ 1.0296]],\n",
      "\n",
      "         [[ 0.9110]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0485]],\n",
      "\n",
      "         [[ 0.3188]],\n",
      "\n",
      "         [[ 0.9348]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.3895]],\n",
      "\n",
      "         [[-2.2507]],\n",
      "\n",
      "         [[-0.3994]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2126]],\n",
      "\n",
      "         [[-1.5146]],\n",
      "\n",
      "         [[-0.2344]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0277]],\n",
      "\n",
      "         [[-0.0707]],\n",
      "\n",
      "         [[ 0.3449]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0305]],\n",
      "\n",
      "         [[ 1.5257]],\n",
      "\n",
      "         [[ 0.3689]]],\n",
      "\n",
      "\n",
      "        [[[-0.4207]],\n",
      "\n",
      "         [[ 0.9856]],\n",
      "\n",
      "         [[ 1.1985]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5621]],\n",
      "\n",
      "         [[-0.3452]],\n",
      "\n",
      "         [[ 0.1172]]]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.9149, -0.1738, -1.5221,  ..., -0.0123, -0.3235, -0.1902],\n",
      "        [-1.5199, -0.8707,  1.5471,  ...,  0.5137, -0.1739, -0.8620],\n",
      "        [-2.0453,  0.5171, -0.0490,  ..., -0.1254,  1.5477, -1.9291],\n",
      "        ...,\n",
      "        [ 0.1107, -0.2730, -0.5153,  ..., -0.3546,  0.6514, -0.1260],\n",
      "        [ 0.2490,  0.2001,  0.4091,  ...,  0.4737,  0.6457,  0.3853],\n",
      "        [ 0.2010,  0.5043, -0.6141,  ...,  0.2090,  0.1722, -0.5969]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 0.1764, -0.7218, -0.6683,  ..., -0.4595, -0.1929, -0.2947],\n",
      "        [ 0.7556,  0.3410, -0.7237,  ...,  1.5261,  0.8309,  0.7880],\n",
      "        [ 0.1487,  0.2001,  0.1383,  ..., -0.1522,  1.4927,  0.2573],\n",
      "        ...,\n",
      "        [-1.1927,  0.2938,  1.3548,  ...,  0.5227, -1.4526,  0.0685],\n",
      "        [ 0.3330, -0.6471,  1.1187,  ...,  1.0006,  1.2142,  0.5255],\n",
      "        [ 1.3276, -0.0193,  1.8072,  ..., -0.0738,  0.2960,  0.0952]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.6621, -0.1087,  0.1631,  ..., -1.9974, -1.2058,  0.7434],\n",
      "        [ 0.1035, -2.8203, -0.0956,  ..., -0.6193, -0.1516,  1.4740],\n",
      "        [ 0.0431, -0.4894,  0.7171,  ...,  0.3617, -0.8886, -0.3028],\n",
      "        ...,\n",
      "        [-0.6311,  0.7708,  0.3047,  ..., -0.2273,  0.1902, -0.2279],\n",
      "        [-1.1456,  0.4084,  0.3911,  ...,  1.1343,  2.0450,  1.5610],\n",
      "        [-0.4477, -0.1349, -0.1064,  ..., -0.0170,  0.8662, -1.3354]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-0.9739,  0.2197,  0.2919,  ..., -1.6131, -1.0582,  0.7908],\n",
      "        [ 0.1101, -0.6486,  0.1610,  ...,  0.0106,  0.2801,  0.8191],\n",
      "        [-0.8591,  1.0183, -0.1021,  ...,  0.6937, -0.3595, -0.6012],\n",
      "        ...,\n",
      "        [ 0.7061, -1.5734, -0.2561,  ...,  0.9150,  1.7193, -3.0839],\n",
      "        [-0.1497,  0.9118, -0.0636,  ...,  0.6179, -0.8640, -0.2594],\n",
      "        [ 0.0276,  1.1165,  1.2896,  ..., -1.1820, -0.3787,  0.3856]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[ 1.3084, -0.7008,  0.0151,  ...,  0.2160,  0.0523,  0.0657],\n",
      "        [ 1.3331,  0.4282,  0.3622,  ..., -0.0589,  0.6987,  0.5073],\n",
      "        [-0.0468,  0.4158, -0.1751,  ..., -0.0488, -0.0452, -0.0955],\n",
      "        ...,\n",
      "        [ 0.1391, -0.2404,  0.5139,  ..., -0.1178,  1.0006,  0.4460],\n",
      "        [-0.2366,  0.2044, -0.4503,  ...,  0.1106, -2.4003, -2.8432],\n",
      "        [-1.4330,  0.4258,  0.4134,  ..., -1.4790,  0.6801, -0.1322]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-0.9778, -1.2205,  0.9849,  ..., -0.6827,  1.0005, -0.0326],\n",
      "        [-0.9971, -0.4447,  0.1096,  ..., -0.7183,  1.1795,  0.0870],\n",
      "        [-0.2594,  0.0751,  1.2534,  ...,  0.3324, -1.8102,  0.9153],\n",
      "        ...,\n",
      "        [-0.0833,  0.1817, -0.8707,  ..., -0.1245,  0.5991, -0.4147],\n",
      "        [-0.8309,  0.2898,  0.2635,  ...,  0.9717, -0.3753,  0.6751],\n",
      "        [-1.6303,  0.7178, -1.0663,  ...,  0.2174,  0.5677, -0.0913]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.8315, -0.7130,  0.8990,  ..., -0.5969, -1.2419, -0.1750],\n",
      "        [ 0.8058,  0.4912,  1.2189,  ..., -0.4417, -0.4623, -0.1493],\n",
      "        [-1.4709,  0.8569,  1.1435,  ..., -1.9781,  1.0506, -0.6694],\n",
      "        ...,\n",
      "        [-0.2107,  0.8615, -0.2242,  ...,  0.6904,  0.8192,  0.3790],\n",
      "        [ 1.5818, -0.0090, -0.9303,  ...,  1.5091, -0.4035,  0.1872],\n",
      "        [ 0.0453,  0.2103, -0.3457,  ..., -0.5653, -1.8193, -1.6639]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 0.9233, -0.1098, -0.0422,  ...,  1.4895,  0.3269,  0.3402],\n",
      "        [-0.4413, -0.4600,  1.5943,  ...,  0.2584, -1.3340, -0.6251],\n",
      "        [-0.6656,  0.1048,  1.1461,  ..., -0.9497, -1.3737, -1.1136],\n",
      "        ...,\n",
      "        [ 0.8096, -0.0746,  1.0690,  ...,  0.1278, -0.3393, -0.6090],\n",
      "        [ 0.2140,  0.7124,  0.8288,  ..., -0.7914, -1.3404,  0.6968],\n",
      "        [ 1.8341,  0.1328, -1.8014,  ..., -0.2102, -0.1399, -1.0483]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 0.5556, -0.6145, -1.1921,  ..., -1.2835,  0.6209, -1.2376],\n",
      "        [-0.3950,  1.5960, -0.6134,  ..., -0.5070, -0.6617,  0.0710],\n",
      "        [ 0.6075, -0.2472,  0.1547,  ...,  0.3860, -0.1123,  0.5111],\n",
      "        ...,\n",
      "        [-0.2732, -0.1595,  0.5328,  ..., -0.8353, -0.9038, -1.8808],\n",
      "        [-0.7927, -2.0331,  1.0657,  ..., -0.0601,  1.6825, -2.4536],\n",
      "        [-0.1526, -0.8891,  0.2037,  ...,  1.1375, -0.7658,  0.6416]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-0.5496, -1.7155, -1.0043,  ...,  0.0980, -0.1380,  0.2316],\n",
      "        [-0.8503, -0.5199,  2.0725,  ...,  0.9619, -1.2751, -1.3573],\n",
      "        [-1.4046, -0.2176,  1.5657,  ..., -1.5876,  1.1881, -0.2402],\n",
      "        ...,\n",
      "        [-0.7851,  0.8902, -1.9477,  ...,  2.0063, -0.0199,  1.5091],\n",
      "        [ 1.4524, -0.8784, -0.2717,  ..., -0.2531, -1.4164,  0.6477],\n",
      "        [ 0.5653, -2.3760,  1.0752,  ...,  0.4809,  1.7779,  0.8994]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[ 1.2005, -0.1200, -0.2107,  ..., -0.7673, -0.0782, -0.9563],\n",
      "        [-0.0450,  0.2388,  0.3338,  ...,  0.9546, -0.1090, -0.4650],\n",
      "        [ 0.0206,  1.2784, -0.4628,  ..., -0.8812,  1.8824,  0.8086],\n",
      "        ...,\n",
      "        [ 1.3963, -0.4205,  0.5074,  ..., -1.9134, -1.3817, -1.3892],\n",
      "        [-0.8131,  0.3741, -0.9240,  ..., -0.1829, -2.6477,  1.1681],\n",
      "        [-0.7447,  1.3079,  2.2150,  ...,  1.0502, -0.0251,  0.1908]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-0.2284, -0.7645, -0.2012,  ..., -0.2455,  0.2616, -0.6570],\n",
      "        [ 0.1714, -0.9294, -0.4393,  ...,  0.1923, -0.5642, -0.4827],\n",
      "        [ 0.0359, -0.8799, -1.6440,  ..., -1.0962,  0.2493, -0.7370],\n",
      "        ...,\n",
      "        [ 0.4291, -1.5907,  0.0731,  ...,  1.1596, -0.5038,  1.0392],\n",
      "        [-0.8348,  0.3273,  0.2296,  ..., -0.6953,  0.0896,  0.0059],\n",
      "        [-1.1950, -0.8868, -1.0832,  ..., -0.3038, -0.3080, -0.3976]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.8054, -1.7738, -0.7723,  ...,  0.7761,  0.6137, -0.0075],\n",
      "        [-1.1714,  0.1604, -0.1510,  ...,  0.0157, -0.1947, -0.0975],\n",
      "        [ 0.9303,  1.0196, -0.7771,  ..., -0.3853,  0.7086, -0.0404],\n",
      "        ...,\n",
      "        [ 1.2160,  0.2040, -0.5107,  ...,  0.9367, -2.0286, -2.0486],\n",
      "        [ 0.5105,  0.9841,  1.8166,  ..., -1.1129,  0.3772, -1.1684],\n",
      "        [ 0.1199, -0.2044, -0.4676,  ..., -0.8536, -0.3579,  0.5241]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 0.0286, -0.5638,  0.4235,  ...,  0.7284,  0.7355, -0.7924],\n",
      "        [ 0.1331,  1.2916,  0.5598,  ..., -0.6948,  0.7189, -0.7367],\n",
      "        [ 0.0979, -0.8762, -0.3309,  ...,  0.0238, -0.5847, -0.2715],\n",
      "        ...,\n",
      "        [-0.3232,  0.2505, -0.6040,  ..., -0.4483,  0.4701, -0.7948],\n",
      "        [ 1.2414, -1.3265, -0.9517,  ...,  1.2192,  2.1718, -0.5902],\n",
      "        [-1.0798, -0.9297, -1.9525,  ...,  0.7023,  0.4838, -0.3528]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 0.0443, -0.3159, -2.0116,  ...,  0.8244, -0.4154, -0.3988],\n",
      "        [-1.1662, -2.3281, -1.1561,  ...,  1.3470, -0.3980,  0.0164],\n",
      "        [-0.2572,  1.5778, -0.1339,  ..., -0.5861, -0.8835,  1.7604],\n",
      "        ...,\n",
      "        [-0.6969, -0.4849, -0.8573,  ..., -0.8388,  0.3099, -0.5451],\n",
      "        [-1.8867, -0.0912,  0.0188,  ...,  0.6358, -0.3181,  0.1812],\n",
      "        [ 0.3932,  0.1749,  0.8153,  ...,  0.0974,  0.7269, -0.0973]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-1.1809, -2.1603,  0.1215,  ...,  0.5629, -1.7759,  0.5701],\n",
      "        [ 0.5978,  1.1424,  1.5824,  ...,  1.0586,  0.7392, -1.5714],\n",
      "        [-0.8676, -0.5707, -0.8755,  ...,  0.2190, -0.7116,  0.1421],\n",
      "        ...,\n",
      "        [-0.6080, -2.0491, -0.2975,  ..., -0.4156,  0.4908, -1.7963],\n",
      "        [-0.3829, -1.2493, -1.4192,  ..., -2.3270,  0.5354, -2.3731],\n",
      "        [-0.2367,  0.4980,  0.8580,  ...,  0.8968, -0.9896,  1.4502]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[ 0.4353,  0.1891,  0.4091,  ...,  0.0511, -0.4766,  0.3969],\n",
      "        [-0.0079, -0.1207, -0.0459,  ..., -0.7433, -0.0206, -0.8298],\n",
      "        [-0.5219,  0.5514,  0.5692,  ...,  1.3316, -0.3887, -0.0062],\n",
      "        ...,\n",
      "        [ 0.0789,  0.4439,  0.1485,  ...,  0.2615, -1.6302,  1.6676],\n",
      "        [ 0.4748, -0.1282, -0.0067,  ...,  0.0170, -0.8025, -0.3504],\n",
      "        [ 0.3544,  0.5101,  0.6549,  ...,  1.3075, -0.8576, -0.1025]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shape: torch.Size([10240, 320])\n",
      "tensor([[-0.8389,  0.4289, -0.3448,  ...,  0.6159,  1.9283,  0.0198],\n",
      "        [-0.0283, -0.2275,  0.4559,  ..., -0.1053, -0.8947, -0.6661],\n",
      "        [-0.3766,  1.6437,  1.4545,  ...,  0.4555,  0.0547, -0.5537],\n",
      "        ...,\n",
      "        [ 0.3629,  0.8867, -0.3497,  ...,  0.0041, -1.1063, -1.3233],\n",
      "        [-0.8702,  1.1282,  0.1712,  ..., -0.8313,  0.6959,  0.0520],\n",
      "        [-0.6891, -2.4856,  0.7506,  ..., -0.3005, -0.1220,  0.9044]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight, Shape: torch.Size([320, 5120])\n",
      "tensor([[-1.0189,  0.3422,  0.2320,  ..., -0.4320, -1.0810, -1.1288],\n",
      "        [ 0.4072,  1.7097, -0.8905,  ...,  0.4967, -1.0455, -0.9303],\n",
      "        [-0.6365, -0.0498,  0.6385,  ..., -1.3274,  1.0126,  0.7682],\n",
      "        ...,\n",
      "        [-0.3424, -0.7536, -0.4129,  ..., -0.7112, -0.7231, -0.1830],\n",
      "        [ 1.3169,  0.5740,  0.2851,  ...,  0.0402,  1.3584,  0.2979],\n",
      "        [-0.9210, -1.6027, -0.9919,  ..., -1.6432, -0.2266,  0.1143]])\n",
      "Layer: unet.up_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-1.0734, -1.0495,  0.9742,  ..., -0.2817,  0.2268,  0.8020],\n",
      "        [ 0.3134, -0.3046, -0.8176,  ..., -0.4298, -0.1762, -0.1496],\n",
      "        [ 0.3327,  1.3194, -0.3682,  ...,  0.3983,  1.3932,  0.5718],\n",
      "        ...,\n",
      "        [-0.4082,  1.1889, -0.4012,  ..., -0.4983, -0.0937, -1.4838],\n",
      "        [ 0.1509,  0.2659, -0.0939,  ...,  0.0127, -0.4139,  0.1178],\n",
      "        [-0.6928, -1.5290, -0.5143,  ...,  1.2852,  0.1071,  0.1630]])\n",
      "Layer: unet.up_blocks.1.attentions.1.proj_in.lora.down.weight, Shape: torch.Size([320, 1280, 1, 1])\n",
      "tensor([[[[ 0.0401]],\n",
      "\n",
      "         [[ 2.2798]],\n",
      "\n",
      "         [[-0.0245]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6236]],\n",
      "\n",
      "         [[ 0.3232]],\n",
      "\n",
      "         [[ 0.2858]]],\n",
      "\n",
      "\n",
      "        [[[-0.6914]],\n",
      "\n",
      "         [[-0.9092]],\n",
      "\n",
      "         [[ 0.9163]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.3734]],\n",
      "\n",
      "         [[-1.1303]],\n",
      "\n",
      "         [[ 0.5042]]],\n",
      "\n",
      "\n",
      "        [[[-0.0308]],\n",
      "\n",
      "         [[-0.6865]],\n",
      "\n",
      "         [[ 1.4924]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4561]],\n",
      "\n",
      "         [[-1.3216]],\n",
      "\n",
      "         [[ 1.1562]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.2788]],\n",
      "\n",
      "         [[ 0.3846]],\n",
      "\n",
      "         [[ 0.7031]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6410]],\n",
      "\n",
      "         [[ 0.4465]],\n",
      "\n",
      "         [[ 1.3480]]],\n",
      "\n",
      "\n",
      "        [[[-0.0484]],\n",
      "\n",
      "         [[-0.1203]],\n",
      "\n",
      "         [[ 0.2026]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1864]],\n",
      "\n",
      "         [[ 0.1957]],\n",
      "\n",
      "         [[-0.0198]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9253]],\n",
      "\n",
      "         [[-0.0208]],\n",
      "\n",
      "         [[ 1.8718]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3295]],\n",
      "\n",
      "         [[ 0.5971]],\n",
      "\n",
      "         [[ 1.0411]]]])\n",
      "Layer: unet.up_blocks.1.attentions.1.proj_in.lora.up.weight, Shape: torch.Size([1280, 320, 1, 1])\n",
      "tensor([[[[ 3.9157e-01]],\n",
      "\n",
      "         [[ 1.2241e+00]],\n",
      "\n",
      "         [[-2.0567e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0951e+00]],\n",
      "\n",
      "         [[-5.1852e-01]],\n",
      "\n",
      "         [[-6.2073e-01]]],\n",
      "\n",
      "\n",
      "        [[[-6.2937e-01]],\n",
      "\n",
      "         [[ 1.5730e-01]],\n",
      "\n",
      "         [[ 1.9939e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9162e-01]],\n",
      "\n",
      "         [[-8.7657e-01]],\n",
      "\n",
      "         [[-1.6820e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 5.3712e-04]],\n",
      "\n",
      "         [[-2.0984e+00]],\n",
      "\n",
      "         [[ 1.4202e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8444e+00]],\n",
      "\n",
      "         [[ 3.1075e-01]],\n",
      "\n",
      "         [[-1.4211e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-9.0377e-01]],\n",
      "\n",
      "         [[-9.1144e-01]],\n",
      "\n",
      "         [[ 2.4200e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 7.1841e-02]],\n",
      "\n",
      "         [[ 1.5201e-01]],\n",
      "\n",
      "         [[-1.6086e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5634e+00]],\n",
      "\n",
      "         [[-1.4003e-01]],\n",
      "\n",
      "         [[-1.2272e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.6960e-01]],\n",
      "\n",
      "         [[-5.3897e-02]],\n",
      "\n",
      "         [[ 1.9386e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.3564e-01]],\n",
      "\n",
      "         [[-1.0581e+00]],\n",
      "\n",
      "         [[ 8.6933e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.2553e-01]],\n",
      "\n",
      "         [[-5.8641e-01]],\n",
      "\n",
      "         [[ 7.1792e-01]]]])\n",
      "Layer: unet.up_blocks.1.attentions.1.proj_out.lora.down.weight, Shape: torch.Size([320, 1280, 1, 1])\n",
      "tensor([[[[ 1.0726]],\n",
      "\n",
      "         [[ 0.3670]],\n",
      "\n",
      "         [[-1.0977]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0177]],\n",
      "\n",
      "         [[-0.1966]],\n",
      "\n",
      "         [[ 0.4680]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8438]],\n",
      "\n",
      "         [[ 1.8818]],\n",
      "\n",
      "         [[ 1.5359]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5192]],\n",
      "\n",
      "         [[ 1.1396]],\n",
      "\n",
      "         [[ 0.4499]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7180]],\n",
      "\n",
      "         [[-1.1598]],\n",
      "\n",
      "         [[ 0.3263]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7555]],\n",
      "\n",
      "         [[-1.4672]],\n",
      "\n",
      "         [[-0.3266]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.2226]],\n",
      "\n",
      "         [[ 0.4977]],\n",
      "\n",
      "         [[-0.4353]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.9041]],\n",
      "\n",
      "         [[ 0.2597]],\n",
      "\n",
      "         [[ 0.7473]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5845]],\n",
      "\n",
      "         [[ 0.9291]],\n",
      "\n",
      "         [[-1.0022]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2178]],\n",
      "\n",
      "         [[ 1.2306]],\n",
      "\n",
      "         [[-0.8047]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5247]],\n",
      "\n",
      "         [[-0.0400]],\n",
      "\n",
      "         [[ 1.1456]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4307]],\n",
      "\n",
      "         [[-0.8112]],\n",
      "\n",
      "         [[-2.0174]]]])\n",
      "Layer: unet.up_blocks.1.attentions.1.proj_out.lora.up.weight, Shape: torch.Size([1280, 320, 1, 1])\n",
      "tensor([[[[ 0.3270]],\n",
      "\n",
      "         [[-1.8760]],\n",
      "\n",
      "         [[-0.2390]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3742]],\n",
      "\n",
      "         [[-0.1101]],\n",
      "\n",
      "         [[-1.3870]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1004]],\n",
      "\n",
      "         [[-0.9869]],\n",
      "\n",
      "         [[-0.2173]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2969]],\n",
      "\n",
      "         [[-1.4729]],\n",
      "\n",
      "         [[-0.7474]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8210]],\n",
      "\n",
      "         [[-0.8549]],\n",
      "\n",
      "         [[ 0.1649]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4402]],\n",
      "\n",
      "         [[-0.0935]],\n",
      "\n",
      "         [[ 0.3426]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.2880]],\n",
      "\n",
      "         [[ 1.2747]],\n",
      "\n",
      "         [[ 2.8555]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1287]],\n",
      "\n",
      "         [[ 0.3481]],\n",
      "\n",
      "         [[ 1.3046]]],\n",
      "\n",
      "\n",
      "        [[[-0.4664]],\n",
      "\n",
      "         [[-0.7810]],\n",
      "\n",
      "         [[ 1.8402]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4996]],\n",
      "\n",
      "         [[-0.3241]],\n",
      "\n",
      "         [[ 1.6910]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0448]],\n",
      "\n",
      "         [[-2.4630]],\n",
      "\n",
      "         [[-0.2353]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4847]],\n",
      "\n",
      "         [[-1.4900]],\n",
      "\n",
      "         [[-0.7460]]]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[ 0.2709,  1.6662, -0.8294,  ..., -2.2265,  1.2102,  1.3123],\n",
      "        [-1.0317,  0.7040,  0.0137,  ...,  0.3589,  0.5611,  1.5270],\n",
      "        [-0.3757, -0.4758,  0.6145,  ...,  0.6974, -0.7205, -2.2059],\n",
      "        ...,\n",
      "        [-0.6123, -2.0303, -0.8483,  ..., -0.4637, -1.0106, -1.5882],\n",
      "        [-1.0656, -0.0452, -0.4808,  ..., -1.0336,  0.5240, -0.0654],\n",
      "        [ 0.6014,  1.4871, -1.0731,  ...,  0.8396,  0.6353,  0.4453]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-0.4241, -0.6713,  0.3901,  ...,  0.6786,  1.2303,  1.0159],\n",
      "        [ 1.1991, -0.0350,  0.1739,  ...,  0.8057, -0.7935,  0.1309],\n",
      "        [-0.4456,  0.9267,  0.0909,  ...,  0.0623,  0.7804, -0.2560],\n",
      "        ...,\n",
      "        [-1.9361, -2.1201,  1.8724,  ...,  2.3080,  1.1554, -0.0508],\n",
      "        [-0.4477, -0.1483, -0.8446,  ..., -1.1026,  1.9786,  0.0195],\n",
      "        [-1.2334,  0.2677, -0.8098,  ...,  0.6697,  0.3171,  1.6725]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-1.1394, -0.8835,  0.6513,  ..., -0.8992, -0.8617, -0.2659],\n",
      "        [-0.0402,  0.5626,  0.2414,  ..., -1.5291, -1.8000, -0.8441],\n",
      "        [ 0.2374,  0.0669,  0.2299,  ..., -0.2747,  0.7042,  2.2289],\n",
      "        ...,\n",
      "        [ 0.1952,  0.0446,  0.7052,  ..., -3.2535, -3.0081, -1.3109],\n",
      "        [ 0.4863,  0.4631, -0.3214,  ..., -1.0483, -0.9031, -1.6454],\n",
      "        [-0.5035,  0.6733, -1.2880,  ..., -0.6165,  0.3557,  1.2854]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 1.8072,  0.2293, -1.3807,  ..., -0.6310,  0.7105, -0.5021],\n",
      "        [ 0.5889, -0.1984,  1.4264,  ..., -1.0057,  1.7540,  1.7643],\n",
      "        [-0.1774,  0.0716,  0.6139,  ..., -0.2308, -1.5872,  0.5851],\n",
      "        ...,\n",
      "        [ 0.5751,  0.9045, -0.5241,  ...,  0.3680, -1.2688,  0.5954],\n",
      "        [ 0.1718, -0.4550,  1.2616,  ..., -1.5808,  0.8161, -1.6335],\n",
      "        [ 0.8976,  0.2045, -1.4312,  ...,  0.8184, -0.6080,  0.6120]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.1213,  1.3002, -0.0485,  ...,  0.7495,  0.5497, -0.4962],\n",
      "        [-1.3394, -1.3953,  0.6310,  ...,  0.1564, -1.3437,  0.6136],\n",
      "        [ 0.0215,  0.0532,  0.8998,  ...,  1.6735, -0.0773,  0.4505],\n",
      "        ...,\n",
      "        [-0.7463,  0.4451, -0.1255,  ..., -0.7563,  0.1553, -0.0735],\n",
      "        [-2.1173,  0.1183,  1.7226,  ..., -0.7052, -0.7521, -0.0544],\n",
      "        [ 0.1059,  0.3380,  0.7714,  ..., -0.7525,  0.5724, -1.9355]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-4.5189e-01,  7.3710e-01,  3.2230e-02,  ...,  1.7982e+00,\n",
      "          1.4621e-01, -3.2065e-01],\n",
      "        [ 8.6213e-01, -3.1979e-01,  8.4305e-01,  ...,  2.0493e-01,\n",
      "         -9.3965e-01,  5.9983e-02],\n",
      "        [ 4.3003e-02, -1.9531e-01,  2.1565e-01,  ...,  2.2292e-01,\n",
      "         -1.1301e-03, -6.3951e-01],\n",
      "        ...,\n",
      "        [ 3.6548e+00,  2.1165e+00,  8.4951e-01,  ..., -2.8598e+00,\n",
      "          1.6082e+00, -1.9462e+00],\n",
      "        [ 1.8073e+00,  7.3812e-01,  1.7050e+00,  ..., -3.4901e+00,\n",
      "          1.2655e+00, -1.6705e+00],\n",
      "        [ 2.3090e-01, -1.2910e-01,  2.5900e+00,  ...,  4.8927e-01,\n",
      "          4.2169e+00,  4.1994e+00]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.7764,  0.2066,  0.5710,  ...,  1.1885,  0.1055,  1.5303],\n",
      "        [ 0.9503, -1.2985,  0.6508,  ..., -0.7660, -0.5724,  0.5035],\n",
      "        [-0.4253, -1.0182,  0.4958,  ..., -0.6361, -1.0846, -1.1115],\n",
      "        ...,\n",
      "        [ 0.4203, -0.0259,  0.3720,  ..., -0.3494, -0.1887,  0.1521],\n",
      "        [-0.2494, -1.7124,  0.1368,  ..., -0.9727, -0.9867, -0.9086],\n",
      "        [-0.5235, -0.3374, -0.1773,  ..., -0.4105, -0.7204,  0.1272]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-1.1627, -0.8515, -2.3983,  ...,  0.7029, -1.8909,  0.3253],\n",
      "        [ 1.2797,  0.1105,  0.1905,  ..., -0.9050, -0.3374,  1.4763],\n",
      "        [ 0.6375,  0.9913, -1.2265,  ...,  0.0554,  0.7514, -1.8062],\n",
      "        ...,\n",
      "        [ 0.5206, -1.9995, -1.5409,  ..., -1.2403, -0.4321, -0.5884],\n",
      "        [ 1.3470, -0.7819,  0.4444,  ..., -1.3895,  3.7888,  1.4701],\n",
      "        [-0.2025, -1.0494,  3.0929,  ..., -0.0941, -0.7181, -2.5125]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 0.1604, -1.0502, -2.1207,  ..., -0.1167, -0.1365, -2.4697],\n",
      "        [ 0.6293, -0.5242,  0.1296,  ..., -0.7412, -0.5192,  0.5602],\n",
      "        [ 2.0021,  0.8402, -0.2472,  ...,  0.0688, -0.2023,  0.3614],\n",
      "        ...,\n",
      "        [-1.1787,  0.4289, -0.3019,  ...,  1.1960, -1.0171, -1.0190],\n",
      "        [ 1.6685,  0.1381,  0.9874,  ...,  0.2314,  1.9260,  1.8444],\n",
      "        [ 2.2807, -0.2861,  0.0569,  ...,  0.6250, -0.9981, -0.8196]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 5.6844e-01,  2.4861e-01,  2.1909e-01,  ...,  3.9934e-01,\n",
      "          2.5056e+00,  3.6478e-01],\n",
      "        [-1.8251e-01, -6.3828e-01, -2.3172e+00,  ...,  7.7845e-02,\n",
      "         -1.6522e+00,  4.3697e-01],\n",
      "        [-2.1272e+00, -1.5548e-01,  2.2298e+00,  ...,  1.5688e+00,\n",
      "         -1.0183e+00,  1.9209e-01],\n",
      "        ...,\n",
      "        [-2.1553e+00,  5.3306e-01,  1.3089e+00,  ..., -8.8650e-01,\n",
      "          1.4022e+00, -1.7777e-01],\n",
      "        [ 1.4962e+00,  5.6511e-01,  7.9067e-01,  ..., -7.7834e-01,\n",
      "         -2.0546e-03,  2.4690e-01],\n",
      "        [-8.6703e-01, -7.6153e-01,  4.0537e-01,  ..., -2.3174e-01,\n",
      "          1.1156e+00, -1.7866e+00]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-1.3854,  0.3937, -0.0311,  ...,  0.0710,  0.4533,  1.1733],\n",
      "        [-0.1858, -0.2486,  0.0788,  ...,  0.4123, -0.0674,  0.0029],\n",
      "        [ 1.5649,  0.8324, -0.6230,  ...,  0.3637, -0.1783, -0.5986],\n",
      "        ...,\n",
      "        [ 0.4813,  1.1661,  0.2494,  ..., -0.1978, -0.3619, -0.3035],\n",
      "        [ 0.0750, -0.7648,  1.4681,  ..., -0.8180, -0.2052,  0.2066],\n",
      "        [ 0.4252,  0.7776,  0.1151,  ...,  1.4640, -0.0401,  0.2030]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 0.1818, -0.9426,  0.2561,  ...,  0.7461,  1.0583, -0.1133],\n",
      "        [ 1.6600, -2.8984,  0.1329,  ...,  0.3285, -0.5595,  2.5185],\n",
      "        [-2.2399, -0.3950, -1.3498,  ...,  0.8640,  0.0678, -1.4273],\n",
      "        ...,\n",
      "        [-0.5515, -0.3908, -0.2390,  ..., -0.9633,  0.7265,  1.2499],\n",
      "        [ 0.6770,  0.7047,  1.0080,  ...,  0.9223, -0.8527, -0.5997],\n",
      "        [ 0.0963, -0.0629,  0.2724,  ...,  0.4475,  0.3063,  0.5655]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.8199, -0.8307, -0.0429,  ..., -0.6577,  0.3230,  0.0864],\n",
      "        [-0.5970, -1.0124, -0.0254,  ..., -0.2808,  0.2865,  0.5981],\n",
      "        [-0.6421, -0.5433,  1.0337,  ...,  1.2132,  1.0391,  0.0270],\n",
      "        ...,\n",
      "        [ 0.2983,  0.7044, -0.1030,  ...,  0.4676, -0.2826, -0.5610],\n",
      "        [-1.4248, -0.8254,  1.5111,  ..., -0.3508, -0.0524, -0.2999],\n",
      "        [ 0.3398, -0.1549, -0.3383,  ...,  0.4791,  1.2258, -0.3852]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-1.8010,  0.3926, -0.7966,  ..., -1.0156, -0.8927,  0.2647],\n",
      "        [ 0.7292,  0.6552, -0.2172,  ...,  0.1536, -1.7150, -0.9057],\n",
      "        [-1.5300, -0.4619, -0.0026,  ..., -0.3201, -0.2821,  0.3414],\n",
      "        ...,\n",
      "        [ 2.4131,  0.3521,  0.0801,  ...,  0.1647, -0.1436,  0.0038],\n",
      "        [ 1.1697, -0.2191,  0.3579,  ..., -0.3247,  0.9110,  0.2390],\n",
      "        [ 1.1690, -0.1821,  0.2313,  ..., -0.5137, -1.7858,  0.2234]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 0.7376, -1.8054,  0.6103,  ..., -0.4467,  0.1383, -0.0545],\n",
      "        [ 0.0923, -0.1012, -1.1761,  ..., -0.5253, -0.3767,  0.3056],\n",
      "        [ 0.4753, -1.2223,  1.9748,  ...,  0.5092, -1.2943,  0.7739],\n",
      "        ...,\n",
      "        [ 0.2332, -0.9015, -0.8953,  ..., -0.3870, -0.1163,  1.2172],\n",
      "        [-0.4726, -0.1773, -0.3132,  ..., -0.0565, -0.9267, -0.7017],\n",
      "        [-0.6983,  0.5113, -0.1008,  ...,  0.6228, -1.2547,  0.8698]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-1.0189, -1.1516, -1.0093,  ...,  0.2908,  0.5096, -1.8544],\n",
      "        [-0.4390, -1.1898,  1.2171,  ...,  0.9958, -0.1457, -0.5256],\n",
      "        [ 0.0241,  1.3299,  0.3945,  ..., -1.0293,  0.4256, -0.0843],\n",
      "        ...,\n",
      "        [ 0.2948, -0.2009, -0.5450,  ...,  2.0435, -0.6015,  2.4000],\n",
      "        [-1.1601, -0.6525, -0.3667,  ..., -3.2436,  1.8573, -0.5068],\n",
      "        [ 0.4655, -0.3214,  1.1568,  ...,  1.0243,  0.7425, -1.4955]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-8.1296e-01, -8.8100e-01,  1.1267e+00,  ..., -8.9090e-01,\n",
      "          3.5888e-01,  2.3516e-01],\n",
      "        [ 1.3774e+00,  5.9733e-01, -1.7104e-01,  ...,  6.3280e-01,\n",
      "          4.0558e-04,  5.7329e-01],\n",
      "        [-8.5329e-01, -3.9323e-01, -9.3998e-01,  ...,  1.4013e+00,\n",
      "          1.0780e+00, -1.8816e-01],\n",
      "        ...,\n",
      "        [ 9.9349e-01,  2.0944e+00, -2.4276e+00,  ...,  5.6572e-01,\n",
      "          1.2828e+00, -2.7986e-01],\n",
      "        [-3.2686e-01, -5.8028e-01,  1.4865e-01,  ...,  2.6011e+00,\n",
      "          1.8386e-01,  9.7167e-01],\n",
      "        [ 1.6811e+00,  1.6979e+00, -3.9970e-01,  ..., -9.2972e-01,\n",
      "          1.1457e-01, -2.6049e-01]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shape: torch.Size([10240, 320])\n",
      "tensor([[ 2.5379,  0.5636,  0.4503,  ...,  0.7883,  0.2079,  0.8477],\n",
      "        [-1.2658,  0.0548, -0.8261,  ..., -0.1499, -0.0083, -0.2590],\n",
      "        [-1.1443,  0.7486, -0.3219,  ..., -0.1671,  0.7171, -2.0694],\n",
      "        ...,\n",
      "        [-0.8332, -0.5618,  0.5128,  ..., -1.7076, -0.5722, -0.3302],\n",
      "        [-0.2557, -0.4927,  0.0807,  ..., -0.3041, -1.1252,  0.7400],\n",
      "        [-0.0811, -0.5458,  0.3050,  ..., -0.3903, -0.1083, -0.0159]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight, Shape: torch.Size([320, 5120])\n",
      "tensor([[-1.6272,  0.6451, -0.1322,  ...,  0.9536,  0.4278,  0.8030],\n",
      "        [-1.2351,  0.3149, -1.3701,  ..., -0.9398, -0.7990,  1.2771],\n",
      "        [-0.6174, -0.5887, -1.4818,  ..., -0.4663,  1.0559,  0.9554],\n",
      "        ...,\n",
      "        [-0.3252,  0.6522, -0.5843,  ...,  0.4953, -0.4872, -0.2616],\n",
      "        [ 0.6515,  1.2548, -0.2694,  ...,  0.7692, -0.7626, -1.1896],\n",
      "        [-0.5855,  0.9057, -0.7479,  ..., -1.8266,  2.1240, -0.6629]])\n",
      "Layer: unet.up_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-0.9285, -2.9915, -0.3147,  ..., -1.2317,  1.7059, -0.3084],\n",
      "        [ 0.3258,  1.2806, -0.7905,  ..., -0.0407, -0.9447,  0.5244],\n",
      "        [ 0.0177,  1.5422,  1.1414,  ..., -0.6802, -0.3162, -2.2233],\n",
      "        ...,\n",
      "        [-0.7983,  1.6536,  0.8566,  ...,  0.2037, -1.7322, -0.7204],\n",
      "        [ 0.1513,  0.4861,  2.0070,  ...,  1.5713, -0.0771, -1.1825],\n",
      "        [ 0.8184,  2.4969, -1.6406,  ...,  1.4532,  0.0490, -0.5683]])\n",
      "Layer: unet.up_blocks.1.attentions.2.proj_in.lora.down.weight, Shape: torch.Size([320, 1280, 1, 1])\n",
      "tensor([[[[ 0.4140]],\n",
      "\n",
      "         [[-0.2697]],\n",
      "\n",
      "         [[ 0.8471]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0242]],\n",
      "\n",
      "         [[ 1.2489]],\n",
      "\n",
      "         [[-1.9762]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5197]],\n",
      "\n",
      "         [[-0.5654]],\n",
      "\n",
      "         [[-0.9678]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8726]],\n",
      "\n",
      "         [[-0.5450]],\n",
      "\n",
      "         [[-0.0769]]],\n",
      "\n",
      "\n",
      "        [[[-0.4544]],\n",
      "\n",
      "         [[ 0.9261]],\n",
      "\n",
      "         [[-0.0657]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2857]],\n",
      "\n",
      "         [[ 0.3779]],\n",
      "\n",
      "         [[ 1.5892]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.3974]],\n",
      "\n",
      "         [[ 1.1717]],\n",
      "\n",
      "         [[ 0.0695]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0623]],\n",
      "\n",
      "         [[ 0.1670]],\n",
      "\n",
      "         [[-0.6072]]],\n",
      "\n",
      "\n",
      "        [[[-0.1189]],\n",
      "\n",
      "         [[-0.6319]],\n",
      "\n",
      "         [[-1.7528]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0315]],\n",
      "\n",
      "         [[-0.5512]],\n",
      "\n",
      "         [[ 0.8865]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1955]],\n",
      "\n",
      "         [[ 0.5877]],\n",
      "\n",
      "         [[ 0.4170]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4061]],\n",
      "\n",
      "         [[ 0.2309]],\n",
      "\n",
      "         [[ 0.0633]]]])\n",
      "Layer: unet.up_blocks.1.attentions.2.proj_in.lora.up.weight, Shape: torch.Size([1280, 320, 1, 1])\n",
      "tensor([[[[-3.4743e-01]],\n",
      "\n",
      "         [[-9.0320e-01]],\n",
      "\n",
      "         [[-4.4289e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.9722e-01]],\n",
      "\n",
      "         [[ 6.9768e-01]],\n",
      "\n",
      "         [[ 1.7027e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 7.0349e-01]],\n",
      "\n",
      "         [[ 5.6314e-01]],\n",
      "\n",
      "         [[-9.0727e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2190e+00]],\n",
      "\n",
      "         [[-3.5068e-01]],\n",
      "\n",
      "         [[-9.6764e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.2543e-03]],\n",
      "\n",
      "         [[-9.8538e-01]],\n",
      "\n",
      "         [[-8.7743e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.5454e-01]],\n",
      "\n",
      "         [[ 1.6440e+00]],\n",
      "\n",
      "         [[ 5.7916e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-3.5953e-01]],\n",
      "\n",
      "         [[ 9.8347e-01]],\n",
      "\n",
      "         [[ 1.0571e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2307e-01]],\n",
      "\n",
      "         [[ 7.2126e-03]],\n",
      "\n",
      "         [[ 1.3619e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 2.8947e+00]],\n",
      "\n",
      "         [[ 9.1527e-01]],\n",
      "\n",
      "         [[-1.1997e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8846e+00]],\n",
      "\n",
      "         [[-1.0561e+00]],\n",
      "\n",
      "         [[ 6.2226e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 8.4715e-01]],\n",
      "\n",
      "         [[-7.3544e-01]],\n",
      "\n",
      "         [[-2.8537e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1518e+00]],\n",
      "\n",
      "         [[-7.4798e-01]],\n",
      "\n",
      "         [[-1.0959e+00]]]])\n",
      "Layer: unet.up_blocks.1.attentions.2.proj_out.lora.down.weight, Shape: torch.Size([320, 1280, 1, 1])\n",
      "tensor([[[[ 0.9683]],\n",
      "\n",
      "         [[ 3.0531]],\n",
      "\n",
      "         [[-0.8039]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5602]],\n",
      "\n",
      "         [[-1.3890]],\n",
      "\n",
      "         [[-1.3634]]],\n",
      "\n",
      "\n",
      "        [[[-0.4680]],\n",
      "\n",
      "         [[-1.7566]],\n",
      "\n",
      "         [[-2.0457]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2387]],\n",
      "\n",
      "         [[ 1.1239]],\n",
      "\n",
      "         [[ 0.0623]]],\n",
      "\n",
      "\n",
      "        [[[-0.9038]],\n",
      "\n",
      "         [[ 1.0458]],\n",
      "\n",
      "         [[-1.4004]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2038]],\n",
      "\n",
      "         [[-0.3831]],\n",
      "\n",
      "         [[-1.1381]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0432]],\n",
      "\n",
      "         [[-0.4673]],\n",
      "\n",
      "         [[ 1.5698]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3571]],\n",
      "\n",
      "         [[-0.0268]],\n",
      "\n",
      "         [[-0.0679]]],\n",
      "\n",
      "\n",
      "        [[[-0.1156]],\n",
      "\n",
      "         [[ 0.6112]],\n",
      "\n",
      "         [[-2.1605]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0333]],\n",
      "\n",
      "         [[-1.3515]],\n",
      "\n",
      "         [[-0.6861]]],\n",
      "\n",
      "\n",
      "        [[[-0.1390]],\n",
      "\n",
      "         [[ 0.5510]],\n",
      "\n",
      "         [[-0.2711]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5432]],\n",
      "\n",
      "         [[ 1.0765]],\n",
      "\n",
      "         [[-0.4418]]]])\n",
      "Layer: unet.up_blocks.1.attentions.2.proj_out.lora.up.weight, Shape: torch.Size([1280, 320, 1, 1])\n",
      "tensor([[[[ 0.0982]],\n",
      "\n",
      "         [[-0.4261]],\n",
      "\n",
      "         [[-0.2334]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6222]],\n",
      "\n",
      "         [[-0.8854]],\n",
      "\n",
      "         [[-0.3614]]],\n",
      "\n",
      "\n",
      "        [[[-0.7829]],\n",
      "\n",
      "         [[ 0.2048]],\n",
      "\n",
      "         [[-0.2876]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8006]],\n",
      "\n",
      "         [[ 0.5064]],\n",
      "\n",
      "         [[-0.6592]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4470]],\n",
      "\n",
      "         [[ 2.9334]],\n",
      "\n",
      "         [[-2.0815]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0430]],\n",
      "\n",
      "         [[-0.6072]],\n",
      "\n",
      "         [[-1.7863]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.5140]],\n",
      "\n",
      "         [[ 1.1032]],\n",
      "\n",
      "         [[ 1.6686]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2424]],\n",
      "\n",
      "         [[-0.2661]],\n",
      "\n",
      "         [[ 0.5250]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2516]],\n",
      "\n",
      "         [[ 1.9341]],\n",
      "\n",
      "         [[-1.6420]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0474]],\n",
      "\n",
      "         [[ 0.4312]],\n",
      "\n",
      "         [[-0.4096]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7178]],\n",
      "\n",
      "         [[-0.3561]],\n",
      "\n",
      "         [[ 1.0036]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3886]],\n",
      "\n",
      "         [[ 0.2109]],\n",
      "\n",
      "         [[ 0.0068]]]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.4478, -1.4472, -0.0255,  ...,  1.4011,  0.4190,  1.1424],\n",
      "        [-0.1293,  0.5178, -0.3219,  ..., -0.3758,  0.0126, -0.8253],\n",
      "        [ 0.6218,  0.8527, -0.0433,  ..., -0.2014, -0.3150,  0.5735],\n",
      "        ...,\n",
      "        [ 1.1776, -0.1929, -1.3570,  ..., -0.0619, -1.3212, -1.1202],\n",
      "        [-1.2164,  0.1530, -0.6899,  ..., -1.1220,  0.3641,  0.7906],\n",
      "        [-0.1578, -0.5334,  0.5639,  ..., -0.0741,  0.2073, -0.3435]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-1.4633, -0.1097, -1.0799,  ..., -0.8552,  1.4518, -0.2148],\n",
      "        [-0.8178,  0.0486, -0.3473,  ...,  1.7676,  0.1932, -0.8739],\n",
      "        [-1.9875, -0.4136,  0.3688,  ...,  0.2084, -0.3891, -0.9747],\n",
      "        ...,\n",
      "        [ 0.5760, -0.6604,  0.2478,  ..., -1.3532, -0.0893, -0.9174],\n",
      "        [ 1.6695, -0.1478,  1.8445,  ...,  2.8996, -0.3914, -0.5769],\n",
      "        [ 1.2180,  0.2490,  0.2693,  ...,  0.9927, -0.8087,  0.3599]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-2.2039, -0.2957,  0.2135,  ..., -0.5055, -2.3098, -1.0885],\n",
      "        [ 0.4818,  0.3724, -1.2761,  ..., -0.3463, -1.5252, -0.3383],\n",
      "        [ 0.0600, -0.1040, -0.8533,  ..., -1.0034, -1.8974,  1.1269],\n",
      "        ...,\n",
      "        [ 0.6753, -0.2939,  0.8554,  ...,  0.8105,  1.5100, -0.2774],\n",
      "        [-1.4969,  0.2234, -1.1175,  ..., -0.8482,  1.0243, -0.6608],\n",
      "        [-0.8936,  0.5158, -0.0346,  ...,  0.9387,  1.2529, -0.7419]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-0.2513,  0.9588, -0.9656,  ...,  1.2514, -0.8552,  0.9097],\n",
      "        [ 0.2658,  0.7066,  1.4265,  ..., -0.9339,  1.8899,  1.1403],\n",
      "        [-0.8843, -1.0130, -1.6982,  ..., -1.2956,  0.9873,  0.4533],\n",
      "        ...,\n",
      "        [ 0.1376, -0.4475, -0.7989,  ..., -0.0447, -0.2288,  1.3135],\n",
      "        [ 1.1652,  1.1359,  1.5598,  ...,  1.0173, -0.8805, -0.0885],\n",
      "        [-1.2917,  0.1486, -0.0624,  ...,  0.2767,  1.3949, -1.4469]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-1.0308,  1.0449,  0.1737,  ..., -0.8877, -0.0318, -0.5370],\n",
      "        [-0.4202, -0.3480, -0.6876,  ...,  1.1067,  0.5534,  0.0466],\n",
      "        [ 0.1420,  0.4360, -0.4171,  ...,  0.4169, -0.3228, -0.3199],\n",
      "        ...,\n",
      "        [ 0.5684,  1.3423,  0.8836,  ...,  1.1321,  1.8423, -0.8345],\n",
      "        [ 1.6481, -2.2446, -0.3890,  ...,  1.2911, -0.6250, -0.3797],\n",
      "        [-0.6211, -0.0718, -0.7022,  ..., -0.3227, -0.5578, -0.4105]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-0.9594, -0.4628, -0.2171,  ...,  2.2133,  0.3686,  0.2485],\n",
      "        [ 0.2718, -0.0264, -0.4964,  ..., -0.1264,  3.8924, -0.7223],\n",
      "        [-0.6743, -0.5576,  1.0771,  ...,  0.2489, -2.2662,  0.6477],\n",
      "        ...,\n",
      "        [ 0.2436,  0.0234,  0.1789,  ...,  1.5422, -0.2681,  0.5879],\n",
      "        [ 0.8564,  0.4570,  0.7231,  ...,  0.7439, -3.0968,  0.8676],\n",
      "        [-1.0737,  0.0074, -0.6327,  ...,  0.8795,  0.2374,  0.4388]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.1326, -0.2246, -0.5261,  ..., -0.5925,  0.9890, -1.4828],\n",
      "        [ 1.2209,  0.4426, -0.1472,  ..., -0.1796,  0.2978,  0.9239],\n",
      "        [-1.0030, -0.3249,  0.7939,  ...,  0.1007, -1.1986,  0.7315],\n",
      "        ...,\n",
      "        [ 0.6074,  0.7575, -0.0312,  ...,  0.0037, -0.8507,  0.5342],\n",
      "        [-0.7583, -0.3961,  0.6960,  ..., -1.2302, -0.2519,  0.0556],\n",
      "        [ 0.6504,  1.1766, -0.0359,  ...,  2.8456,  0.4559,  0.2421]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-1.6658, -0.6483,  0.0091,  ...,  0.4877, -3.0438,  0.6936],\n",
      "        [ 0.6594, -0.4909,  0.1388,  ..., -0.3246,  1.4589, -0.1883],\n",
      "        [-0.5749,  0.9361, -1.0123,  ...,  1.0307,  0.7118, -0.1737],\n",
      "        ...,\n",
      "        [-0.3468, -0.6475,  0.1872,  ..., -0.3573,  1.2715, -1.0662],\n",
      "        [-0.3717,  0.4554, -2.2583,  ..., -0.6876, -0.1982,  1.0476],\n",
      "        [-0.8807,  0.7133, -2.0607,  ..., -0.8342,  0.5587,  0.0991]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[-0.2030,  0.2758,  0.0757,  ...,  0.3566, -0.9835, -1.0033],\n",
      "        [-0.0072,  1.0193,  0.8197,  ...,  0.3532, -1.3113, -0.6267],\n",
      "        [ 0.6342,  2.1697,  1.2658,  ...,  1.2705,  0.8107,  0.6544],\n",
      "        ...,\n",
      "        [ 1.6171,  0.1100,  0.4687,  ..., -0.1549,  0.1084,  0.3309],\n",
      "        [ 0.2260, -1.4164, -1.0108,  ...,  1.7148, -2.4607, -0.4954],\n",
      "        [-1.3949, -0.1133,  1.0122,  ..., -1.8437, -0.9758,  0.6459]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-0.4455,  0.9191,  0.9792,  ...,  0.6300, -0.4434,  0.3162],\n",
      "        [ 0.0063,  0.2662,  1.0118,  ...,  1.4642,  0.2982,  0.5783],\n",
      "        [ 0.7959, -0.1793,  0.3189,  ...,  0.4853, -0.2120,  0.9072],\n",
      "        ...,\n",
      "        [ 0.0127,  0.0166, -1.0583,  ...,  0.4260,  0.9638,  0.1476],\n",
      "        [-0.2817, -0.2334, -0.6611,  ..., -0.7098,  0.3319,  0.4943],\n",
      "        [-0.2559, -0.6933, -1.0787,  ..., -0.4895,  0.9284, -0.1391]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[ 0.8149, -0.2440,  0.2353,  ..., -2.6127,  0.5259,  0.1869],\n",
      "        [-0.5456,  0.0227, -0.2288,  ...,  0.9024,  0.4663,  0.8974],\n",
      "        [ 0.1334,  1.4054,  1.2907,  ...,  1.2811,  0.7439, -0.0506],\n",
      "        ...,\n",
      "        [-0.5099,  0.5538, -0.2907,  ...,  0.2267, -0.7133, -0.0735],\n",
      "        [ 0.8913,  0.6650,  1.2784,  ..., -1.7081, -1.4628,  0.1799],\n",
      "        [ 0.7520,  0.8308,  0.0260,  ..., -0.9159, -0.0126, -0.4845]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 2.2766e+00,  5.7640e-01,  1.0428e+00,  ..., -1.7712e-05,\n",
      "         -4.4261e-01, -3.4380e-01],\n",
      "        [ 1.6165e+00, -7.6066e-01, -6.9257e-01,  ...,  1.0101e+00,\n",
      "          8.0231e-02,  1.7815e-01],\n",
      "        [ 9.2765e-01, -6.0766e-02, -2.0449e+00,  ..., -4.8971e-01,\n",
      "         -2.4186e-01,  9.1263e-01],\n",
      "        ...,\n",
      "        [-1.4154e+00,  7.8972e-01,  1.0573e-01,  ..., -8.4052e-02,\n",
      "          3.4358e-01,  7.6703e-01],\n",
      "        [ 1.1212e+00,  2.7092e-01, -2.1518e+00,  ...,  5.8062e-01,\n",
      "          8.2561e-01, -6.5206e-01],\n",
      "        [-1.4133e+00,  4.3501e-01,  2.7565e-01,  ...,  8.9126e-02,\n",
      "          9.1622e-01, -3.6630e-01]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.8444,  0.9369,  0.6394,  ..., -0.2341,  0.6393,  1.1445],\n",
      "        [ 1.6383, -0.2522, -0.1917,  ...,  0.1218,  0.6750,  0.8148],\n",
      "        [ 0.0397,  0.9797,  0.2345,  ..., -0.0142, -0.3838,  0.7530],\n",
      "        ...,\n",
      "        [-0.4020, -0.2763,  1.1550,  ...,  0.3569, -0.9012,  0.5719],\n",
      "        [-1.0843,  0.0945, -0.3985,  ..., -1.4937, -1.3669,  1.5925],\n",
      "        [ 0.4188,  0.4868,  0.0179,  ...,  0.5139,  0.0452, -0.8357]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-1.1559, -0.2776,  0.4500,  ..., -0.9071, -0.6797, -0.0152],\n",
      "        [-0.2339,  0.0123,  1.1004,  ..., -0.2149,  0.1391,  0.2758],\n",
      "        [-0.9190, -0.5468, -1.0721,  ..., -0.0604,  1.3581,  0.1044],\n",
      "        ...,\n",
      "        [ 0.6575, -0.0671, -0.3080,  ..., -0.7642, -0.9928, -0.0065],\n",
      "        [ 1.5540,  0.1591, -0.3002,  ...,  0.7598, -1.9777, -0.2683],\n",
      "        [-0.9645,  0.3398,  0.2884,  ..., -0.3091,  0.5495, -0.1260]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 4.3005e-01,  3.0139e-01,  6.0084e-01,  ..., -1.2320e-01,\n",
      "          1.2736e+00,  2.4306e-01],\n",
      "        [ 4.9745e-01,  1.0916e+00,  5.4273e-01,  ..., -5.7270e-01,\n",
      "         -9.2278e-01, -1.1313e+00],\n",
      "        [ 1.6473e-01,  3.8590e-01, -4.7398e-02,  ...,  6.3895e-01,\n",
      "          2.0367e-01,  9.7008e-01],\n",
      "        ...,\n",
      "        [ 1.0487e-01, -1.5013e-01,  5.1022e-02,  ..., -8.5012e-01,\n",
      "          5.9941e-01,  8.9686e-01],\n",
      "        [-9.5451e-01, -4.2228e+00, -1.1474e+00,  ..., -1.1562e-01,\n",
      "         -1.1852e+00, -7.9548e-01],\n",
      "        [-7.5046e-01,  1.3263e+00,  1.3804e+00,  ...,  3.1399e-03,\n",
      "          2.2761e-01,  1.0018e-01]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[ 0.6112,  0.2118,  0.1055,  ..., -0.0161, -0.6629, -0.4214],\n",
      "        [ 0.5309, -0.4067,  1.2180,  ...,  0.6110,  1.5128, -1.5551],\n",
      "        [-0.4216, -0.4821,  1.1399,  ...,  0.7715, -0.2931,  1.0070],\n",
      "        ...,\n",
      "        [-0.2086,  0.5747,  0.3995,  ...,  1.8076, -0.3617,  1.3234],\n",
      "        [ 1.0505, -0.2395,  1.9056,  ..., -0.0782, -1.2288, -1.2286],\n",
      "        [ 0.4813, -2.4569,  0.3740,  ..., -0.6961, -0.7091,  0.3136]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[ 0.1356,  1.5130,  0.0980,  ...,  1.9120,  0.9307, -0.9215],\n",
      "        [-0.5302,  1.7126,  0.4374,  ..., -0.0863,  0.6356, -0.6114],\n",
      "        [-0.4454,  3.1299,  0.7189,  ...,  1.6418,  1.5917, -1.7605],\n",
      "        ...,\n",
      "        [-1.4119, -0.2800, -0.3153,  ..., -0.2966, -0.2896,  1.5436],\n",
      "        [-0.8432, -0.2410, -0.3559,  ...,  1.4489,  0.3698, -1.7509],\n",
      "        [ 1.5852,  0.4615,  0.4721,  ...,  0.6386,  0.3020, -0.7229]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shape: torch.Size([10240, 320])\n",
      "tensor([[-9.2131e-01,  1.2017e+00, -2.4279e+00,  ..., -8.1257e-01,\n",
      "         -5.0242e-01, -1.0487e+00],\n",
      "        [-9.3299e-01,  9.1017e-01, -8.6130e-01,  ...,  1.1559e-01,\n",
      "         -5.4480e-01, -5.7541e-02],\n",
      "        [-5.0383e-01,  2.1486e-01,  4.0947e-01,  ..., -5.0402e-01,\n",
      "         -2.8936e-01, -4.9702e-01],\n",
      "        ...,\n",
      "        [ 1.7000e+00, -5.9283e-03, -2.2752e-01,  ..., -2.9810e-03,\n",
      "         -1.6985e-02,  4.8199e-01],\n",
      "        [-8.3152e+00,  1.9407e+00,  1.3966e+00,  ..., -6.0690e+00,\n",
      "         -1.1205e+01, -1.1838e+01],\n",
      "        [ 3.6380e-01, -1.7850e-01,  2.3678e+00,  ..., -6.0851e-01,\n",
      "         -7.4247e-01, -5.9307e-01]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.lora.down.weight, Shape: torch.Size([320, 5120])\n",
      "tensor([[-3.4601e-01, -1.6217e+00,  1.0667e+00,  ..., -7.8216e-01,\n",
      "         -3.7618e+00,  3.6874e-01],\n",
      "        [ 1.5517e+00,  5.0994e-01,  1.1429e+00,  ..., -8.9463e-01,\n",
      "          2.2259e+00,  9.4693e-02],\n",
      "        [ 4.7539e-01,  1.4444e+00,  7.5805e-01,  ..., -1.4573e+00,\n",
      "         -1.1314e+00, -1.6811e+00],\n",
      "        ...,\n",
      "        [ 3.6558e-01,  1.4681e+00,  3.7030e-01,  ..., -9.9307e-01,\n",
      "          9.0894e+00,  6.5204e-02],\n",
      "        [ 1.1001e+00, -8.0881e-01,  7.8171e-01,  ..., -8.0137e-01,\n",
      "         -6.3897e+00,  5.3010e-01],\n",
      "        [ 1.5279e+00, -3.3691e-03,  7.5100e-01,  ..., -1.5336e+00,\n",
      "         -2.4772e+00, -1.5892e-01]])\n",
      "Layer: unet.up_blocks.1.attentions.2.transformer_blocks.0.ff.net.2.lora.up.weight, Shape: torch.Size([1280, 320])\n",
      "tensor([[-1.5446, -0.8340,  1.3807,  ...,  0.5599,  0.3511, -1.2482],\n",
      "        [-0.8229,  0.8276, -0.8974,  ..., -0.8737,  1.0114,  1.3121],\n",
      "        [-1.4502, -0.7998,  0.1582,  ..., -0.1145, -0.8387, -0.1609],\n",
      "        ...,\n",
      "        [-0.0122, -0.0985, -2.0266,  ...,  1.4763, -0.2569,  0.6995],\n",
      "        [ 0.7472,  0.1964, -0.5959,  ..., -0.3258,  2.0379,  0.1721],\n",
      "        [-0.0947, -0.5931, -0.1265,  ...,  0.1931, -1.1257,  1.3342]])\n",
      "Layer: unet.up_blocks.2.attentions.0.proj_in.lora.down.weight, Shape: torch.Size([320, 640, 1, 1])\n",
      "tensor([[[[ 0.4033]],\n",
      "\n",
      "         [[-0.4427]],\n",
      "\n",
      "         [[-0.1298]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5320]],\n",
      "\n",
      "         [[ 0.7019]],\n",
      "\n",
      "         [[ 1.0443]]],\n",
      "\n",
      "\n",
      "        [[[-0.7951]],\n",
      "\n",
      "         [[-2.5104]],\n",
      "\n",
      "         [[-0.6233]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1538]],\n",
      "\n",
      "         [[-0.4143]],\n",
      "\n",
      "         [[ 1.3178]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2637]],\n",
      "\n",
      "         [[ 0.1447]],\n",
      "\n",
      "         [[ 0.5494]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2551]],\n",
      "\n",
      "         [[ 0.6663]],\n",
      "\n",
      "         [[-1.0414]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0853]],\n",
      "\n",
      "         [[ 1.1010]],\n",
      "\n",
      "         [[-1.2254]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5091]],\n",
      "\n",
      "         [[-0.2149]],\n",
      "\n",
      "         [[-0.2976]]],\n",
      "\n",
      "\n",
      "        [[[-0.6127]],\n",
      "\n",
      "         [[-1.6839]],\n",
      "\n",
      "         [[-0.9836]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4768]],\n",
      "\n",
      "         [[ 0.0516]],\n",
      "\n",
      "         [[-0.7940]]],\n",
      "\n",
      "\n",
      "        [[[-0.1196]],\n",
      "\n",
      "         [[ 0.3525]],\n",
      "\n",
      "         [[-0.6212]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3851]],\n",
      "\n",
      "         [[-1.1783]],\n",
      "\n",
      "         [[ 2.2118]]]])\n",
      "Layer: unet.up_blocks.2.attentions.0.proj_in.lora.up.weight, Shape: torch.Size([640, 320, 1, 1])\n",
      "tensor([[[[ 0.2843]],\n",
      "\n",
      "         [[-0.0083]],\n",
      "\n",
      "         [[-0.7356]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2103]],\n",
      "\n",
      "         [[ 1.1328]],\n",
      "\n",
      "         [[ 0.8864]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4672]],\n",
      "\n",
      "         [[ 0.4792]],\n",
      "\n",
      "         [[ 1.4445]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5358]],\n",
      "\n",
      "         [[-0.9391]],\n",
      "\n",
      "         [[-0.2189]]],\n",
      "\n",
      "\n",
      "        [[[-0.8069]],\n",
      "\n",
      "         [[ 0.2957]],\n",
      "\n",
      "         [[ 1.5746]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0341]],\n",
      "\n",
      "         [[-0.3429]],\n",
      "\n",
      "         [[-0.9033]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0038]],\n",
      "\n",
      "         [[-0.3341]],\n",
      "\n",
      "         [[ 1.1630]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6483]],\n",
      "\n",
      "         [[-0.7570]],\n",
      "\n",
      "         [[ 0.0132]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0285]],\n",
      "\n",
      "         [[ 1.0339]],\n",
      "\n",
      "         [[ 0.0151]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4181]],\n",
      "\n",
      "         [[ 1.8412]],\n",
      "\n",
      "         [[ 0.5082]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7386]],\n",
      "\n",
      "         [[-0.0133]],\n",
      "\n",
      "         [[ 0.8738]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4193]],\n",
      "\n",
      "         [[-0.7870]],\n",
      "\n",
      "         [[ 1.1103]]]])\n",
      "Layer: unet.up_blocks.2.attentions.0.proj_out.lora.down.weight, Shape: torch.Size([320, 640, 1, 1])\n",
      "tensor([[[[ 0.0772]],\n",
      "\n",
      "         [[-0.2000]],\n",
      "\n",
      "         [[ 0.5783]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.8737]],\n",
      "\n",
      "         [[-0.2989]],\n",
      "\n",
      "         [[-0.4672]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6141]],\n",
      "\n",
      "         [[ 0.4975]],\n",
      "\n",
      "         [[-2.1028]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.2711]],\n",
      "\n",
      "         [[-0.2586]],\n",
      "\n",
      "         [[ 1.4641]]],\n",
      "\n",
      "\n",
      "        [[[-0.9967]],\n",
      "\n",
      "         [[ 1.8527]],\n",
      "\n",
      "         [[ 0.8829]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4405]],\n",
      "\n",
      "         [[-0.0190]],\n",
      "\n",
      "         [[-0.6144]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.4633]],\n",
      "\n",
      "         [[-0.2106]],\n",
      "\n",
      "         [[-0.2147]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.6044]],\n",
      "\n",
      "         [[-0.8169]],\n",
      "\n",
      "         [[ 1.8429]]],\n",
      "\n",
      "\n",
      "        [[[-0.4601]],\n",
      "\n",
      "         [[-3.6159]],\n",
      "\n",
      "         [[-2.1823]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8186]],\n",
      "\n",
      "         [[-0.4801]],\n",
      "\n",
      "         [[ 1.1399]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1006]],\n",
      "\n",
      "         [[ 1.6066]],\n",
      "\n",
      "         [[ 0.1038]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1249]],\n",
      "\n",
      "         [[ 0.9414]],\n",
      "\n",
      "         [[ 0.0062]]]])\n",
      "Layer: unet.up_blocks.2.attentions.0.proj_out.lora.up.weight, Shape: torch.Size([640, 320, 1, 1])\n",
      "tensor([[[[ 1.5840]],\n",
      "\n",
      "         [[ 0.6933]],\n",
      "\n",
      "         [[ 0.1612]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.7192]],\n",
      "\n",
      "         [[ 0.4303]],\n",
      "\n",
      "         [[-1.8279]]],\n",
      "\n",
      "\n",
      "        [[[-1.6098]],\n",
      "\n",
      "         [[-1.4744]],\n",
      "\n",
      "         [[-0.1976]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3272]],\n",
      "\n",
      "         [[-0.6610]],\n",
      "\n",
      "         [[ 0.7126]]],\n",
      "\n",
      "\n",
      "        [[[-0.6601]],\n",
      "\n",
      "         [[ 2.2167]],\n",
      "\n",
      "         [[-0.4784]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1946]],\n",
      "\n",
      "         [[ 0.5411]],\n",
      "\n",
      "         [[-0.2277]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0767]],\n",
      "\n",
      "         [[-0.3828]],\n",
      "\n",
      "         [[-1.2015]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6045]],\n",
      "\n",
      "         [[ 1.5024]],\n",
      "\n",
      "         [[-0.6304]]],\n",
      "\n",
      "\n",
      "        [[[-0.3442]],\n",
      "\n",
      "         [[-0.5287]],\n",
      "\n",
      "         [[-0.2970]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2774]],\n",
      "\n",
      "         [[ 1.0590]],\n",
      "\n",
      "         [[-0.6592]]],\n",
      "\n",
      "\n",
      "        [[[-0.3207]],\n",
      "\n",
      "         [[-0.2691]],\n",
      "\n",
      "         [[-0.4545]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8240]],\n",
      "\n",
      "         [[ 0.4847]],\n",
      "\n",
      "         [[ 0.9514]]]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 0.2142,  1.2821,  0.6380,  ...,  0.3817,  1.3657, -0.8707],\n",
      "        [-0.0767, -0.6824,  0.5697,  ...,  0.2897, -0.8073, -1.2462],\n",
      "        [ 1.4220, -0.7476,  0.6040,  ..., -1.0549, -0.3657, -0.3935],\n",
      "        ...,\n",
      "        [ 0.5462, -0.5013,  1.0854,  ..., -0.6649,  0.1311,  0.9214],\n",
      "        [ 0.6442,  0.6934, -1.0846,  ...,  0.0424,  0.9192, -0.0541],\n",
      "        [-0.3070,  0.6123, -1.7655,  ...,  0.6584,  0.7256,  0.2187]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-1.2491,  0.0041,  0.2104,  ...,  1.1995,  0.2606,  0.4799],\n",
      "        [ 1.2841,  0.3012, -0.6824,  ...,  0.0960,  2.0459,  0.0272],\n",
      "        [ 0.7377,  0.2290, -1.5716,  ...,  0.9050, -1.3037, -0.2244],\n",
      "        ...,\n",
      "        [ 0.5649, -0.2313,  0.4940,  ..., -1.0600,  0.0716,  0.6018],\n",
      "        [ 1.3083, -1.2378, -0.1138,  ..., -0.4127, -0.3694,  0.0585],\n",
      "        [-0.0736, -0.8301,  0.5643,  ...,  1.0060,  0.9613, -0.6232]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[-0.1459, -0.7541,  1.5109,  ...,  3.0131, -1.6081,  0.2072],\n",
      "        [-0.4545,  0.5055, -0.4289,  ...,  0.1929, -0.0819,  1.9154],\n",
      "        [ 0.9776, -0.4599, -0.0215,  ..., -1.0606,  0.1454,  0.8779],\n",
      "        ...,\n",
      "        [ 1.1006,  1.1130, -0.8808,  ...,  1.6063, -0.0926, -0.8581],\n",
      "        [ 0.8262,  0.1731,  0.9380,  ..., -0.4819, -0.1803, -0.2221],\n",
      "        [ 0.5097, -0.4945,  0.5431,  ..., -1.6626,  1.0875,  0.9152]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 1.2427,  0.4342, -1.4448,  ..., -0.1860, -0.4254, -1.8740],\n",
      "        [-0.7274,  1.4650, -0.1352,  ..., -1.1434, -0.6771,  0.9075],\n",
      "        [ 1.2023,  1.1755, -1.2688,  ...,  2.0648,  0.7115,  1.5364],\n",
      "        ...,\n",
      "        [ 0.8771, -1.0269,  1.0968,  ...,  1.1947, -2.5464, -0.7288],\n",
      "        [ 0.2470,  0.1543, -0.8647,  ...,  0.5626,  0.7620, -0.5160],\n",
      "        [ 1.5599,  1.3965, -0.9237,  ...,  0.7188, -0.3150,  1.0714]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[-0.8805,  2.2148, -0.8017,  ..., -0.3007, -0.0167,  1.1569],\n",
      "        [ 0.1137, -0.9922,  0.1035,  ...,  0.2336, -0.0508,  0.2431],\n",
      "        [ 0.2949,  0.3275,  0.1260,  ...,  0.2828,  0.3554, -1.6893],\n",
      "        ...,\n",
      "        [-0.9198,  0.4919,  0.2048,  ..., -0.0845, -0.1425, -0.2446],\n",
      "        [ 1.0064,  0.5888,  0.0647,  ..., -2.1395,  0.7617,  0.7067],\n",
      "        [ 0.6111,  0.0976, -0.9806,  ...,  0.5628,  1.7037, -0.4869]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-0.0569, -0.2488,  0.2267,  ..., -0.4818,  2.1863,  0.4198],\n",
      "        [ 0.1161,  0.1602, -0.4539,  ..., -1.9347, -0.7312,  0.9688],\n",
      "        [ 0.4768, -1.3381, -0.0745,  ...,  1.2750,  0.4275,  0.0349],\n",
      "        ...,\n",
      "        [ 1.1092, -1.0762, -0.7836,  ..., -0.5000,  0.2975,  1.0173],\n",
      "        [-0.3629, -0.5149,  0.3333,  ..., -1.0450,  0.2868,  0.4485],\n",
      "        [-0.0321, -0.6177, -1.3842,  ..., -0.2760, -0.9788, -0.3401]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 0.8087,  0.1810,  1.1183,  ...,  0.3542, -0.2957, -0.6852],\n",
      "        [-0.4126,  0.1022,  0.1040,  ..., -0.2073,  0.9780, -0.1191],\n",
      "        [ 0.1614,  1.2684,  0.3241,  ...,  0.4078,  0.2989, -0.5967],\n",
      "        ...,\n",
      "        [-0.8970, -1.1515, -0.2155,  ...,  1.6585, -2.2967,  1.0521],\n",
      "        [-2.0829,  0.9323,  0.6170,  ..., -0.5700,  0.5241, -1.0316],\n",
      "        [-0.0194,  0.2776,  0.5171,  ...,  1.0976, -1.3765,  0.0554]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-2.1091, -0.1501, -0.5548,  ..., -0.9338,  1.9868, -1.0464],\n",
      "        [ 1.2112,  0.5158,  0.3985,  ..., -0.4052,  0.7448,  0.1922],\n",
      "        [ 0.3271,  0.9653,  0.0293,  ..., -0.6650, -0.0630,  0.8027],\n",
      "        ...,\n",
      "        [-0.0409, -0.2658, -0.9937,  ...,  0.7909,  1.2320,  0.0900],\n",
      "        [-1.2142,  1.0840, -0.7678,  ..., -0.1500, -0.4327, -0.8366],\n",
      "        [-0.2256,  1.1626, -1.4052,  ..., -2.2277,  0.2783,  0.8183]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[-3.5674, -0.4592,  1.3608,  ..., -1.2632,  0.2329,  0.4308],\n",
      "        [-2.3700,  1.4699, -0.7734,  ...,  0.1475, -1.0654, -0.2329],\n",
      "        [ 0.5987, -0.1203, -0.5085,  ...,  0.1447,  0.4766,  0.8695],\n",
      "        ...,\n",
      "        [-1.9749, -1.0880, -1.1215,  ...,  0.7545,  0.3436,  0.9635],\n",
      "        [ 0.2297,  0.7687,  1.2024,  ..., -1.1072, -0.2729,  0.5601],\n",
      "        [-1.1182,  0.1470,  0.5818,  ...,  0.6252, -1.2869, -0.6450]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-0.0796,  0.5112,  0.2562,  ...,  0.0046,  0.9685,  0.1797],\n",
      "        [-0.0565, -0.2219,  0.1085,  ...,  0.1033, -0.9753, -0.3226],\n",
      "        [-0.7387,  0.0923, -0.0737,  ...,  0.2188,  0.3018,  0.5232],\n",
      "        ...,\n",
      "        [-0.1663,  0.5574, -0.0315,  ...,  0.2799,  1.0509, -0.0278],\n",
      "        [ 0.3357, -0.6459,  0.1088,  ...,  0.1399, -0.6093, -0.1463],\n",
      "        [ 0.0773,  0.2887, -0.4328,  ...,  0.4146, -1.9322, -0.2785]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[-0.3173, -2.3295,  0.2916,  ...,  0.0259,  0.8468, -0.3720],\n",
      "        [-1.0319, -0.6096,  0.2097,  ..., -0.9865, -0.2975, -0.4603],\n",
      "        [ 0.4218,  0.1657,  0.3994,  ...,  0.8530,  0.9479, -0.4490],\n",
      "        ...,\n",
      "        [-0.7135,  0.6248,  0.7403,  ...,  0.1869,  0.1881, -1.0493],\n",
      "        [ 1.6370,  0.3030,  1.3062,  ..., -0.9457,  0.0761, -0.3843],\n",
      "        [-0.1859, -2.2364,  0.5184,  ...,  0.8795, -0.7243,  0.3032]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 7.8794e-01, -1.0451e+00,  6.3704e-01,  ...,  6.0776e-01,\n",
      "         -1.0075e+00,  1.5314e-01],\n",
      "        [-9.8646e-01,  1.1882e+00,  8.4114e-01,  ...,  2.5191e-01,\n",
      "          1.4624e+00,  9.5015e-02],\n",
      "        [-4.5243e-01,  1.3159e+00, -1.2754e+00,  ...,  4.4154e-01,\n",
      "          6.9046e-01,  4.4994e-01],\n",
      "        ...,\n",
      "        [-1.3319e+00,  9.8494e-01, -1.8821e+00,  ..., -1.5420e-01,\n",
      "         -1.2068e+00, -1.0377e-01],\n",
      "        [-1.2075e+00,  1.6621e+00,  2.2814e-01,  ..., -1.1979e+00,\n",
      "         -1.2705e+00, -1.2372e+00],\n",
      "        [ 1.5188e-01,  1.0574e+00,  5.2284e-04,  ..., -8.8234e-01,\n",
      "         -3.7421e-01,  7.8963e-01]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 1.9830, -0.0814,  0.6001,  ...,  0.9843, -0.0132, -0.0848],\n",
      "        [-0.3727,  0.0409, -0.7806,  ..., -0.1673,  0.8172, -0.5299],\n",
      "        [ 1.0589, -0.3033, -0.7522,  ...,  1.3367,  1.3255,  0.1414],\n",
      "        ...,\n",
      "        [ 1.3364, -0.7909,  1.7611,  ...,  0.0865,  0.6489,  0.8890],\n",
      "        [-1.5733, -0.0254, -0.7403,  ..., -1.0413,  1.1571, -0.5836],\n",
      "        [-0.2729,  1.8307, -0.2345,  ..., -0.8070, -0.9402,  0.6202]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-1.0795, -0.2786,  0.3341,  ..., -0.4164, -1.1447, -0.1916],\n",
      "        [-0.5050,  0.3323, -0.1869,  ...,  0.2364,  0.6483,  0.2507],\n",
      "        [-0.2237, -0.1029,  0.0221,  ..., -0.0649, -2.1210,  0.2853],\n",
      "        ...,\n",
      "        [ 0.0634, -0.0989, -0.1345,  ...,  0.2842, -0.5285,  0.4276],\n",
      "        [-0.0461,  0.3575, -0.4608,  ...,  0.2962,  1.0021,  0.2103],\n",
      "        [-0.3261, -0.2392, -0.3891,  ..., -0.3785,  1.0170,  0.2971]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 1.1259, -1.4016, -0.9429,  ..., -0.5367,  0.3805, -1.3960],\n",
      "        [-0.9488,  0.0573,  1.1433,  ..., -0.7692, -0.1903,  1.1516],\n",
      "        [ 0.1841,  0.5765, -0.1007,  ...,  0.7753,  0.5726,  0.6009],\n",
      "        ...,\n",
      "        [-2.1382,  0.3846, -2.0696,  ...,  1.3727, -0.0374,  0.1756],\n",
      "        [ 0.5012,  1.6874, -0.8387,  ...,  1.5417, -1.7704, -2.7979],\n",
      "        [ 1.2911, -0.7988, -0.1844,  ...,  0.2914, -0.8045,  0.3373]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.2790, -0.9535,  0.4475,  ...,  1.1821,  1.1073, -0.7395],\n",
      "        [ 0.1920, -0.9149,  0.8869,  ..., -2.7620, -0.3875,  0.1393],\n",
      "        [-0.2273,  0.7596, -1.7529,  ...,  0.7006,  0.7130,  1.2822],\n",
      "        ...,\n",
      "        [ 0.5362,  1.8893, -2.2050,  ..., -1.3003,  0.6866,  0.1575],\n",
      "        [-0.3036, -0.7629,  2.0944,  ...,  1.7368,  0.7095,  0.5238],\n",
      "        [-1.4619,  0.1025, -0.1866,  ..., -0.6420,  0.7828, -0.3573]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 0.9836, -0.8636,  0.0333,  ..., -0.7938,  0.9676, -0.3058],\n",
      "        [ 0.1378, -0.7863, -0.2683,  ..., -0.0656, -0.9115, -0.2220],\n",
      "        [ 0.8071,  0.5165,  0.1110,  ...,  0.6087, -0.4534, -0.8019],\n",
      "        ...,\n",
      "        [ 0.1817,  0.2168,  0.9930,  ..., -0.2104, -0.3371, -0.5068],\n",
      "        [ 0.4779,  1.9687,  0.4211,  ..., -0.5273, -0.7473, -0.2884],\n",
      "        [-0.0652,  0.6359,  1.0496,  ...,  1.6075, -0.5406,  1.6443]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shape: torch.Size([5120, 320])\n",
      "tensor([[ 1.7615,  0.6713, -0.3905,  ...,  0.6140,  1.0006,  0.2803],\n",
      "        [-0.2465, -0.5030,  0.3076,  ...,  0.2927, -2.5245,  0.1954],\n",
      "        [ 0.2835,  1.7592,  0.8981,  ...,  0.2602, -0.5764, -1.0148],\n",
      "        ...,\n",
      "        [ 0.5312,  1.6210,  0.2307,  ...,  0.5991, -0.7373,  1.0500],\n",
      "        [ 0.8894, -0.5314, -1.4371,  ..., -0.9972,  0.4887,  0.0929],\n",
      "        [ 1.9524,  0.7580,  1.8872,  ...,  1.1667,  1.8286, -1.3662]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight, Shape: torch.Size([320, 2560])\n",
      "tensor([[ 1.8626, -0.3749,  0.2563,  ...,  0.5703, -0.1521,  0.3016],\n",
      "        [ 1.0441, -0.5592,  0.3960,  ...,  0.6911, -1.7829,  2.0378],\n",
      "        [-0.1302,  0.8367,  0.0737,  ...,  0.2329, -0.0325,  0.2444],\n",
      "        ...,\n",
      "        [-0.2638,  1.5678,  1.1544,  ...,  0.3642,  0.8415,  0.0326],\n",
      "        [-0.7690, -2.4693, -0.8307,  ..., -1.2581,  0.7682, -0.4317],\n",
      "        [ 1.2996, -0.5215,  0.2721,  ..., -1.5257, -0.8325, -0.2995]])\n",
      "Layer: unet.up_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.4132,  1.2934, -0.4582,  ..., -0.2534,  0.9853, -0.1729],\n",
      "        [ 1.3056,  0.7164, -0.2508,  ...,  1.7303,  0.8191,  1.7812],\n",
      "        [ 1.0838,  0.3919, -0.8188,  ...,  0.6059,  0.7038,  0.2058],\n",
      "        ...,\n",
      "        [ 0.5907,  1.7504,  0.7598,  ...,  0.5790,  2.3441, -0.7098],\n",
      "        [-0.2948, -0.6695,  0.2346,  ..., -0.7695, -0.3796,  0.8810],\n",
      "        [-0.7009,  0.0101,  0.1109,  ..., -0.5759,  0.1989, -1.1019]])\n",
      "Layer: unet.up_blocks.2.attentions.1.proj_in.lora.down.weight, Shape: torch.Size([320, 640, 1, 1])\n",
      "tensor([[[[ 0.5915]],\n",
      "\n",
      "         [[-2.4414]],\n",
      "\n",
      "         [[ 0.9513]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8433]],\n",
      "\n",
      "         [[-0.4271]],\n",
      "\n",
      "         [[-1.8140]]],\n",
      "\n",
      "\n",
      "        [[[-1.0577]],\n",
      "\n",
      "         [[ 1.8069]],\n",
      "\n",
      "         [[ 0.2663]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9351]],\n",
      "\n",
      "         [[-0.7621]],\n",
      "\n",
      "         [[-0.3766]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1680]],\n",
      "\n",
      "         [[-1.1331]],\n",
      "\n",
      "         [[ 0.0109]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5313]],\n",
      "\n",
      "         [[ 0.0105]],\n",
      "\n",
      "         [[ 0.6832]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.9453]],\n",
      "\n",
      "         [[-0.4168]],\n",
      "\n",
      "         [[-0.5324]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2101]],\n",
      "\n",
      "         [[ 1.2019]],\n",
      "\n",
      "         [[-1.3274]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5674]],\n",
      "\n",
      "         [[ 1.6990]],\n",
      "\n",
      "         [[ 0.1959]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0791]],\n",
      "\n",
      "         [[ 0.3948]],\n",
      "\n",
      "         [[ 0.2451]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0573]],\n",
      "\n",
      "         [[ 0.8376]],\n",
      "\n",
      "         [[ 0.0866]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9238]],\n",
      "\n",
      "         [[ 0.6964]],\n",
      "\n",
      "         [[ 0.8608]]]])\n",
      "Layer: unet.up_blocks.2.attentions.1.proj_in.lora.up.weight, Shape: torch.Size([640, 320, 1, 1])\n",
      "tensor([[[[ 0.2174]],\n",
      "\n",
      "         [[-0.5851]],\n",
      "\n",
      "         [[-1.0239]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.8460]],\n",
      "\n",
      "         [[ 0.8525]],\n",
      "\n",
      "         [[ 0.3157]]],\n",
      "\n",
      "\n",
      "        [[[-1.0084]],\n",
      "\n",
      "         [[-0.8189]],\n",
      "\n",
      "         [[ 1.6352]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0447]],\n",
      "\n",
      "         [[-0.2051]],\n",
      "\n",
      "         [[ 0.7250]]],\n",
      "\n",
      "\n",
      "        [[[-0.5237]],\n",
      "\n",
      "         [[-0.1561]],\n",
      "\n",
      "         [[-0.9149]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9443]],\n",
      "\n",
      "         [[ 1.3592]],\n",
      "\n",
      "         [[-0.2778]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.1449]],\n",
      "\n",
      "         [[-1.5016]],\n",
      "\n",
      "         [[-0.0296]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4742]],\n",
      "\n",
      "         [[ 0.1399]],\n",
      "\n",
      "         [[ 0.5743]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7334]],\n",
      "\n",
      "         [[-0.3043]],\n",
      "\n",
      "         [[ 1.3951]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0679]],\n",
      "\n",
      "         [[-0.6683]],\n",
      "\n",
      "         [[ 2.4402]]],\n",
      "\n",
      "\n",
      "        [[[-1.3405]],\n",
      "\n",
      "         [[ 2.7459]],\n",
      "\n",
      "         [[ 0.4095]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1042]],\n",
      "\n",
      "         [[ 0.7358]],\n",
      "\n",
      "         [[-0.5595]]]])\n",
      "Layer: unet.up_blocks.2.attentions.1.proj_out.lora.down.weight, Shape: torch.Size([320, 640, 1, 1])\n",
      "tensor([[[[-0.4708]],\n",
      "\n",
      "         [[-0.5088]],\n",
      "\n",
      "         [[-0.0074]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1378]],\n",
      "\n",
      "         [[ 1.0185]],\n",
      "\n",
      "         [[ 0.6554]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0466]],\n",
      "\n",
      "         [[-1.2420]],\n",
      "\n",
      "         [[ 0.5919]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7859]],\n",
      "\n",
      "         [[ 0.1963]],\n",
      "\n",
      "         [[-0.1328]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8104]],\n",
      "\n",
      "         [[-1.4367]],\n",
      "\n",
      "         [[ 0.3383]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0301]],\n",
      "\n",
      "         [[ 0.2091]],\n",
      "\n",
      "         [[ 0.7022]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4560]],\n",
      "\n",
      "         [[ 0.6608]],\n",
      "\n",
      "         [[-0.3503]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1569]],\n",
      "\n",
      "         [[-0.1316]],\n",
      "\n",
      "         [[ 0.9765]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1518]],\n",
      "\n",
      "         [[ 1.6333]],\n",
      "\n",
      "         [[ 0.1924]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1677]],\n",
      "\n",
      "         [[ 0.2074]],\n",
      "\n",
      "         [[ 1.3341]]],\n",
      "\n",
      "\n",
      "        [[[-0.0882]],\n",
      "\n",
      "         [[-0.6305]],\n",
      "\n",
      "         [[ 0.1407]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1902]],\n",
      "\n",
      "         [[ 0.8613]],\n",
      "\n",
      "         [[ 0.8515]]]])\n",
      "Layer: unet.up_blocks.2.attentions.1.proj_out.lora.up.weight, Shape: torch.Size([640, 320, 1, 1])\n",
      "tensor([[[[-0.4027]],\n",
      "\n",
      "         [[ 0.2786]],\n",
      "\n",
      "         [[ 0.4184]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3372]],\n",
      "\n",
      "         [[-3.2285]],\n",
      "\n",
      "         [[-0.9159]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3095]],\n",
      "\n",
      "         [[ 1.4329]],\n",
      "\n",
      "         [[ 1.2519]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8860]],\n",
      "\n",
      "         [[-2.9719]],\n",
      "\n",
      "         [[-0.3737]]],\n",
      "\n",
      "\n",
      "        [[[-0.3641]],\n",
      "\n",
      "         [[ 0.5666]],\n",
      "\n",
      "         [[ 0.8285]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2993]],\n",
      "\n",
      "         [[-0.0148]],\n",
      "\n",
      "         [[ 0.7936]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.5696]],\n",
      "\n",
      "         [[-0.3580]],\n",
      "\n",
      "         [[ 0.1454]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1692]],\n",
      "\n",
      "         [[ 1.4384]],\n",
      "\n",
      "         [[-0.0217]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5259]],\n",
      "\n",
      "         [[-0.1247]],\n",
      "\n",
      "         [[ 0.9501]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6001]],\n",
      "\n",
      "         [[ 0.6998]],\n",
      "\n",
      "         [[ 0.5604]]],\n",
      "\n",
      "\n",
      "        [[[ 1.3385]],\n",
      "\n",
      "         [[ 0.9243]],\n",
      "\n",
      "         [[-0.0786]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2819]],\n",
      "\n",
      "         [[ 0.3593]],\n",
      "\n",
      "         [[ 0.7578]]]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[-0.2112,  0.1525, -1.0759,  ...,  0.7226,  0.5396,  0.0463],\n",
      "        [-0.5001,  0.3787,  0.3208,  ...,  1.3126, -0.5226,  0.4841],\n",
      "        [ 0.2365,  0.2536,  0.1340,  ..., -0.0237, -0.3555, -0.0623],\n",
      "        ...,\n",
      "        [-0.1864, -0.5781,  1.2999,  ..., -0.5210, -0.0144, -1.2734],\n",
      "        [-1.3592, -0.1892, -0.1510,  ..., -1.5405,  1.9521, -0.6079],\n",
      "        [-0.0902, -0.4688,  0.1776,  ..., -0.4133, -1.8028,  0.4915]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-2.0267, -0.5131, -0.3380,  ..., -0.3605, -0.1505,  0.9903],\n",
      "        [-3.2366,  0.3936, -0.3012,  ..., -1.0737, -0.7502,  0.4359],\n",
      "        [ 0.5681, -0.6409, -0.2200,  ...,  0.0386,  0.5308,  0.4358],\n",
      "        ...,\n",
      "        [-2.0740,  0.2700, -1.1447,  ..., -0.3009, -0.2615,  0.8459],\n",
      "        [ 1.3720,  1.1329,  0.7631,  ..., -0.4530, -1.0985,  0.7144],\n",
      "        [-0.1307,  0.3168,  0.1471,  ...,  0.1698, -2.0830, -0.2143]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 1.2172,  0.6982, -0.2605,  ..., -0.4636, -0.9456,  1.7740],\n",
      "        [ 0.1890,  0.7977, -1.9744,  ..., -0.7250, -0.8219, -0.3801],\n",
      "        [-0.0138,  0.5114, -1.2618,  ...,  0.2958,  0.1002, -0.0084],\n",
      "        ...,\n",
      "        [-0.4373, -1.8797, -0.0466,  ...,  0.6588, -0.2053,  0.7795],\n",
      "        [-1.1917,  0.8243, -0.1538,  ...,  0.7991,  0.0888,  0.8914],\n",
      "        [-0.0967, -0.3597, -0.5161,  ...,  0.2348,  0.3940, -1.7300]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 5.2326e-01, -2.9199e-01, -8.5701e-01,  ...,  5.8932e-01,\n",
      "          9.3667e-01, -1.0306e+00],\n",
      "        [ 7.4321e-01,  9.5360e-01, -2.1952e+00,  ..., -1.5849e+00,\n",
      "         -7.0399e-01, -9.2900e-01],\n",
      "        [-3.9544e-01,  2.2265e-01, -4.2434e-01,  ...,  1.0463e+00,\n",
      "          2.2902e-03, -3.5338e-01],\n",
      "        ...,\n",
      "        [-2.4964e-01, -2.7478e-01, -1.4698e+00,  ...,  2.4773e+00,\n",
      "         -6.2656e-01, -4.8182e-01],\n",
      "        [-7.3304e-03, -9.6751e-01, -1.1100e+00,  ..., -4.8784e-01,\n",
      "          1.5258e+00, -8.7430e-01],\n",
      "        [-8.6648e-01, -3.6152e-02,  8.4365e-01,  ..., -6.6630e-01,\n",
      "          1.1806e+00,  7.2429e-01]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 0.3596,  0.0816,  0.3965,  ...,  0.4804,  0.5079, -1.2179],\n",
      "        [-0.5706,  0.5420,  1.6445,  ..., -0.3720, -0.2148, -0.1436],\n",
      "        [ 0.4843, -0.8876,  0.3447,  ..., -0.2115, -0.2392, -0.6866],\n",
      "        ...,\n",
      "        [ 0.0480, -0.5797, -0.6833,  ..., -0.4722, -0.9770, -0.0669],\n",
      "        [ 0.6999, -1.0104,  1.2480,  ...,  1.8923,  0.0187, -1.8919],\n",
      "        [-0.0191,  0.5732, -0.2130,  ..., -0.0788,  1.0075,  0.7131]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-2.1647,  0.6204, -0.2292,  ..., -0.4361,  0.3895, -0.0537],\n",
      "        [-0.0791,  0.1653,  0.6712,  ...,  1.3395,  0.1612,  0.3253],\n",
      "        [ 0.8935, -1.0067,  2.0179,  ...,  1.0371,  0.8984,  0.7921],\n",
      "        ...,\n",
      "        [-1.9914, -0.1917, -0.0175,  ..., -1.0299, -0.4320, -1.2697],\n",
      "        [-0.7830, -0.7622,  0.1435,  ...,  1.0648, -1.3267, -0.6962],\n",
      "        [ 0.0393, -0.3602, -0.7699,  ...,  0.4803, -1.0407,  0.1578]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[-1.2820, -0.7867, -0.8223,  ...,  0.2088, -0.4004,  0.4395],\n",
      "        [ 0.3601, -0.0525, -0.0195,  ..., -0.5040, -0.0493, -0.1930],\n",
      "        [-0.6810,  1.6739, -0.2430,  ..., -0.3498,  0.6507, -0.3522],\n",
      "        ...,\n",
      "        [-0.8746, -0.2157,  0.4736,  ..., -0.3486,  1.5060,  0.0291],\n",
      "        [-0.6048,  0.3869, -0.5188,  ...,  0.0194, -0.0383,  0.4782],\n",
      "        [ 0.0742,  0.9271, -0.9574,  ...,  0.0153,  0.2679, -1.6527]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.0297, -0.2706,  1.9365,  ...,  0.2312, -1.5202,  0.3012],\n",
      "        [-1.2144, -0.4750, -1.3974,  ..., -1.0904,  0.8178, -0.0028],\n",
      "        [-0.5159,  0.7418,  0.8399,  ..., -1.5664,  1.1512, -0.6136],\n",
      "        ...,\n",
      "        [-0.7359, -1.1655, -0.7401,  ..., -0.7822, -0.0621, -0.0162],\n",
      "        [-0.0267, -0.0844,  0.1732,  ...,  0.7643,  0.9579,  0.3842],\n",
      "        [ 0.5071, -0.2241,  0.0882,  ..., -0.4926,  0.0640, -0.0888]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 1.1956, -0.1193,  0.9363,  ..., -2.1322,  0.8204, -2.3875],\n",
      "        [ 0.7480, -0.0996, -0.0644,  ...,  0.2687, -0.6468, -1.7680],\n",
      "        [-0.3037, -0.3664, -0.3390,  ..., -0.5360,  0.5297, -0.6150],\n",
      "        ...,\n",
      "        [-0.2485,  1.5860,  0.9546,  ..., -0.1388,  0.6863,  0.2603],\n",
      "        [-0.8935,  1.3712, -0.7758,  ...,  0.7559,  0.1492,  2.6094],\n",
      "        [-2.2488,  0.2690, -0.8836,  ..., -1.6984,  1.0946,  1.1195]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-0.6370,  0.5303, -1.4763,  ...,  0.1891, -0.3898,  1.5128],\n",
      "        [-0.1573,  0.3698,  0.2566,  ..., -0.1122, -0.5459,  0.0309],\n",
      "        [-0.6971,  0.1204, -0.7759,  ...,  0.3510,  1.2079, -0.1519],\n",
      "        ...,\n",
      "        [ 0.9821, -0.5861,  0.7020,  ...,  0.3939, -1.0486,  0.5914],\n",
      "        [ 0.6270, -0.9076, -0.3436,  ...,  0.8568,  0.1853, -0.3982],\n",
      "        [ 0.3597, -0.5271, -0.9217,  ...,  0.0294, -0.9307,  1.5012]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 1.2945,  1.0924,  0.0272,  ...,  0.6657,  0.6179,  0.3399],\n",
      "        [ 0.8274,  0.1650,  0.4981,  ...,  0.8643,  0.4331, -1.0228],\n",
      "        [-0.7071,  0.2380, -0.9914,  ...,  0.0079, -1.1681, -0.1067],\n",
      "        ...,\n",
      "        [-0.6247,  0.1390, -0.7044,  ...,  0.1673,  0.4584, -0.1973],\n",
      "        [-0.2612, -1.7070,  0.4893,  ...,  0.5428,  0.0667, -0.1947],\n",
      "        [ 0.4453,  1.2074, -0.6440,  ...,  0.6048,  0.0134, -0.0173]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-1.2904, -0.2401,  0.7264,  ...,  0.3124,  0.3871,  1.8910],\n",
      "        [-1.3893, -1.0499,  0.6843,  ..., -0.0170, -0.7574, -1.1874],\n",
      "        [ 2.4572, -0.6426, -0.3136,  ...,  0.5633,  0.7224,  1.5328],\n",
      "        ...,\n",
      "        [ 0.6483, -0.7664,  1.6532,  ...,  0.7980, -1.1420, -0.0687],\n",
      "        [-0.8487,  1.3584, -0.2018,  ..., -0.5982, -1.2861,  0.6121],\n",
      "        [-1.1639,  1.7372,  0.1098,  ...,  0.4006,  2.1432,  0.1513]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 0.7000,  0.3337,  1.2137,  ..., -1.3580,  0.6999, -0.6362],\n",
      "        [ 1.7907,  1.4366, -0.6620,  ...,  0.1769,  0.2330, -1.1691],\n",
      "        [-0.6458,  0.1597,  0.0984,  ..., -1.2686,  0.7904,  0.1187],\n",
      "        ...,\n",
      "        [-2.0242,  0.9173, -0.7011,  ..., -0.4090, -0.5024,  0.1434],\n",
      "        [-0.2062, -0.2635,  0.5366,  ..., -0.5596, -0.8449,  0.4073],\n",
      "        [-0.4492,  0.1033,  1.0515,  ..., -0.1148, -0.0125, -1.1242]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.4336, -1.0309, -0.0955,  ...,  0.0697, -2.6056,  0.2886],\n",
      "        [ 0.1391,  0.0576,  0.2546,  ..., -0.0134,  3.9303,  0.3933],\n",
      "        [-0.6830, -0.4140, -0.6871,  ...,  0.2240,  1.8327, -0.6934],\n",
      "        ...,\n",
      "        [ 0.3150, -0.1525, -0.6663,  ..., -0.2075,  0.1527, -0.4842],\n",
      "        [-0.1154,  0.1329, -0.1646,  ..., -0.2965, -1.1906, -0.7824],\n",
      "        [-0.4201,  0.0339, -0.0366,  ..., -0.6065, -0.4414, -0.0890]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 0.5294, -0.0649,  1.2568,  ...,  0.2580, -0.2908, -2.7847],\n",
      "        [-1.2394, -0.0354,  0.0778,  ...,  0.7524, -0.5982,  1.3783],\n",
      "        [ 0.1810,  0.8223, -1.2567,  ...,  1.0214,  0.2666, -0.0418],\n",
      "        ...,\n",
      "        [ 0.6536, -0.2733,  0.6974,  ...,  0.6760,  1.0811,  1.7321],\n",
      "        [ 0.3092, -0.9051,  1.4171,  ...,  0.1132,  1.2297, -0.6752],\n",
      "        [ 0.7309,  0.8142,  0.3483,  ...,  0.3997, -0.3804,  0.7796]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.4428,  0.1035, -0.1034,  ..., -0.5843, -1.4862,  1.3158],\n",
      "        [ 0.3618,  1.0756,  3.1232,  ..., -0.0536, -0.9951,  0.9410],\n",
      "        [-0.3607, -1.8785,  0.1721,  ...,  0.6775, -0.2555,  1.2269],\n",
      "        ...,\n",
      "        [-0.6881, -1.1874, -0.5234,  ..., -2.2064,  1.1326,  2.1495],\n",
      "        [ 1.0394, -1.7428, -1.5586,  ..., -0.7408, -1.3476,  0.4585],\n",
      "        [-0.6218, -2.9318, -1.4626,  ...,  0.6125, -0.5148,  0.1684]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 1.3008, -1.7970, -0.2213,  ..., -1.6374,  0.1171,  0.3151],\n",
      "        [ 0.6171, -0.2990,  0.7775,  ...,  0.6034,  0.3917,  0.9629],\n",
      "        [-0.5308, -0.0980, -1.0321,  ..., -0.0843, -0.0664, -1.2896],\n",
      "        ...,\n",
      "        [ 0.5760, -0.2633, -0.1197,  ...,  0.3789, -0.6068,  0.5322],\n",
      "        [-1.0960,  0.9162,  1.7277,  ...,  0.8921, -0.7325,  2.2793],\n",
      "        [-0.5568,  0.9919, -0.1832,  ...,  0.4386, -0.6764,  0.5578]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shape: torch.Size([5120, 320])\n",
      "tensor([[ 1.9010, -1.4127,  1.4237,  ..., -0.0317, -4.2503, -1.9455],\n",
      "        [-0.5864, -0.4727,  1.0485,  ..., -1.0641,  0.1543, -0.3702],\n",
      "        [ 0.6524,  0.2833,  0.3097,  ...,  0.2390, -0.5005,  1.0239],\n",
      "        ...,\n",
      "        [ 0.7606, -0.3288,  0.6217,  ..., -0.0783, -0.4998, -0.2764],\n",
      "        [ 0.3025,  1.0707, -0.0933,  ..., -0.6067, -0.2458,  1.4296],\n",
      "        [ 1.1218, -1.4119, -0.2546,  ..., -0.4133, -0.0993, -0.6991]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight, Shape: torch.Size([320, 2560])\n",
      "tensor([[ 3.8122e+00, -2.1652e-01,  3.3439e-03,  ...,  3.6515e-01,\n",
      "          3.7197e-01,  4.4415e-01],\n",
      "        [ 5.9925e-01, -3.0228e-02,  4.1125e-01,  ..., -3.7162e-01,\n",
      "          9.2315e-01, -1.5282e+00],\n",
      "        [ 1.6367e+00, -1.2981e+00, -3.1877e-02,  ..., -8.4459e-01,\n",
      "         -4.2969e-01, -1.0744e+00],\n",
      "        ...,\n",
      "        [-3.1355e-02, -3.0963e-01,  3.0961e-01,  ..., -9.8523e-01,\n",
      "         -8.2377e-02, -2.8845e-02],\n",
      "        [-2.7553e+00, -2.0845e-01,  1.6374e+00,  ..., -1.4022e-01,\n",
      "         -4.1726e-01, -1.5179e+00],\n",
      "        [ 1.3933e+00,  5.5506e-01, -2.8633e-01,  ...,  4.1776e-01,\n",
      "         -8.6230e-01, -4.9161e-01]])\n",
      "Layer: unet.up_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.5915, -0.3171,  0.3349,  ...,  0.2175, -1.5748,  0.0684],\n",
      "        [-0.0429,  0.5116, -0.1004,  ...,  0.1950, -0.9501, -0.4532],\n",
      "        [-0.7593, -0.7755,  0.9429,  ...,  0.2472,  0.0610, -0.7975],\n",
      "        ...,\n",
      "        [-0.2929,  0.9176,  0.8901,  ..., -0.0118,  0.6241, -0.5100],\n",
      "        [-0.9215, -1.3676, -0.0115,  ...,  0.0834,  0.1800, -0.7863],\n",
      "        [ 1.5591,  0.5491,  0.8104,  ...,  0.7902, -1.4608, -0.4366]])\n",
      "Layer: unet.up_blocks.2.attentions.2.proj_in.lora.down.weight, Shape: torch.Size([320, 640, 1, 1])\n",
      "tensor([[[[-9.3793e-01]],\n",
      "\n",
      "         [[ 2.9493e-01]],\n",
      "\n",
      "         [[-8.4489e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5799e+00]],\n",
      "\n",
      "         [[-6.5771e-01]],\n",
      "\n",
      "         [[-2.3728e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 3.1822e-01]],\n",
      "\n",
      "         [[ 1.9355e+00]],\n",
      "\n",
      "         [[-2.5969e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.5891e-01]],\n",
      "\n",
      "         [[ 2.9115e-01]],\n",
      "\n",
      "         [[-1.0628e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1081e+00]],\n",
      "\n",
      "         [[-7.9680e-02]],\n",
      "\n",
      "         [[ 7.6519e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1399e+00]],\n",
      "\n",
      "         [[ 1.2957e-01]],\n",
      "\n",
      "         [[ 5.1207e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.7691e-03]],\n",
      "\n",
      "         [[ 1.1464e-01]],\n",
      "\n",
      "         [[-1.9704e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.1744e-01]],\n",
      "\n",
      "         [[ 1.5740e+00]],\n",
      "\n",
      "         [[-9.5362e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.0392e-01]],\n",
      "\n",
      "         [[-6.0369e-01]],\n",
      "\n",
      "         [[-8.7848e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.4069e-01]],\n",
      "\n",
      "         [[ 1.5653e+00]],\n",
      "\n",
      "         [[-1.2463e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.3793e-01]],\n",
      "\n",
      "         [[ 8.2754e-01]],\n",
      "\n",
      "         [[-6.9925e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8359e+00]],\n",
      "\n",
      "         [[ 5.5038e-01]],\n",
      "\n",
      "         [[-9.8073e-01]]]])\n",
      "Layer: unet.up_blocks.2.attentions.2.proj_in.lora.up.weight, Shape: torch.Size([640, 320, 1, 1])\n",
      "tensor([[[[ 1.3308]],\n",
      "\n",
      "         [[ 0.0624]],\n",
      "\n",
      "         [[-1.6899]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3953]],\n",
      "\n",
      "         [[ 0.2130]],\n",
      "\n",
      "         [[-1.5693]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1963]],\n",
      "\n",
      "         [[ 0.8873]],\n",
      "\n",
      "         [[ 0.4823]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8282]],\n",
      "\n",
      "         [[-0.5766]],\n",
      "\n",
      "         [[ 0.3861]]],\n",
      "\n",
      "\n",
      "        [[[-0.9006]],\n",
      "\n",
      "         [[-0.8782]],\n",
      "\n",
      "         [[-0.4638]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2432]],\n",
      "\n",
      "         [[ 0.5553]],\n",
      "\n",
      "         [[ 0.3328]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.5781]],\n",
      "\n",
      "         [[-0.6642]],\n",
      "\n",
      "         [[-0.0701]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.8352]],\n",
      "\n",
      "         [[ 0.6897]],\n",
      "\n",
      "         [[-0.5311]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2133]],\n",
      "\n",
      "         [[-0.7334]],\n",
      "\n",
      "         [[-0.6393]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8635]],\n",
      "\n",
      "         [[ 0.0565]],\n",
      "\n",
      "         [[-0.2981]]],\n",
      "\n",
      "\n",
      "        [[[-0.8222]],\n",
      "\n",
      "         [[ 0.6430]],\n",
      "\n",
      "         [[-0.9001]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3437]],\n",
      "\n",
      "         [[ 1.1836]],\n",
      "\n",
      "         [[-0.1033]]]])\n",
      "Layer: unet.up_blocks.2.attentions.2.proj_out.lora.down.weight, Shape: torch.Size([320, 640, 1, 1])\n",
      "tensor([[[[ 0.8863]],\n",
      "\n",
      "         [[-1.8175]],\n",
      "\n",
      "         [[ 1.2033]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3798]],\n",
      "\n",
      "         [[ 2.1528]],\n",
      "\n",
      "         [[-1.4257]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5241]],\n",
      "\n",
      "         [[ 0.3210]],\n",
      "\n",
      "         [[-1.2790]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.5373]],\n",
      "\n",
      "         [[-1.5181]],\n",
      "\n",
      "         [[ 0.9924]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1996]],\n",
      "\n",
      "         [[ 0.0600]],\n",
      "\n",
      "         [[ 0.5842]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4225]],\n",
      "\n",
      "         [[ 1.2774]],\n",
      "\n",
      "         [[-0.6839]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4085]],\n",
      "\n",
      "         [[-0.4771]],\n",
      "\n",
      "         [[-0.5811]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1879]],\n",
      "\n",
      "         [[-0.8372]],\n",
      "\n",
      "         [[-0.1514]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0302]],\n",
      "\n",
      "         [[ 0.0492]],\n",
      "\n",
      "         [[-1.5346]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7927]],\n",
      "\n",
      "         [[ 0.5895]],\n",
      "\n",
      "         [[-1.5665]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3311]],\n",
      "\n",
      "         [[-1.0364]],\n",
      "\n",
      "         [[ 0.3031]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8864]],\n",
      "\n",
      "         [[-1.1215]],\n",
      "\n",
      "         [[-2.6578]]]])\n",
      "Layer: unet.up_blocks.2.attentions.2.proj_out.lora.up.weight, Shape: torch.Size([640, 320, 1, 1])\n",
      "tensor([[[[-0.0063]],\n",
      "\n",
      "         [[ 1.2241]],\n",
      "\n",
      "         [[ 1.3291]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0149]],\n",
      "\n",
      "         [[-1.7058]],\n",
      "\n",
      "         [[-0.4936]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4044]],\n",
      "\n",
      "         [[ 0.9293]],\n",
      "\n",
      "         [[ 1.6095]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.2682]],\n",
      "\n",
      "         [[ 2.2433]],\n",
      "\n",
      "         [[-0.8678]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6181]],\n",
      "\n",
      "         [[-0.6447]],\n",
      "\n",
      "         [[ 0.2000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1581]],\n",
      "\n",
      "         [[-0.2349]],\n",
      "\n",
      "         [[ 0.2048]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.7301]],\n",
      "\n",
      "         [[ 0.4697]],\n",
      "\n",
      "         [[-0.0605]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7937]],\n",
      "\n",
      "         [[-0.2620]],\n",
      "\n",
      "         [[ 0.1167]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2227]],\n",
      "\n",
      "         [[-0.2016]],\n",
      "\n",
      "         [[-2.6538]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2776]],\n",
      "\n",
      "         [[-0.8573]],\n",
      "\n",
      "         [[ 0.4165]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9914]],\n",
      "\n",
      "         [[-0.3358]],\n",
      "\n",
      "         [[ 2.0117]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.6246]],\n",
      "\n",
      "         [[ 1.2501]],\n",
      "\n",
      "         [[ 0.1365]]]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[-1.2764e-01, -5.7185e-01, -3.3595e-01,  ..., -4.6923e-02,\n",
      "         -1.3878e+00, -9.7302e-04],\n",
      "        [-1.5245e+00, -6.5515e-01, -1.2228e+00,  ...,  2.0901e-01,\n",
      "          2.7132e-01,  1.2196e+00],\n",
      "        [ 6.2831e-01, -4.8624e-01,  6.9150e-01,  ..., -1.0392e-02,\n",
      "         -9.6484e-01, -4.5020e-01],\n",
      "        ...,\n",
      "        [ 1.2070e-01,  4.1614e-01,  6.0258e-01,  ...,  1.2689e+00,\n",
      "          6.9640e-02, -1.1615e+00],\n",
      "        [-1.0575e+00, -5.6897e-01,  6.8731e-02,  ..., -1.3799e+00,\n",
      "          2.7115e+00,  4.8945e-01],\n",
      "        [ 6.6784e-01,  5.1603e-01, -1.0327e-01,  ..., -3.0676e-01,\n",
      "          8.9278e-01,  1.5462e-02]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-0.0609, -0.0876, -0.1948,  ..., -1.0735,  0.2776,  0.2683],\n",
      "        [-1.4525, -1.5923,  0.0963,  ..., -0.0679, -0.9981,  0.7739],\n",
      "        [ 0.3382,  0.2014, -0.6933,  ...,  0.1600, -0.6231,  0.4162],\n",
      "        ...,\n",
      "        [-0.0187, -0.3018,  0.9505,  ...,  0.6524, -2.1116, -0.1758],\n",
      "        [ 2.2483,  0.3905, -0.3739,  ..., -0.0210, -1.0025, -0.1035],\n",
      "        [ 0.9115, -0.3099, -1.2766,  ..., -0.3361,  0.4317,  0.3696]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[-1.8069, -1.2561,  0.7880,  ..., -1.2207,  0.8480,  1.1310],\n",
      "        [-0.2678,  0.2784,  0.5180,  ..., -0.1091, -0.5102,  0.2369],\n",
      "        [-0.6103,  0.1379,  0.2452,  ..., -0.6037,  0.4268, -0.3059],\n",
      "        ...,\n",
      "        [ 0.0682, -0.8978,  1.3886,  ...,  1.3171,  0.1548,  0.5810],\n",
      "        [-1.4042,  0.1837,  1.4475,  ..., -1.1525,  0.1024,  0.8082],\n",
      "        [ 0.1633,  1.1156,  0.8165,  ...,  0.9550,  0.4839, -0.6264]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-0.2218,  0.4537, -0.4787,  ...,  0.2429,  0.4770,  1.2505],\n",
      "        [-1.5853, -0.7542,  0.0342,  ...,  0.1100, -0.0219, -0.0246],\n",
      "        [-0.6757, -0.5011,  1.1532,  ...,  1.1466, -0.1648, -1.7540],\n",
      "        ...,\n",
      "        [-0.1977,  0.5932, -0.4501,  ...,  1.5683,  2.6475, -1.3437],\n",
      "        [-1.3439, -0.6447, -0.6988,  ..., -0.1877,  0.3361, -1.1105],\n",
      "        [ 0.2333, -0.5595,  1.0003,  ..., -0.4928, -1.4982, -2.2296]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 0.0599, -0.1097, -0.6172,  ...,  0.0238, -0.4594,  1.6668],\n",
      "        [-0.6370, -0.1999, -0.2580,  ..., -0.1660,  0.8494,  1.1431],\n",
      "        [ 0.4712, -0.6152, -0.3036,  ..., -0.4613,  0.1481, -0.5495],\n",
      "        ...,\n",
      "        [-0.2209, -0.2442,  1.0154,  ...,  0.6129, -0.6826,  0.3499],\n",
      "        [-0.9931,  0.9218, -1.3021,  ..., -0.6274, -0.6835,  0.7165],\n",
      "        [-0.7773,  0.9511, -1.0208,  ...,  0.2374,  0.9174,  0.2713]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-1.5818, -0.0341,  0.8530,  ...,  0.8219,  0.1588, -1.1040],\n",
      "        [ 1.1579,  0.1118, -1.2397,  ...,  0.0955, -0.7484, -0.9634],\n",
      "        [ 0.1537, -0.4033,  0.1696,  ..., -0.7774, -0.5133, -0.2517],\n",
      "        ...,\n",
      "        [-1.0355, -0.0749, -0.1812,  ...,  0.0099, -1.1207, -0.3611],\n",
      "        [-1.0734,  0.1130, -0.4222,  ..., -0.0535, -2.0489, -0.5197],\n",
      "        [ 0.6866, -0.1998,  0.1662,  ..., -0.1398, -0.2609, -0.4997]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 0.2382,  0.0581, -1.0541,  ...,  0.1975, -0.5796, -0.4342],\n",
      "        [-0.1604,  0.7382,  0.0169,  ...,  1.4239,  0.4993, -0.9217],\n",
      "        [-2.1096, -0.6325, -0.1618,  ...,  1.0884, -0.4168, -0.2895],\n",
      "        ...,\n",
      "        [ 1.0641,  0.2429, -0.6082,  ...,  1.0572,  0.5707,  0.5421],\n",
      "        [ 0.1014,  1.2995, -1.0603,  ...,  1.0958, -1.8495, -1.6540],\n",
      "        [-1.0305,  1.1294, -0.3731,  ...,  0.5757,  1.6136, -0.3600]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.7164, -0.7482, -0.8839,  ...,  0.2461,  0.0209,  0.7680],\n",
      "        [-2.0517,  0.3938, -0.2934,  ..., -0.9912, -2.2592,  0.3413],\n",
      "        [ 1.2426, -0.1708, -1.2072,  ...,  0.4248, -0.3852, -0.2250],\n",
      "        ...,\n",
      "        [-1.0448, -0.2579, -0.5113,  ...,  0.3109, -1.4374, -0.3457],\n",
      "        [ 0.5833,  0.0919,  0.7366,  ..., -0.2767, -1.1261, -1.4732],\n",
      "        [-0.9321,  0.8573, -0.0329,  ..., -0.2288, -0.6420, -0.2092]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 0.0739,  2.8010,  0.1240,  ...,  0.3939, -0.7408,  0.8842],\n",
      "        [-0.1203, -0.0693, -0.0952,  ..., -1.1789,  0.7560, -0.0784],\n",
      "        [-0.6190, -0.6398, -1.0035,  ..., -0.4676,  0.1560,  0.1211],\n",
      "        ...,\n",
      "        [-0.7347,  0.9387,  0.3618,  ...,  0.3255,  0.3935, -0.9390],\n",
      "        [-0.0834, -0.7533, -0.8203,  ...,  1.2446, -0.2577,  1.8338],\n",
      "        [ 0.1171, -1.1158, -0.8728,  ..., -0.3514, -1.5427, -1.6008]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-0.6312, -0.0069,  0.7427,  ...,  1.0916,  0.6458,  0.1004],\n",
      "        [ 0.2792, -0.2799, -0.5277,  ..., -0.0371, -2.3926, -0.0394],\n",
      "        [-0.0551, -0.0981, -0.1341,  ..., -0.1459, -0.9015,  0.2020],\n",
      "        ...,\n",
      "        [-0.7425,  0.2058, -0.3858,  ...,  1.3922,  1.4926, -0.0751],\n",
      "        [-0.0375, -0.0274,  0.0695,  ...,  0.2110,  1.8189,  0.3882],\n",
      "        [-0.3032,  0.0671,  0.2010,  ..., -0.0702,  1.1241, -0.2489]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[-1.1923, -0.1065, -0.7104,  ..., -1.3551,  0.6866,  0.1058],\n",
      "        [ 1.9268, -1.0113,  0.3581,  ..., -0.6663, -0.1384, -1.1837],\n",
      "        [ 0.1304, -0.8633, -0.5860,  ...,  0.9401,  0.5533, -0.3890],\n",
      "        ...,\n",
      "        [-0.5649,  0.2363,  0.2321,  ..., -0.0647, -1.5451,  1.0532],\n",
      "        [-2.2473, -0.2182, -0.2496,  ...,  0.7383,  0.2718, -1.4201],\n",
      "        [ 0.5552, -0.0984,  0.2124,  ..., -0.4673,  0.0568, -0.5594]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.4375, -1.5889,  0.9600,  ..., -0.8156,  0.2086,  1.5207],\n",
      "        [-0.0800, -0.7832,  0.9354,  ...,  0.1932,  1.1150, -1.4309],\n",
      "        [ 0.6561,  1.3734,  0.0982,  ...,  0.0220, -2.1676, -1.0070],\n",
      "        ...,\n",
      "        [ 0.3991, -1.3080,  1.2541,  ..., -0.3244,  1.1340,  0.0286],\n",
      "        [ 0.6531, -0.3109, -0.7155,  ...,  0.0044, -1.0439,  0.3927],\n",
      "        [ 0.1927,  1.4602, -1.0241,  ...,  1.4081, -0.7964, -0.0136]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[-0.0630,  0.5124,  1.1333,  ...,  1.2471, -0.0863, -0.2656],\n",
      "        [-0.3852, -0.7151,  0.1210,  ..., -0.0941,  0.1129,  0.3412],\n",
      "        [ 0.4188, -0.3221,  0.2602,  ...,  0.9750,  0.4720,  0.8354],\n",
      "        ...,\n",
      "        [-0.4206,  0.8143,  0.2751,  ...,  1.2037,  1.3425, -0.6021],\n",
      "        [-0.8482, -1.5286, -1.0515,  ..., -1.8963, -0.2456,  2.3362],\n",
      "        [ 0.4698, -0.7132,  0.3287,  ..., -0.5499,  0.7916,  0.1979]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.1602,  0.0807,  0.0052,  ...,  0.7299,  1.2884, -0.1483],\n",
      "        [ 0.0726,  0.0641, -0.1419,  ...,  1.3713,  1.2156,  0.4142],\n",
      "        [-0.4320,  0.1398, -0.0285,  ..., -0.2509,  0.8327, -0.2435],\n",
      "        ...,\n",
      "        [-0.1567, -0.0688, -0.4202,  ...,  0.3521,  1.5177,  0.0234],\n",
      "        [ 0.2152,  0.1794, -0.3105,  ...,  0.3050,  1.2759,  0.0358],\n",
      "        [ 0.2404, -0.0319, -0.0316,  ...,  0.5921, -1.1795,  0.2147]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[-0.6926, -0.0087,  0.7653,  ..., -0.1506, -0.1382, -1.1860],\n",
      "        [ 1.2397,  0.2646,  0.7197,  ..., -0.4407, -0.2010,  0.9436],\n",
      "        [ 2.0027, -0.6800,  1.1848,  ...,  1.5872,  0.3059, -0.1557],\n",
      "        ...,\n",
      "        [ 0.3146,  0.7887, -0.6926,  ...,  1.0433,  1.1813,  0.8132],\n",
      "        [-1.0189, -1.6133, -0.4045,  ..., -1.2035, -0.5595, -1.1778],\n",
      "        [ 0.0892, -0.2150, -1.8241,  ...,  0.2410, -0.0885, -0.7488]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[ 0.4386, -1.6001,  1.4791,  ...,  1.0932,  0.2383,  0.1887],\n",
      "        [ 0.9513, -1.8999, -0.4998,  ..., -1.6134,  0.8761, -0.5228],\n",
      "        [ 0.6247,  1.2619,  1.1150,  ...,  0.3744, -0.2417,  0.1931],\n",
      "        ...,\n",
      "        [ 0.7819,  0.8060, -2.0174,  ..., -0.9139, -0.8506,  1.7783],\n",
      "        [-0.6024,  0.2213, -1.1536,  ..., -1.4956,  1.6171,  1.0993],\n",
      "        [ 0.0911, -0.8174,  2.1638,  ...,  3.6901,  0.6565,  1.1654]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shape: torch.Size([320, 640])\n",
      "tensor([[ 0.5167, -1.0727, -0.4923,  ...,  0.9397,  0.4471,  1.3988],\n",
      "        [-0.2617, -0.5778, -0.5370,  ...,  0.0312,  0.5775, -0.3020],\n",
      "        [ 0.0963,  0.9974, -0.7061,  ...,  0.1511,  0.5173, -1.1584],\n",
      "        ...,\n",
      "        [ 0.1440,  1.5536, -0.0345,  ..., -0.2418, -0.2183,  0.9164],\n",
      "        [ 0.4363,  2.0452,  0.3398,  ...,  0.2278,  0.2858, -0.1392],\n",
      "        [ 0.9682,  0.7523, -0.1844,  ...,  1.4843, -1.8737, -0.0839]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shape: torch.Size([5120, 320])\n",
      "tensor([[-0.6290, -0.8545, -0.9774,  ...,  0.3791,  1.0522, -0.2807],\n",
      "        [ 0.7507, -0.0706,  1.0971,  ..., -0.3434,  1.2668, -0.1427],\n",
      "        [-0.0069,  1.8246,  0.3490,  ..., -1.6253, -0.2060, -0.0323],\n",
      "        ...,\n",
      "        [ 0.4130,  0.0208, -0.1888,  ..., -0.3281,  0.8034,  0.4384],\n",
      "        [ 0.0851,  1.0666, -0.1022,  ...,  1.4037,  0.5093,  1.5241],\n",
      "        [ 0.0566, -0.4997, -0.2241,  ...,  0.2199, -0.1270, -2.1689]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.lora.down.weight, Shape: torch.Size([320, 2560])\n",
      "tensor([[ 0.1809,  0.4562, -0.2056,  ..., -0.1170,  0.9154, -0.2358],\n",
      "        [ 1.3685, -0.0197, -1.4272,  ...,  0.4055, -1.7005, -0.3996],\n",
      "        [-0.3398,  0.6066,  0.2024,  ..., -0.5130, -0.4163,  0.2706],\n",
      "        ...,\n",
      "        [-0.0635,  0.7635, -1.2752,  ...,  0.8469,  0.9146, -0.3921],\n",
      "        [ 1.0103,  0.8326,  0.5694,  ..., -1.2270,  0.4977, -0.2476],\n",
      "        [ 0.1299, -0.2189, -0.0382,  ..., -0.5391,  1.0004, -1.1318]])\n",
      "Layer: unet.up_blocks.2.attentions.2.transformer_blocks.0.ff.net.2.lora.up.weight, Shape: torch.Size([640, 320])\n",
      "tensor([[-1.0483, -1.2483, -0.3893,  ..., -0.1965,  0.5608, -1.0697],\n",
      "        [-0.6584,  0.4138,  0.3959,  ...,  0.2496, -0.2931, -0.2868],\n",
      "        [-0.7636, -0.4158, -0.0849,  ...,  0.2326,  0.5343,  0.7242],\n",
      "        ...,\n",
      "        [ 2.1767,  0.3949,  1.1827,  ..., -0.7698,  0.9000, -0.4667],\n",
      "        [ 0.0078,  0.0637, -0.3611,  ..., -0.5426, -2.8839, -0.6178],\n",
      "        [ 0.7595, -0.0072, -1.2460,  ...,  0.2547,  0.7629,  1.0906]])\n",
      "Layer: unet.up_blocks.3.attentions.0.proj_in.lora.down.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[-0.0804]],\n",
      "\n",
      "         [[-0.9791]],\n",
      "\n",
      "         [[-2.0259]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1605]],\n",
      "\n",
      "         [[-0.3941]],\n",
      "\n",
      "         [[ 0.1427]]],\n",
      "\n",
      "\n",
      "        [[[-0.1154]],\n",
      "\n",
      "         [[ 0.7984]],\n",
      "\n",
      "         [[-0.1615]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5404]],\n",
      "\n",
      "         [[-0.1324]],\n",
      "\n",
      "         [[-0.6190]]],\n",
      "\n",
      "\n",
      "        [[[-0.1431]],\n",
      "\n",
      "         [[ 0.9046]],\n",
      "\n",
      "         [[ 1.2067]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1281]],\n",
      "\n",
      "         [[-0.7988]],\n",
      "\n",
      "         [[-0.4116]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.1118]],\n",
      "\n",
      "         [[ 1.0763]],\n",
      "\n",
      "         [[ 0.7344]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4543]],\n",
      "\n",
      "         [[ 0.0732]],\n",
      "\n",
      "         [[-0.0528]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1492]],\n",
      "\n",
      "         [[-0.8209]],\n",
      "\n",
      "         [[ 0.9646]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2969]],\n",
      "\n",
      "         [[-0.3182]],\n",
      "\n",
      "         [[ 1.2690]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6512]],\n",
      "\n",
      "         [[-0.4366]],\n",
      "\n",
      "         [[-0.1194]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7175]],\n",
      "\n",
      "         [[-0.1347]],\n",
      "\n",
      "         [[-0.0662]]]])\n",
      "Layer: unet.up_blocks.3.attentions.0.proj_in.lora.up.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[-0.1046]],\n",
      "\n",
      "         [[-0.5714]],\n",
      "\n",
      "         [[-0.7754]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9520]],\n",
      "\n",
      "         [[ 1.2025]],\n",
      "\n",
      "         [[ 0.2791]]],\n",
      "\n",
      "\n",
      "        [[[-0.0793]],\n",
      "\n",
      "         [[ 1.1661]],\n",
      "\n",
      "         [[-0.0144]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3246]],\n",
      "\n",
      "         [[-0.5668]],\n",
      "\n",
      "         [[-0.6767]]],\n",
      "\n",
      "\n",
      "        [[[-0.8759]],\n",
      "\n",
      "         [[-1.3705]],\n",
      "\n",
      "         [[-0.5132]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4260]],\n",
      "\n",
      "         [[-1.3454]],\n",
      "\n",
      "         [[ 0.5220]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.1954]],\n",
      "\n",
      "         [[ 0.9728]],\n",
      "\n",
      "         [[ 0.8434]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2566]],\n",
      "\n",
      "         [[ 0.0537]],\n",
      "\n",
      "         [[ 0.2278]]],\n",
      "\n",
      "\n",
      "        [[[-0.7504]],\n",
      "\n",
      "         [[ 0.2221]],\n",
      "\n",
      "         [[ 0.3843]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8654]],\n",
      "\n",
      "         [[-0.5035]],\n",
      "\n",
      "         [[-0.5645]]],\n",
      "\n",
      "\n",
      "        [[[-0.2489]],\n",
      "\n",
      "         [[-0.0091]],\n",
      "\n",
      "         [[ 0.6350]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.8530]],\n",
      "\n",
      "         [[ 1.1593]],\n",
      "\n",
      "         [[-0.0949]]]])\n",
      "Layer: unet.up_blocks.3.attentions.0.proj_out.lora.down.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[-0.7677]],\n",
      "\n",
      "         [[ 1.7699]],\n",
      "\n",
      "         [[ 0.2459]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9026]],\n",
      "\n",
      "         [[-0.4096]],\n",
      "\n",
      "         [[ 0.1202]]],\n",
      "\n",
      "\n",
      "        [[[-1.2898]],\n",
      "\n",
      "         [[ 0.6445]],\n",
      "\n",
      "         [[-0.0949]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2050]],\n",
      "\n",
      "         [[ 0.1142]],\n",
      "\n",
      "         [[-0.1312]]],\n",
      "\n",
      "\n",
      "        [[[-0.7007]],\n",
      "\n",
      "         [[ 0.4872]],\n",
      "\n",
      "         [[-0.3887]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0083]],\n",
      "\n",
      "         [[-1.1475]],\n",
      "\n",
      "         [[ 0.7774]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.8777]],\n",
      "\n",
      "         [[ 0.1814]],\n",
      "\n",
      "         [[ 1.0136]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3281]],\n",
      "\n",
      "         [[ 0.9109]],\n",
      "\n",
      "         [[-0.0466]]],\n",
      "\n",
      "\n",
      "        [[[-0.5210]],\n",
      "\n",
      "         [[ 1.0737]],\n",
      "\n",
      "         [[ 0.2612]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4165]],\n",
      "\n",
      "         [[-2.0003]],\n",
      "\n",
      "         [[ 0.5637]]],\n",
      "\n",
      "\n",
      "        [[[-0.8253]],\n",
      "\n",
      "         [[ 1.0854]],\n",
      "\n",
      "         [[ 0.8952]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8541]],\n",
      "\n",
      "         [[-0.3959]],\n",
      "\n",
      "         [[ 0.1248]]]])\n",
      "Layer: unet.up_blocks.3.attentions.0.proj_out.lora.up.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[-4.5478e-01]],\n",
      "\n",
      "         [[ 4.6842e-01]],\n",
      "\n",
      "         [[ 9.8956e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.6103e-01]],\n",
      "\n",
      "         [[ 1.2541e+00]],\n",
      "\n",
      "         [[ 4.2260e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6563e-05]],\n",
      "\n",
      "         [[ 3.0624e-01]],\n",
      "\n",
      "         [[-3.9452e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1935e-01]],\n",
      "\n",
      "         [[-7.3401e-01]],\n",
      "\n",
      "         [[-3.3385e-01]]],\n",
      "\n",
      "\n",
      "        [[[-5.9537e-01]],\n",
      "\n",
      "         [[ 5.7869e-01]],\n",
      "\n",
      "         [[-4.4777e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7479e+00]],\n",
      "\n",
      "         [[-3.5459e-01]],\n",
      "\n",
      "         [[-9.2818e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 7.4433e-01]],\n",
      "\n",
      "         [[-4.8425e-01]],\n",
      "\n",
      "         [[ 5.4825e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0979e+00]],\n",
      "\n",
      "         [[ 7.3793e-01]],\n",
      "\n",
      "         [[ 3.0913e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.1209e+00]],\n",
      "\n",
      "         [[ 7.4545e-01]],\n",
      "\n",
      "         [[ 8.0406e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1116e+00]],\n",
      "\n",
      "         [[-3.1825e-01]],\n",
      "\n",
      "         [[-9.4854e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.9306e-01]],\n",
      "\n",
      "         [[ 5.7958e-02]],\n",
      "\n",
      "         [[ 9.1751e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0544e+00]],\n",
      "\n",
      "         [[-6.1942e-01]],\n",
      "\n",
      "         [[ 6.8845e-01]]]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.6618, -1.3420,  0.4921,  ..., -0.2620, -0.3570, -0.7228],\n",
      "        [ 1.0186, -0.2459, -0.7479,  ...,  0.8146,  1.5618,  0.8167],\n",
      "        [-0.7103, -0.6802, -0.6827,  ..., -0.0828, -1.0012,  0.2267],\n",
      "        ...,\n",
      "        [-1.0969, -0.1217, -0.8876,  ..., -0.2807,  1.9042,  0.0919],\n",
      "        [-0.2754,  0.9706, -0.6415,  ..., -0.3266, -0.4789, -1.1543],\n",
      "        [ 0.8596, -0.6339,  0.1361,  ...,  0.5076, -2.0904,  0.2705]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-2.4167,  0.3435,  0.2316,  ...,  0.7576,  0.8416,  0.2295],\n",
      "        [ 0.9031,  0.1101, -0.5439,  ...,  0.2620, -0.3869, -1.4560],\n",
      "        [-0.5934, -0.5099,  0.8048,  ..., -0.4480, -0.3995, -0.2413],\n",
      "        ...,\n",
      "        [ 0.9207, -0.6176,  2.9212,  ...,  0.1606,  0.6286,  0.3276],\n",
      "        [-0.6579, -0.0134,  1.2140,  ..., -0.3182,  1.6166, -2.1503],\n",
      "        [ 0.9562,  0.2011, -1.8086,  ...,  0.3349, -0.1198, -0.5964]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.0728, -0.7221,  0.0186,  ...,  0.1833, -0.3396, -0.9901],\n",
      "        [-0.7788, -0.6029, -0.9792,  ...,  2.0071,  1.5928,  0.5459],\n",
      "        [ 1.5282, -0.4047, -0.3647,  ...,  0.2378, -0.7435, -0.6065],\n",
      "        ...,\n",
      "        [-1.1361,  0.0816, -0.1192,  ..., -0.6875,  0.9614,  0.5652],\n",
      "        [ 0.7178, -0.7929, -1.8285,  ..., -0.8306, -0.7205,  0.2649],\n",
      "        [ 0.6109,  1.6356, -0.8672,  ..., -0.8055, -0.1919, -0.7462]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.5443,  2.9706,  1.2514,  ..., -0.4140,  1.3690, -0.1835],\n",
      "        [ 0.2070, -1.2972,  0.7268,  ..., -0.0772,  0.1076,  0.6413],\n",
      "        [ 0.4125,  0.0104,  0.2761,  ..., -1.1149,  1.2722, -0.8787],\n",
      "        ...,\n",
      "        [ 0.2845,  1.2164,  0.3225,  ..., -0.4927,  0.1486,  1.5353],\n",
      "        [ 0.0550, -0.3580,  1.7695,  ..., -0.5065, -0.7039,  0.2407],\n",
      "        [-0.7140, -0.0261, -1.2873,  ..., -0.0345,  0.7270, -0.8437]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 2.3279, -0.1509,  1.4369,  ...,  0.6414, -0.9690, -1.4821],\n",
      "        [-1.2210, -0.1249, -1.6162,  ..., -1.0845,  1.3849,  0.3462],\n",
      "        [ 0.1335, -1.0311,  0.8962,  ...,  0.0564, -0.5791, -1.2019],\n",
      "        ...,\n",
      "        [-0.4657, -0.6568, -1.3760,  ...,  0.8346,  1.3013, -0.1488],\n",
      "        [-2.0501, -0.1247, -0.9331,  ..., -0.0074, -0.1914,  0.4798],\n",
      "        [ 0.5636, -0.3157, -0.2419,  ..., -0.8132,  0.7528, -0.6574]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.8882,  0.4793, -0.1500,  ...,  0.7287, -1.0408, -0.5452],\n",
      "        [ 2.1342,  0.3259,  0.7540,  ..., -0.6591,  1.8311, -0.0543],\n",
      "        [ 0.1319, -1.1916,  1.1246,  ..., -0.0155,  0.5256, -0.0378],\n",
      "        ...,\n",
      "        [ 0.2707, -0.2553, -0.7166,  ..., -1.2590, -1.0844, -0.0646],\n",
      "        [-1.5755,  0.3000,  0.0504,  ..., -0.6394, -1.2357,  0.4676],\n",
      "        [-0.1425, -0.5447, -0.7273,  ...,  0.5611, -1.7414, -0.6866]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.6818, -0.2777,  0.4186,  ...,  1.0987,  0.4867, -0.9401],\n",
      "        [-2.0605, -1.9233,  1.9062,  ...,  0.5867, -0.8896,  0.5577],\n",
      "        [ 0.4286,  0.6985,  0.7245,  ..., -0.0279,  2.1577, -1.3388],\n",
      "        ...,\n",
      "        [-0.4425, -0.9565,  1.4529,  ...,  0.0872,  0.9854, -0.2100],\n",
      "        [-0.2552,  0.8631, -0.5801,  ...,  0.0959,  0.0652,  1.2062],\n",
      "        [-0.5255, -0.8824,  0.3256,  ..., -0.4103,  0.1869, -1.3013]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.4699, -0.8055, -0.3998,  ..., -1.5447, -1.5341, -1.3002],\n",
      "        [ 0.0643,  1.5710,  1.3160,  ..., -0.3967,  1.2715, -1.4306],\n",
      "        [ 1.8984, -0.0891, -1.0885,  ..., -0.0700,  1.1656,  1.1726],\n",
      "        ...,\n",
      "        [ 0.3629, -0.2722,  1.3206,  ..., -0.2094, -1.8790, -0.7850],\n",
      "        [ 1.1808, -0.5263,  0.9476,  ...,  0.2771, -0.3606,  0.8109],\n",
      "        [ 0.3716,  0.2224,  0.2518,  ..., -0.6399,  0.0171,  0.1417]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[-0.0561,  1.2654, -0.4854,  ...,  0.7206,  0.2762, -1.0562],\n",
      "        [ 0.7091, -1.3291,  0.3007,  ...,  0.4886, -0.6621,  0.8654],\n",
      "        [ 1.0203, -0.4049, -0.4591,  ...,  1.5751, -0.9088, -0.8161],\n",
      "        ...,\n",
      "        [ 0.2786, -1.4101, -0.0444,  ...,  0.8522,  0.8582,  0.6949],\n",
      "        [ 0.3222, -0.8425,  0.9067,  ...,  0.3153, -0.4828,  0.4036],\n",
      "        [-1.0532, -0.3858,  0.8006,  ..., -0.4161, -0.4208, -2.0205]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.4556,  0.5534, -0.1553,  ...,  0.1947, -2.2498,  0.0508],\n",
      "        [ 0.5660, -0.6175, -0.0340,  ..., -0.4765,  1.2010,  0.2245],\n",
      "        [-0.4678,  0.0045,  0.2984,  ...,  0.6684,  0.0992,  0.3466],\n",
      "        ...,\n",
      "        [-1.0318, -0.5776, -0.1555,  ...,  1.4867,  0.0566, -0.7484],\n",
      "        [-1.3054, -0.3915,  0.0455,  ...,  0.0436,  0.1705,  0.1039],\n",
      "        [-0.3144, -0.1797, -0.6601,  ...,  0.4724, -0.1325, -0.1796]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.6230, -0.6827,  0.0202,  ...,  1.2694,  1.2017, -0.4906],\n",
      "        [ 1.3348,  0.8212, -0.5809,  ...,  0.1556, -0.4197, -0.1165],\n",
      "        [-0.9673, -0.8539, -0.7256,  ..., -0.6691,  0.6162,  0.8470],\n",
      "        ...,\n",
      "        [-1.5218, -0.3429,  0.0556,  ..., -0.8377,  0.5562, -0.9765],\n",
      "        [ 0.5316, -1.0662, -0.1211,  ..., -0.3782,  0.5648,  1.0768],\n",
      "        [ 0.7580, -2.3338,  1.2793,  ...,  1.2600,  0.9554,  0.1106]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.1020,  1.2833, -0.6872,  ..., -0.3753,  1.4020,  0.8183],\n",
      "        [ 0.1076, -0.3470,  2.0379,  ..., -0.6007,  0.1878,  0.2069],\n",
      "        [ 1.9776, -0.5202, -0.2938,  ...,  0.5726,  0.3822,  2.4815],\n",
      "        ...,\n",
      "        [ 0.5300, -0.8088, -0.0729,  ..., -0.7086,  1.2081,  0.1673],\n",
      "        [ 0.0311,  2.2385, -1.4195,  ...,  0.1224, -0.1395, -2.4206],\n",
      "        [ 1.0174, -0.8377,  0.9079,  ..., -0.0505, -0.6295, -0.8949]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.0616, -0.1029, -0.3204,  ..., -1.0950, -0.5595, -0.6179],\n",
      "        [-0.5023,  0.7415,  0.2874,  ..., -1.5804,  0.3898,  0.3630],\n",
      "        [-0.2389, -1.3479, -0.1397,  ...,  1.5002,  0.4559,  0.3996],\n",
      "        ...,\n",
      "        [-0.6389, -0.3793, -1.6567,  ..., -0.9663,  0.6075,  1.3172],\n",
      "        [-0.7961, -0.8053, -0.3617,  ...,  0.7157,  0.3481,  1.1787],\n",
      "        [ 0.4080,  0.6346,  1.8387,  ..., -1.7508, -0.4550, -0.3366]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.7033,  0.6244,  0.7045,  ...,  0.4215,  0.1691, -0.9802],\n",
      "        [ 0.2767,  0.9372,  0.1378,  ..., -0.7300,  2.6714, -0.2005],\n",
      "        [-0.0104, -0.3882,  0.0332,  ..., -0.3783,  0.1322,  0.4247],\n",
      "        ...,\n",
      "        [-1.3240,  0.8964, -2.8512,  ..., -0.5077,  0.0231, -0.4266],\n",
      "        [ 0.3673, -0.7945,  0.3409,  ..., -0.1007, -2.1692,  1.0767],\n",
      "        [-0.1265,  0.0187,  0.1024,  ...,  0.3632, -0.9538, -0.2132]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 0.7150, -1.7444,  0.1444,  ...,  0.1741, -0.8271,  1.5455],\n",
      "        [ 0.0933,  1.7799, -0.0058,  ..., -0.9338, -0.1735, -1.4301],\n",
      "        [-0.0674,  0.1236, -0.3097,  ...,  0.0188,  0.7163, -0.0306],\n",
      "        ...,\n",
      "        [ 1.9137,  1.1572, -0.2412,  ..., -0.4072,  0.4761, -1.2188],\n",
      "        [-0.3336, -0.8527,  1.4295,  ...,  0.6037,  0.4811, -0.6592],\n",
      "        [ 1.2183,  1.0982,  1.0044,  ...,  1.6421, -0.8722, -1.4706]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.1204, -1.0554,  0.5777,  ...,  0.8086,  0.7442, -1.5606],\n",
      "        [-0.5213,  0.2723, -1.3165,  ..., -0.0079, -0.8231,  0.2419],\n",
      "        [ 0.3512,  1.7385, -0.3076,  ..., -0.8061, -0.2321,  0.3570],\n",
      "        ...,\n",
      "        [-0.7731,  0.1861,  0.1049,  ..., -0.5315,  0.7463, -0.0766],\n",
      "        [-0.5104, -0.4268,  0.3173,  ...,  0.0289,  0.8232, -1.9184],\n",
      "        [-0.9108, -1.1498,  1.4342,  ...,  0.0724, -0.7445,  0.7808]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.6351, -0.5630,  1.2451,  ..., -1.3198,  1.1687,  0.2587],\n",
      "        [-1.1231, -0.5878,  0.7295,  ..., -0.7516,  0.2663, -0.3738],\n",
      "        [ 0.8076,  0.2688, -0.3278,  ...,  0.1686,  0.0025,  0.1391],\n",
      "        ...,\n",
      "        [ 0.1948, -0.4081,  0.5156,  ...,  0.1330,  0.7875,  0.1408],\n",
      "        [-0.3701,  1.5248, -1.2158,  ...,  1.4377,  0.2868,  0.3791],\n",
      "        [ 0.0786,  0.2439, -1.4307,  ...,  0.0915, -0.8732,  1.1519]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shape: torch.Size([2560, 320])\n",
      "tensor([[-0.1118, -0.6999, -0.0063,  ..., -1.0501,  0.5295, -0.4810],\n",
      "        [ 0.6856, -0.2103, -0.5142,  ...,  0.6371, -1.0494,  1.3037],\n",
      "        [-0.6494,  0.1547,  0.7737,  ..., -0.3552, -0.8472, -0.5828],\n",
      "        ...,\n",
      "        [-0.7157,  0.3347, -1.6242,  ..., -0.3623,  0.0277,  0.6124],\n",
      "        [-0.8111, -1.1657,  0.3088,  ..., -0.7825,  0.9086, -0.9220],\n",
      "        [-0.0838,  0.8348,  0.9158,  ...,  0.8853,  0.1548,  1.1371]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[ 1.1420, -0.0491,  1.3515,  ...,  1.2796,  0.5082, -0.2939],\n",
      "        [ 0.4592,  0.5024, -0.7702,  ...,  0.7542,  0.7107, -0.0414],\n",
      "        [ 0.6542, -0.0276, -1.3674,  ..., -0.2898, -0.2338,  0.5185],\n",
      "        ...,\n",
      "        [ 1.2870,  0.2660, -1.0313,  ...,  0.5515, -0.5069,  1.1800],\n",
      "        [ 0.2878, -1.1337, -0.6188,  ...,  0.0406,  1.3601, -1.8972],\n",
      "        [-0.6674, -0.4925,  0.2695,  ..., -0.7164, -0.0874, -0.6637]])\n",
      "Layer: unet.up_blocks.3.attentions.0.transformer_blocks.0.ff.net.2.lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.0893,  1.1797, -0.7131,  ..., -0.0180,  0.4595, -0.0915],\n",
      "        [-0.6108,  0.3553, -0.6761,  ..., -0.4159,  0.1327, -0.4636],\n",
      "        [-0.1561,  0.2060,  0.1146,  ...,  0.8276,  3.8125,  0.6414],\n",
      "        ...,\n",
      "        [-1.5770,  0.7744, -0.7478,  ...,  0.2624,  1.3397,  0.5662],\n",
      "        [ 0.3345, -0.2534, -0.2135,  ...,  0.0685, -1.3415, -1.0172],\n",
      "        [-0.8215,  0.2904, -1.1583,  ..., -0.1022, -0.9225, -0.5802]])\n",
      "Layer: unet.up_blocks.3.attentions.1.proj_in.lora.down.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[-1.1626]],\n",
      "\n",
      "         [[-1.1802]],\n",
      "\n",
      "         [[-1.1777]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3128]],\n",
      "\n",
      "         [[-1.3286]],\n",
      "\n",
      "         [[-1.5695]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1230]],\n",
      "\n",
      "         [[ 0.2837]],\n",
      "\n",
      "         [[-0.0616]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7298]],\n",
      "\n",
      "         [[-0.0298]],\n",
      "\n",
      "         [[ 0.1164]]],\n",
      "\n",
      "\n",
      "        [[[-0.9804]],\n",
      "\n",
      "         [[ 0.1458]],\n",
      "\n",
      "         [[-0.5812]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0909]],\n",
      "\n",
      "         [[ 0.2539]],\n",
      "\n",
      "         [[-0.6468]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.2129]],\n",
      "\n",
      "         [[-0.1848]],\n",
      "\n",
      "         [[-0.3653]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5154]],\n",
      "\n",
      "         [[-0.7866]],\n",
      "\n",
      "         [[ 0.0178]]],\n",
      "\n",
      "\n",
      "        [[[-0.0893]],\n",
      "\n",
      "         [[-0.0837]],\n",
      "\n",
      "         [[ 0.0601]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3012]],\n",
      "\n",
      "         [[ 0.5782]],\n",
      "\n",
      "         [[ 0.8665]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8681]],\n",
      "\n",
      "         [[-0.1409]],\n",
      "\n",
      "         [[-0.4766]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1535]],\n",
      "\n",
      "         [[ 0.6385]],\n",
      "\n",
      "         [[ 0.9536]]]])\n",
      "Layer: unet.up_blocks.3.attentions.1.proj_in.lora.up.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[ 0.8735]],\n",
      "\n",
      "         [[ 0.4103]],\n",
      "\n",
      "         [[-0.5906]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0303]],\n",
      "\n",
      "         [[-0.5694]],\n",
      "\n",
      "         [[-0.4729]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2467]],\n",
      "\n",
      "         [[-0.6360]],\n",
      "\n",
      "         [[-0.4016]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4513]],\n",
      "\n",
      "         [[ 1.3336]],\n",
      "\n",
      "         [[ 0.5808]]],\n",
      "\n",
      "\n",
      "        [[[-0.8218]],\n",
      "\n",
      "         [[-0.4951]],\n",
      "\n",
      "         [[ 0.5782]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3719]],\n",
      "\n",
      "         [[-1.1699]],\n",
      "\n",
      "         [[-0.4274]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.3987]],\n",
      "\n",
      "         [[ 0.0700]],\n",
      "\n",
      "         [[ 0.1615]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0830]],\n",
      "\n",
      "         [[-1.1703]],\n",
      "\n",
      "         [[-0.1447]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6053]],\n",
      "\n",
      "         [[ 0.0044]],\n",
      "\n",
      "         [[ 0.3308]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6075]],\n",
      "\n",
      "         [[-0.0393]],\n",
      "\n",
      "         [[-0.6221]]],\n",
      "\n",
      "\n",
      "        [[[-0.9951]],\n",
      "\n",
      "         [[ 0.7692]],\n",
      "\n",
      "         [[ 0.8569]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1222]],\n",
      "\n",
      "         [[ 0.1120]],\n",
      "\n",
      "         [[-0.8671]]]])\n",
      "Layer: unet.up_blocks.3.attentions.1.proj_out.lora.down.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[-0.3840]],\n",
      "\n",
      "         [[ 0.2563]],\n",
      "\n",
      "         [[-0.5624]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5017]],\n",
      "\n",
      "         [[ 0.0464]],\n",
      "\n",
      "         [[ 1.5116]]],\n",
      "\n",
      "\n",
      "        [[[-0.2170]],\n",
      "\n",
      "         [[-1.0013]],\n",
      "\n",
      "         [[-0.5218]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2696]],\n",
      "\n",
      "         [[-0.3458]],\n",
      "\n",
      "         [[-1.2231]]],\n",
      "\n",
      "\n",
      "        [[[-0.5904]],\n",
      "\n",
      "         [[ 0.2267]],\n",
      "\n",
      "         [[ 0.5189]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6387]],\n",
      "\n",
      "         [[ 0.4899]],\n",
      "\n",
      "         [[ 0.8746]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.6564]],\n",
      "\n",
      "         [[-0.7521]],\n",
      "\n",
      "         [[-0.2093]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2393]],\n",
      "\n",
      "         [[-0.4969]],\n",
      "\n",
      "         [[ 0.1236]]],\n",
      "\n",
      "\n",
      "        [[[-1.0032]],\n",
      "\n",
      "         [[-0.2313]],\n",
      "\n",
      "         [[-1.1046]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.8354]],\n",
      "\n",
      "         [[ 0.2219]],\n",
      "\n",
      "         [[ 1.1600]]],\n",
      "\n",
      "\n",
      "        [[[-0.2516]],\n",
      "\n",
      "         [[ 0.6324]],\n",
      "\n",
      "         [[ 0.0348]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0821]],\n",
      "\n",
      "         [[-0.1699]],\n",
      "\n",
      "         [[-0.2026]]]])\n",
      "Layer: unet.up_blocks.3.attentions.1.proj_out.lora.up.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[-0.6206]],\n",
      "\n",
      "         [[ 0.7502]],\n",
      "\n",
      "         [[ 0.1267]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1251]],\n",
      "\n",
      "         [[-0.9029]],\n",
      "\n",
      "         [[ 0.2675]]],\n",
      "\n",
      "\n",
      "        [[[-2.6338]],\n",
      "\n",
      "         [[-0.4133]],\n",
      "\n",
      "         [[-0.2992]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4474]],\n",
      "\n",
      "         [[-1.5422]],\n",
      "\n",
      "         [[-0.4000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6873]],\n",
      "\n",
      "         [[ 0.4115]],\n",
      "\n",
      "         [[-0.4971]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0589]],\n",
      "\n",
      "         [[ 0.8349]],\n",
      "\n",
      "         [[ 0.5659]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.4669]],\n",
      "\n",
      "         [[ 0.0494]],\n",
      "\n",
      "         [[ 0.4165]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1003]],\n",
      "\n",
      "         [[ 1.1982]],\n",
      "\n",
      "         [[-0.4752]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7845]],\n",
      "\n",
      "         [[ 0.1545]],\n",
      "\n",
      "         [[-0.9262]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0186]],\n",
      "\n",
      "         [[-0.8033]],\n",
      "\n",
      "         [[ 1.0858]]],\n",
      "\n",
      "\n",
      "        [[[-1.2601]],\n",
      "\n",
      "         [[ 0.6957]],\n",
      "\n",
      "         [[-0.1447]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0438]],\n",
      "\n",
      "         [[-0.6242]],\n",
      "\n",
      "         [[ 0.1177]]]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 1.6092, -0.7161, -0.6463,  ...,  0.4615, -1.2162,  0.7595],\n",
      "        [ 0.5790,  0.1258,  1.5166,  ...,  0.6009, -0.2913,  0.7592],\n",
      "        [ 0.0458,  0.2305, -0.5803,  ...,  0.6364,  0.3587,  0.0732],\n",
      "        ...,\n",
      "        [-0.1452, -1.6850,  0.7494,  ...,  0.4217,  0.1537,  0.5445],\n",
      "        [-1.3203,  2.7700, -0.1756,  ...,  0.9344,  0.2670, -0.5397],\n",
      "        [-0.4128, -0.6919,  0.2305,  ...,  0.8960, -0.3233,  0.0376]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.3926,  0.2198,  0.0334,  ..., -0.7699, -0.6337,  0.4717],\n",
      "        [ 0.3491,  0.4377,  0.4245,  ...,  0.0573, -0.2520,  0.5798],\n",
      "        [ 0.5920,  0.0332, -0.1562,  ...,  0.2092, -0.4359,  0.2588],\n",
      "        ...,\n",
      "        [ 0.3140,  0.3365,  1.0776,  ...,  0.9529, -1.7682,  0.4493],\n",
      "        [ 0.8841, -0.6319, -0.5013,  ..., -0.0674, -1.2265,  0.6388],\n",
      "        [ 0.4124, -0.1152,  0.4522,  ...,  0.0471, -1.2728,  0.7405]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.7917,  0.4782,  0.8553,  ..., -0.8729,  1.5896, -0.5232],\n",
      "        [ 0.3479, -0.5678,  0.9980,  ...,  0.3130,  0.5683, -0.0375],\n",
      "        [-0.1891,  0.2594,  0.6881,  ...,  0.8942, -0.1606,  1.0176],\n",
      "        ...,\n",
      "        [ 0.1300,  0.1030,  0.4254,  ..., -0.0546,  0.5470, -0.2730],\n",
      "        [ 2.3229, -0.6737,  1.0350,  ..., -3.3144, -0.0314,  0.2763],\n",
      "        [-0.1567, -0.5091, -0.2638,  ..., -0.4127,  0.4479,  1.0381]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-1.1087,  0.7790,  0.7721,  ...,  0.0200,  0.2575,  0.3013],\n",
      "        [-0.0813, -0.0862, -0.3736,  ..., -1.5983,  0.4833,  0.5483],\n",
      "        [ 0.8313,  0.9515, -0.6014,  ..., -0.1772,  0.4993, -0.1597],\n",
      "        ...,\n",
      "        [ 0.3256, -0.1466,  1.1505,  ..., -0.1691,  0.4131,  1.2665],\n",
      "        [-0.5913,  0.0912,  0.0377,  ..., -0.2268,  1.6117,  0.3241],\n",
      "        [-1.4789,  0.6134, -0.7859,  ...,  0.2902, -0.4657, -0.7196]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.4905, -0.8083,  0.7189,  ..., -0.5860, -0.4013, -0.3318],\n",
      "        [ 1.3646, -1.0817, -0.7542,  ...,  0.5005, -0.0646,  0.7093],\n",
      "        [ 0.2491, -0.4401, -0.3137,  ...,  0.0599,  1.9192, -0.6484],\n",
      "        ...,\n",
      "        [ 1.3194, -1.0795, -0.3280,  ..., -0.1529,  0.5106,  0.3384],\n",
      "        [-0.3801, -1.1541, -1.9127,  ..., -0.6605,  0.0359,  0.3041],\n",
      "        [ 0.2101, -0.3664, -1.0579,  ...,  0.2806,  1.0213,  0.4064]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.6927, -0.0040,  0.2422,  ..., -0.0237, -0.7843, -0.3875],\n",
      "        [-1.6504,  0.4105,  0.2439,  ..., -0.0975, -1.5227,  0.1393],\n",
      "        [-0.3319, -0.1044,  1.0744,  ..., -0.4080,  1.6171,  0.0834],\n",
      "        ...,\n",
      "        [-2.3958, -0.1937, -0.5924,  ..., -1.1910, -2.5324,  0.3094],\n",
      "        [-1.4825, -0.0649, -0.5016,  ...,  0.4480, -1.1497,  1.3543],\n",
      "        [-0.9911,  0.5147, -0.4353,  ..., -0.1494, -1.7112,  0.3912]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.8606,  0.1592,  0.6473,  ..., -0.3816,  0.1732,  0.4007],\n",
      "        [ 0.2568,  0.4753,  0.2288,  ..., -0.8055, -1.0016, -0.3461],\n",
      "        [ 0.6701,  0.0486, -0.7673,  ...,  0.4968,  0.4588,  0.0439],\n",
      "        ...,\n",
      "        [-0.7607, -0.5227, -0.5240,  ...,  0.5712, -0.0692, -0.1220],\n",
      "        [-0.2120, -0.6844, -0.5141,  ...,  0.6686, -0.7091, -0.4902],\n",
      "        [-0.2968, -0.2619, -0.6022,  ..., -0.5173, -0.9363, -0.6785]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.2408, -0.3766, -0.4606,  ..., -0.5827,  1.0060,  0.4748],\n",
      "        [-0.9602, -0.3526,  0.0503,  ...,  0.3335,  1.6284, -0.1740],\n",
      "        [-0.4687, -1.6300, -0.3255,  ..., -0.5460, -0.3494, -0.6455],\n",
      "        ...,\n",
      "        [ 1.1807, -0.1145,  0.3461,  ...,  0.9100, -1.7763, -0.1209],\n",
      "        [-1.0623, -0.0642,  0.5960,  ..., -0.6145, -1.3315, -0.2112],\n",
      "        [ 0.3243,  0.0964, -0.0374,  ..., -0.0920,  0.0480, -1.2818]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[-0.8683, -1.5961, -2.2988,  ..., -0.0460, -0.8280,  0.3524],\n",
      "        [-0.7860,  1.2811,  1.0652,  ...,  1.0589, -1.0930, -1.0859],\n",
      "        [ 0.8368,  0.4456, -1.2564,  ...,  0.3685, -0.7281, -0.9265],\n",
      "        ...,\n",
      "        [-1.4006,  0.2116, -0.6513,  ..., -0.4503,  1.1952, -0.6481],\n",
      "        [ 0.7963,  0.9046,  0.4148,  ..., -0.0797, -0.5197,  1.0046],\n",
      "        [ 0.5343,  1.1294, -0.0969,  ...,  1.3695, -0.8363,  0.3556]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.6177, -0.3165, -0.0607,  ...,  0.1747, -1.3391, -0.7716],\n",
      "        [-1.0512,  0.3228, -1.0462,  ..., -0.0596,  2.6641,  0.1338],\n",
      "        [-1.0000,  0.3180,  0.0274,  ..., -0.2823,  2.4695, -0.2310],\n",
      "        ...,\n",
      "        [ 0.5708, -0.7671, -0.5664,  ..., -0.2629, -0.2755, -0.2777],\n",
      "        [ 0.8422,  0.1071, -0.3714,  ..., -0.2974,  0.7217, -0.0892],\n",
      "        [ 0.1179,  0.5098, -0.9218,  ..., -0.4741,  0.8034,  0.0692]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.3104,  0.1221,  0.5491,  ...,  0.3757,  0.7776,  2.3253],\n",
      "        [ 0.5407,  1.2423,  0.6477,  ..., -0.1264, -1.6600,  0.4036],\n",
      "        [-0.1139, -0.0680,  0.9703,  ..., -0.2155, -0.7354,  0.1391],\n",
      "        ...,\n",
      "        [ 0.1161, -0.3969,  1.9319,  ...,  0.6689, -0.7905, -0.6199],\n",
      "        [-1.6594, -0.3733, -0.3022,  ...,  1.4449,  1.0589,  0.5667],\n",
      "        [ 1.2584, -0.4448,  0.2921,  ...,  0.1153,  0.5788, -0.4539]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 1.5091e+00,  9.2110e-01,  8.1032e-01,  ...,  1.1444e+00,\n",
      "         -9.7868e-01,  7.0474e-01],\n",
      "        [-1.1266e+00,  1.0899e+00, -4.7787e-01,  ...,  6.6729e-01,\n",
      "         -5.7079e-01, -2.9936e-01],\n",
      "        [ 3.0244e-01, -9.5457e-01,  1.0882e+00,  ...,  1.8246e-01,\n",
      "         -1.2163e+00, -9.1382e-01],\n",
      "        ...,\n",
      "        [-1.3703e-01, -2.2627e+00,  9.4694e-01,  ..., -1.1019e+00,\n",
      "          1.3208e+00, -1.1337e+00],\n",
      "        [-3.3868e-01,  3.8908e-01,  1.2365e+00,  ...,  1.0293e+00,\n",
      "         -2.2379e-01,  3.3193e-01],\n",
      "        [ 1.2253e+00, -1.1767e-03, -1.2561e+00,  ...,  3.6182e-01,\n",
      "          2.9196e-01, -1.7235e+00]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.1283, -0.0917,  0.1862,  ...,  0.0936, -0.9562,  0.4745],\n",
      "        [ 1.0906,  0.3326,  1.6032,  ..., -0.0237,  0.8586, -1.0185],\n",
      "        [-1.2275, -0.5347, -0.9739,  ...,  0.0369,  0.8514, -1.2251],\n",
      "        ...,\n",
      "        [ 0.2076,  0.4907,  0.2267,  ..., -0.5245,  1.8130, -0.7989],\n",
      "        [ 1.8898,  0.7673,  2.1943,  ...,  2.3597,  0.0550,  0.1699],\n",
      "        [-0.5684, -0.4120,  0.4685,  ...,  0.3084,  0.7313,  0.5995]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.2209,  0.0885,  0.3655,  ..., -0.1215, -0.3355,  0.1230],\n",
      "        [ 0.0140, -0.8069,  0.6153,  ..., -0.0474, -3.2790,  0.1547],\n",
      "        [-0.3446, -0.0039,  0.5334,  ...,  0.7441, -0.3721, -0.0404],\n",
      "        ...,\n",
      "        [-0.0033, -0.5764, -0.1246,  ..., -0.5259,  2.0664,  0.0737],\n",
      "        [-0.4628,  0.3666, -0.4192,  ...,  0.5693, -2.4565, -0.1554],\n",
      "        [ 0.7139,  0.0688,  0.5462,  ..., -0.5118, -0.4904, -0.5174]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 2.4421,  1.3364,  1.4023,  ..., -2.2525,  0.0139, -1.4478],\n",
      "        [ 0.0407, -1.2085, -1.1214,  ..., -0.4962, -0.7157,  0.6463],\n",
      "        [ 0.8570, -0.1513, -1.9501,  ..., -0.5105, -0.6594,  0.3246],\n",
      "        ...,\n",
      "        [-0.6218,  0.0224,  0.2535,  ...,  1.8602,  0.8479, -0.0776],\n",
      "        [-0.0914,  1.7703, -0.8833,  ..., -1.7588,  1.7162, -0.2641],\n",
      "        [ 0.2214,  1.8124,  1.7264,  ..., -0.1930,  1.7098, -1.3018]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.0314,  0.7509, -0.4852,  ...,  0.8083,  0.1072,  1.9628],\n",
      "        [ 1.0362, -1.8477, -0.6671,  ..., -0.0671, -0.5959, -3.0106],\n",
      "        [-0.9844, -1.7668,  0.2428,  ..., -1.2215, -1.0202, -0.5086],\n",
      "        ...,\n",
      "        [ 1.1047,  0.4595,  0.4472,  ..., -0.8864,  0.4948, -2.5480],\n",
      "        [-0.4654, -0.2284, -0.2954,  ...,  0.6910,  0.8535,  2.5473],\n",
      "        [ 1.3977, -1.1247,  1.0035,  ..., -0.1768, -0.0126, -2.6077]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.3716, -0.2506, -0.3882,  ..., -0.1481,  1.0126,  0.7457],\n",
      "        [ 0.3431,  0.4817, -0.7031,  ...,  0.6335,  0.0901, -0.0763],\n",
      "        [-0.2411,  0.1443, -0.7727,  ...,  0.1397,  0.6857, -0.2487],\n",
      "        ...,\n",
      "        [-0.5149,  0.7114, -1.0390,  ..., -1.1298,  0.5341, -0.2534],\n",
      "        [-0.0090,  1.0985,  2.1451,  ...,  1.2071, -1.4840, -1.4936],\n",
      "        [-0.4089, -0.1039,  0.7106,  ..., -0.7963, -0.0261, -0.6616]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shape: torch.Size([2560, 320])\n",
      "tensor([[-0.8805,  0.4112,  1.3028,  ..., -0.4798, -0.6030, -0.0389],\n",
      "        [ 0.3493,  0.5503,  0.1301,  ...,  0.9536,  0.3400, -1.8319],\n",
      "        [-1.8670,  0.0882, -0.1596,  ..., -0.5068, -0.3129,  0.9755],\n",
      "        ...,\n",
      "        [-1.5384, -0.2906,  2.0622,  ..., -0.6696,  0.4574, -0.1254],\n",
      "        [-0.4136,  1.3981,  0.7089,  ..., -0.2552,  1.1159,  0.7287],\n",
      "        [ 1.1087, -0.3714, -0.3216,  ..., -0.5961,  1.2312,  0.2190]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.5964, -1.2257, -0.9342,  ...,  0.4611, -0.4196, -0.0131],\n",
      "        [ 1.0239, -0.2643, -1.0043,  ..., -0.1977, -0.2690,  0.7267],\n",
      "        [-0.3206,  0.7618, -0.5822,  ..., -0.7561,  1.2347, -0.5097],\n",
      "        ...,\n",
      "        [ 0.4552,  0.5003,  0.5020,  ..., -0.9469,  0.1273,  0.2189],\n",
      "        [-0.6160,  0.0112,  1.0115,  ...,  1.1532,  1.2771, -1.9069],\n",
      "        [ 0.2237, -0.2768, -0.1705,  ..., -0.8416,  0.2037, -0.8345]])\n",
      "Layer: unet.up_blocks.3.attentions.1.transformer_blocks.0.ff.net.2.lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.5186,  0.4427, -0.3495,  ..., -0.1105,  0.8963,  0.7154],\n",
      "        [ 1.3139, -0.5432, -0.6866,  ...,  0.0811, -0.4282,  0.3779],\n",
      "        [-2.0888,  0.3068,  0.8061,  ..., -0.5512,  0.8662, -0.2404],\n",
      "        ...,\n",
      "        [-0.1482, -0.1684,  0.9079,  ..., -0.3089, -0.1106,  0.3041],\n",
      "        [-0.5066, -0.1141, -0.6182,  ...,  0.2387, -0.3525, -0.5665],\n",
      "        [ 1.0849,  0.7190,  0.6804,  ..., -0.4532, -0.0030, -0.0185]])\n",
      "Layer: unet.up_blocks.3.attentions.2.proj_in.lora.down.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[ 0.3711]],\n",
      "\n",
      "         [[ 0.0362]],\n",
      "\n",
      "         [[ 0.3655]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4392]],\n",
      "\n",
      "         [[-1.7112]],\n",
      "\n",
      "         [[ 1.4309]]],\n",
      "\n",
      "\n",
      "        [[[-0.4631]],\n",
      "\n",
      "         [[-0.9884]],\n",
      "\n",
      "         [[ 1.3528]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0449]],\n",
      "\n",
      "         [[ 0.3861]],\n",
      "\n",
      "         [[ 0.4666]]],\n",
      "\n",
      "\n",
      "        [[[-1.5934]],\n",
      "\n",
      "         [[ 0.2707]],\n",
      "\n",
      "         [[-0.8053]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.3845]],\n",
      "\n",
      "         [[-0.9934]],\n",
      "\n",
      "         [[ 0.4067]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.8774]],\n",
      "\n",
      "         [[-0.7427]],\n",
      "\n",
      "         [[-0.2090]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8064]],\n",
      "\n",
      "         [[-0.5935]],\n",
      "\n",
      "         [[-0.0892]]],\n",
      "\n",
      "\n",
      "        [[[-0.2146]],\n",
      "\n",
      "         [[-1.3842]],\n",
      "\n",
      "         [[-1.6814]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5138]],\n",
      "\n",
      "         [[ 2.5527]],\n",
      "\n",
      "         [[ 0.7685]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4554]],\n",
      "\n",
      "         [[-0.3755]],\n",
      "\n",
      "         [[ 0.7174]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2373]],\n",
      "\n",
      "         [[-1.1875]],\n",
      "\n",
      "         [[ 1.0252]]]])\n",
      "Layer: unet.up_blocks.3.attentions.2.proj_in.lora.up.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[-0.6276]],\n",
      "\n",
      "         [[-0.4345]],\n",
      "\n",
      "         [[-0.7451]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4469]],\n",
      "\n",
      "         [[-0.5807]],\n",
      "\n",
      "         [[-2.9497]]],\n",
      "\n",
      "\n",
      "        [[[ 0.7486]],\n",
      "\n",
      "         [[ 0.1268]],\n",
      "\n",
      "         [[-0.8987]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3215]],\n",
      "\n",
      "         [[-1.1546]],\n",
      "\n",
      "         [[-0.4499]]],\n",
      "\n",
      "\n",
      "        [[[-3.1405]],\n",
      "\n",
      "         [[-0.9954]],\n",
      "\n",
      "         [[-0.2798]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3050]],\n",
      "\n",
      "         [[ 1.8510]],\n",
      "\n",
      "         [[-0.6485]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.5842]],\n",
      "\n",
      "         [[-0.1093]],\n",
      "\n",
      "         [[ 0.2487]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5567]],\n",
      "\n",
      "         [[-1.4421]],\n",
      "\n",
      "         [[-0.1258]]],\n",
      "\n",
      "\n",
      "        [[[-0.2951]],\n",
      "\n",
      "         [[ 1.3113]],\n",
      "\n",
      "         [[ 0.5999]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7108]],\n",
      "\n",
      "         [[ 0.0297]],\n",
      "\n",
      "         [[ 1.3819]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9615]],\n",
      "\n",
      "         [[ 0.0354]],\n",
      "\n",
      "         [[-0.1394]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9621]],\n",
      "\n",
      "         [[ 1.3116]],\n",
      "\n",
      "         [[-0.5518]]]])\n",
      "Layer: unet.up_blocks.3.attentions.2.proj_out.lora.down.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[ 0.1497]],\n",
      "\n",
      "         [[-0.5133]],\n",
      "\n",
      "         [[ 0.5982]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.1043]],\n",
      "\n",
      "         [[-0.0790]],\n",
      "\n",
      "         [[-0.2527]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8468]],\n",
      "\n",
      "         [[ 1.0889]],\n",
      "\n",
      "         [[-0.7764]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2057]],\n",
      "\n",
      "         [[ 0.2132]],\n",
      "\n",
      "         [[-1.2277]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8630]],\n",
      "\n",
      "         [[-2.1640]],\n",
      "\n",
      "         [[-0.1099]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.9205]],\n",
      "\n",
      "         [[-0.3171]],\n",
      "\n",
      "         [[ 1.5372]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.5363]],\n",
      "\n",
      "         [[-0.8814]],\n",
      "\n",
      "         [[ 1.4758]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.5628]],\n",
      "\n",
      "         [[ 0.5474]],\n",
      "\n",
      "         [[-0.0360]]],\n",
      "\n",
      "\n",
      "        [[[-1.8304]],\n",
      "\n",
      "         [[ 1.1605]],\n",
      "\n",
      "         [[ 0.4184]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.9358]],\n",
      "\n",
      "         [[ 0.1032]],\n",
      "\n",
      "         [[ 0.6717]]],\n",
      "\n",
      "\n",
      "        [[[ 0.9788]],\n",
      "\n",
      "         [[ 1.6711]],\n",
      "\n",
      "         [[-1.7570]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1215]],\n",
      "\n",
      "         [[-0.2693]],\n",
      "\n",
      "         [[-0.8501]]]])\n",
      "Layer: unet.up_blocks.3.attentions.2.proj_out.lora.up.weight, Shape: torch.Size([320, 320, 1, 1])\n",
      "tensor([[[[ 0.5954]],\n",
      "\n",
      "         [[-0.6425]],\n",
      "\n",
      "         [[-0.4003]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.6182]],\n",
      "\n",
      "         [[ 0.1269]],\n",
      "\n",
      "         [[ 0.1492]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3941]],\n",
      "\n",
      "         [[ 0.3110]],\n",
      "\n",
      "         [[-0.1816]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1367]],\n",
      "\n",
      "         [[-1.6670]],\n",
      "\n",
      "         [[-0.4936]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2117]],\n",
      "\n",
      "         [[-0.4860]],\n",
      "\n",
      "         [[ 1.5473]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.3651]],\n",
      "\n",
      "         [[-1.0339]],\n",
      "\n",
      "         [[-2.5201]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.0162]],\n",
      "\n",
      "         [[-0.6688]],\n",
      "\n",
      "         [[ 0.8716]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.4870]],\n",
      "\n",
      "         [[ 1.1597]],\n",
      "\n",
      "         [[-0.9459]]],\n",
      "\n",
      "\n",
      "        [[[-1.7240]],\n",
      "\n",
      "         [[ 0.3683]],\n",
      "\n",
      "         [[ 0.4088]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.8677]],\n",
      "\n",
      "         [[ 0.6605]],\n",
      "\n",
      "         [[-0.6423]]],\n",
      "\n",
      "\n",
      "        [[[-0.3012]],\n",
      "\n",
      "         [[ 0.2875]],\n",
      "\n",
      "         [[-0.2291]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.3412]],\n",
      "\n",
      "         [[-0.1084]],\n",
      "\n",
      "         [[ 1.4636]]]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-1.4311e+00,  5.7902e-01, -1.0540e+00,  ..., -1.0040e+00,\n",
      "          3.2384e-01,  1.2974e+00],\n",
      "        [-2.7730e-01, -6.5599e-02, -3.8937e-01,  ...,  7.6160e-01,\n",
      "          1.8175e-03,  4.1680e-01],\n",
      "        [ 1.7100e-01,  5.7132e-01, -6.0058e-02,  ..., -1.3758e-01,\n",
      "         -6.6354e-01, -8.6038e-02],\n",
      "        ...,\n",
      "        [ 5.9408e-01,  3.7773e-01,  4.9979e-01,  ...,  3.2294e-01,\n",
      "          5.2885e-01,  2.4121e-01],\n",
      "        [ 2.1698e+00,  5.6267e-01,  2.8034e+00,  ...,  1.2560e-01,\n",
      "          7.1800e-01, -8.7583e-01],\n",
      "        [-4.4364e-01,  2.2408e-01,  3.7240e-01,  ..., -3.1429e-01,\n",
      "         -9.1599e-02,  2.6592e-01]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_k_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.3795, -0.6287, -0.5324,  ..., -0.3000,  1.0243,  0.3569],\n",
      "        [-0.6788,  0.1128, -0.2868,  ...,  0.4675,  0.7963,  0.3703],\n",
      "        [ 0.4524,  0.3930,  0.2003,  ...,  0.4266, -0.2778, -0.1369],\n",
      "        ...,\n",
      "        [ 0.9687, -0.2741,  1.1205,  ..., -0.1560, -1.4726, -0.0591],\n",
      "        [ 1.9241, -0.2183, -0.6438,  ...,  0.1272, -1.2210, -0.0346],\n",
      "        [ 1.2169,  0.2360, -0.3995,  ...,  0.0161, -0.2436, -0.4913]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.1545, -0.0094, -0.7768,  ...,  0.5129,  0.5301, -0.5990],\n",
      "        [ 0.3775, -0.6519,  0.0286,  ...,  0.3468,  0.5228,  0.9654],\n",
      "        [-0.8184, -0.6578,  0.0502,  ...,  0.0560, -0.4562,  0.6705],\n",
      "        ...,\n",
      "        [ 0.6707,  0.1709, -0.6161,  ...,  0.6899,  0.9844,  0.2776],\n",
      "        [-0.4780, -0.2999, -0.5831,  ...,  1.3693, -1.0987, -1.8149],\n",
      "        [-0.7844, -1.6936,  0.3254,  ..., -1.5453, -0.9012,  0.7872]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_out_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.4411, -0.2831, -0.8355,  ...,  0.7978, -0.6348,  0.1302],\n",
      "        [-1.1687,  0.4610,  0.6273,  ..., -0.6718,  1.6089, -0.0587],\n",
      "        [-1.5092,  0.6366, -2.1822,  ..., -0.4546,  1.3393, -0.8371],\n",
      "        ...,\n",
      "        [ 0.9083, -0.7675, -0.3038,  ...,  0.8598, -2.4739, -0.0148],\n",
      "        [-0.4729,  0.2327,  0.3252,  ...,  0.8529, -0.1241,  0.7297],\n",
      "        [ 0.2282, -0.1065,  0.1207,  ..., -0.4038, -0.0139, -0.3876]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.0047,  1.4989,  0.1616,  ...,  0.3399, -0.3163, -1.4184],\n",
      "        [-0.0827,  0.1648, -0.0294,  ...,  0.2995, -0.5772,  0.8113],\n",
      "        [ 0.2065, -0.0044, -0.4262,  ...,  0.1753, -0.2518, -0.5750],\n",
      "        ...,\n",
      "        [ 0.1303, -0.0029, -0.1172,  ..., -0.6121, -0.4678,  0.0286],\n",
      "        [-0.7471, -0.9741, -2.2842,  ...,  0.1488, -0.4798,  0.7156],\n",
      "        [-0.1860,  0.2652,  0.1319,  ...,  0.1462, -0.3768,  0.1849]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_q_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.4079,  0.3818,  0.3725,  ..., -0.1995, -1.0771,  0.1009],\n",
      "        [-0.2351,  0.1745, -1.1546,  ..., -0.5525,  0.3749, -0.2330],\n",
      "        [ 0.7572, -0.5366,  0.4132,  ..., -0.2759,  0.2736, -0.0091],\n",
      "        ...,\n",
      "        [ 3.0324,  0.4779, -0.0055,  ..., -0.2743, -1.8012,  0.1119],\n",
      "        [ 1.9165, -0.1595, -0.0338,  ..., -0.0189, -0.4566,  0.1154],\n",
      "        [-0.4938,  0.1751, -0.5029,  ...,  0.1275,  0.3022, -0.4464]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.6558, -0.9801, -0.6672,  ..., -1.8646,  0.7668,  0.1734],\n",
      "        [-0.2479, -0.0232,  0.1668,  ..., -0.6564, -0.5695, -0.2808],\n",
      "        [-0.0845, -0.4637,  0.3282,  ..., -0.4741,  0.1676,  0.0114],\n",
      "        ...,\n",
      "        [ 0.8721,  0.3504, -1.0381,  ..., -0.0532,  1.1238, -0.5170],\n",
      "        [-1.1837, -0.2767,  0.2189,  ...,  1.4612,  1.9422, -0.2529],\n",
      "        [-0.9030,  0.0030, -0.1204,  ...,  0.8233, -0.5748,  1.5797]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor.to_v_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.4400, -0.9878,  0.2176,  ..., -0.5627, -0.8832,  0.8781],\n",
      "        [-0.5836,  0.5114,  0.6702,  ..., -0.3335,  1.4458,  0.2803],\n",
      "        [-0.7401,  0.5022, -0.4496,  ..., -0.4440,  1.3973, -0.1822],\n",
      "        ...,\n",
      "        [-0.4979,  1.3961, -0.2457,  ...,  1.5267,  0.3609,  0.6719],\n",
      "        [ 0.2781,  0.1635, -0.4958,  ..., -0.0689, -0.2548,  0.7924],\n",
      "        [ 0.6183, -0.7108, -0.3456,  ..., -0.9040,  1.6200, -0.0442]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 0.9304, -0.1210, -0.5899,  ...,  0.8157,  0.3186, -0.8710],\n",
      "        [-2.6069, -0.0383, -0.8836,  ...,  2.0116,  0.6864, -0.9419],\n",
      "        [-0.7707,  0.3524,  0.3903,  ..., -1.1236, -0.9762, -0.5759],\n",
      "        ...,\n",
      "        [-0.5270, -0.4030,  0.0514,  ...,  1.3748,  0.6861, -1.4986],\n",
      "        [-1.5327, -1.3753,  0.4782,  ..., -0.7008,  0.0775,  0.4522],\n",
      "        [ 0.2763,  0.3156,  0.3316,  ...,  0.5882, -0.6302, -0.1265]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_k_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.3965, -0.3553, -0.2780,  ...,  0.2873,  1.8552, -0.4570],\n",
      "        [ 0.6367, -0.6859,  0.2861,  ...,  0.6490, -2.4265,  0.0784],\n",
      "        [-0.3216,  0.9166,  1.1734,  ...,  0.2481, -1.0871, -0.0386],\n",
      "        ...,\n",
      "        [ 0.1618, -0.5589, -0.1016,  ...,  0.3136,  0.8967,  0.3064],\n",
      "        [-0.2033, -0.2852, -0.2601,  ...,  0.4876,  0.1791, -0.0421],\n",
      "        [ 0.3853,  0.0705,  0.1183,  ...,  0.0751, -1.4067,  0.2394]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.0044,  0.8300,  0.4856,  ..., -0.5428, -0.1094, -0.5735],\n",
      "        [ 0.2826, -1.4050,  2.6470,  ...,  0.7577, -0.7367, -0.0478],\n",
      "        [ 1.9780, -2.6933,  1.5357,  ..., -0.1989, -1.2489, -0.0075],\n",
      "        ...,\n",
      "        [ 0.5311, -0.8385, -0.6762,  ...,  1.1356,  0.9251,  1.5370],\n",
      "        [ 0.0740,  0.9028,  0.9375,  ..., -1.2145,  0.1489,  1.5615],\n",
      "        [-0.6231, -0.2658, -0.7543,  ...,  1.2562,  1.3633, -0.4798]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_out_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-2.0554,  1.5665,  0.3367,  ..., -1.1430, -0.0076, -1.4652],\n",
      "        [ 0.6542,  1.0257, -0.9110,  ..., -0.1525, -1.2684,  1.4040],\n",
      "        [-0.8282, -0.2143,  0.2889,  ..., -0.6086, -1.1023, -1.1109],\n",
      "        ...,\n",
      "        [ 0.4577, -0.3692, -1.0432,  ...,  0.9981,  1.2699,  0.6551],\n",
      "        [-1.1257, -0.3288,  0.4620,  ...,  1.4611,  0.3852,  1.6212],\n",
      "        [ 1.0284, -2.0483, -0.8797,  ...,  1.1149,  0.8047,  0.7410]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-1.2968,  1.5729,  0.8974,  ..., -0.3435,  0.4826,  1.2332],\n",
      "        [-0.0117, -1.7745, -0.7969,  ..., -0.5658, -1.1112, -0.1887],\n",
      "        [-0.3910, -1.1830, -0.6924,  ..., -0.2782,  0.3214, -0.8608],\n",
      "        ...,\n",
      "        [-1.5474, -0.3784,  0.1230,  ...,  0.1699, -1.1760,  1.5025],\n",
      "        [ 2.2420,  1.4249, -0.7127,  ..., -0.6388, -0.5325,  1.8845],\n",
      "        [ 1.1219,  0.1327,  0.9035,  ..., -1.1386,  2.1671,  0.5977]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_q_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-1.6128e+00,  8.1649e-01, -2.7599e-01,  ...,  3.4068e-01,\n",
      "          5.8313e+00, -8.6588e-01],\n",
      "        [-2.4159e+00, -3.7209e-01, -8.8375e-01,  ..., -1.5162e+00,\n",
      "         -1.0323e+00, -1.8172e-01],\n",
      "        [-1.4133e+00,  1.6977e-01,  4.7138e-01,  ..., -1.4394e+00,\n",
      "          3.9668e+00, -1.7072e+00],\n",
      "        ...,\n",
      "        [ 2.2741e-02,  2.5548e-01,  8.0456e-01,  ...,  2.0186e-01,\n",
      "          1.8896e+00,  2.2332e-01],\n",
      "        [-9.3583e-01, -7.4598e-02,  3.2878e-03,  ..., -1.3434e-02,\n",
      "          3.0294e+00, -9.9053e-03],\n",
      "        [-7.9010e-01, -3.9310e-01, -5.3245e-01,  ...,  3.2350e-01,\n",
      "         -6.4471e-01,  8.3636e-02]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.down.weight, Shape: torch.Size([320, 768])\n",
      "tensor([[ 1.2194, -1.8605, -0.1422,  ..., -0.0905,  0.2131, -1.8005],\n",
      "        [-0.5005,  0.1190, -0.2962,  ..., -0.7240, -1.9375, -0.0932],\n",
      "        [-0.6690,  0.3750,  0.5708,  ..., -0.5739,  0.1878,  1.6133],\n",
      "        ...,\n",
      "        [-1.0475, -0.8312, -0.0553,  ...,  2.0269,  1.1343, -0.4546],\n",
      "        [-0.1605,  1.5488, -1.1591,  ...,  0.2490,  1.4987,  0.1277],\n",
      "        [-0.5182,  0.6366, -1.2854,  ..., -0.7867,  1.6145,  0.4622]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor.to_v_lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[ 0.1312,  0.0590,  1.3771,  ..., -1.0942,  1.8077,  1.5769],\n",
      "        [-0.2865,  1.5393,  0.6314,  ..., -0.8061, -0.0194, -0.5849],\n",
      "        [-0.7223,  2.6336, -0.4220,  ...,  1.1711,  0.2121, -0.8053],\n",
      "        ...,\n",
      "        [ 0.4383, -0.3070, -0.0097,  ...,  1.0524,  1.7070,  0.6763],\n",
      "        [ 0.9198,  0.5996, -0.7082,  ..., -0.7322, -0.0777,  0.0453],\n",
      "        [ 0.7857,  0.3321, -1.4107,  ...,  0.2767,  1.4581, -1.5840]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.down.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-1.3126,  0.5167, -2.2809,  ...,  1.0352, -1.6525, -0.6605],\n",
      "        [ 0.4942,  0.2029, -0.0676,  ...,  0.9549,  0.2039,  0.2647],\n",
      "        [-0.7188, -0.4117, -0.2940,  ...,  0.4720,  0.4186, -0.1101],\n",
      "        ...,\n",
      "        [-0.1858,  0.4546,  1.0327,  ...,  0.4232, -0.8685,  0.4902],\n",
      "        [ 0.4343, -0.1630,  0.7435,  ..., -0.0643,  0.1137, -0.0598],\n",
      "        [-1.3284,  0.3126,  0.4750,  ...,  0.8340, -0.2841,  0.1119]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.0.proj.lora.up.weight, Shape: torch.Size([2560, 320])\n",
      "tensor([[ 1.0273,  0.9930,  0.7280,  ..., -0.6491,  1.0802,  0.5935],\n",
      "        [-1.6307,  0.3901,  1.5471,  ..., -0.5234, -1.5482, -0.6960],\n",
      "        [ 0.3244, -0.3889, -0.6330,  ...,  0.5511, -1.5440,  0.0144],\n",
      "        ...,\n",
      "        [ 0.3523,  0.1333, -0.7707,  ...,  0.5600,  0.3832,  0.5692],\n",
      "        [-0.2775, -0.0993,  0.3494,  ...,  0.5690, -0.2304,  0.1553],\n",
      "        [-0.5232,  0.5407,  0.6076,  ..., -0.6973,  0.3466,  0.2939]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.lora.down.weight, Shape: torch.Size([320, 1280])\n",
      "tensor([[-0.5774,  0.4550,  0.0393,  ...,  0.8810, -0.4370, -0.3017],\n",
      "        [-0.9790,  0.1875, -1.5931,  ...,  0.5111,  0.2338, -0.6194],\n",
      "        [-1.1097, -0.6207, -0.9341,  ...,  0.4352, -0.3134, -1.3948],\n",
      "        ...,\n",
      "        [-0.5921,  0.4364, -0.7045,  ..., -1.3602, -0.8962, -0.7577],\n",
      "        [-0.4487,  0.7581, -0.2954,  ..., -0.5001, -1.3246,  1.1477],\n",
      "        [ 0.6914, -0.8691, -1.1592,  ..., -0.2955, -1.4489,  0.6200]])\n",
      "Layer: unet.up_blocks.3.attentions.2.transformer_blocks.0.ff.net.2.lora.up.weight, Shape: torch.Size([320, 320])\n",
      "tensor([[-0.5369, -1.1228,  0.2615,  ...,  0.0235, -0.5939, -0.7564],\n",
      "        [ 2.0594, -0.8771, -0.0670,  ..., -0.7570, -1.3573,  0.5674],\n",
      "        [ 0.1094, -0.8488, -0.4023,  ..., -0.2422, -1.2641, -0.4398],\n",
      "        ...,\n",
      "        [ 2.1558, -0.4905,  0.6206,  ...,  0.0768,  0.2710, -0.6607],\n",
      "        [-0.2968, -0.1151, -0.0639,  ..., -0.2511,  1.1458,  0.0736],\n",
      "        [-0.4847, -1.0380,  0.2125,  ..., -0.2727,  1.4970, -0.1399]])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "观察发现重建的结果一致，此方法可行。\n",
    "2024/10/15 22:20更新: 找到了论文的代码，方法和我一样"
   ],
   "id": "5b522f4956df0066"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:16:34.370745Z",
     "start_time": "2024-10-19T04:16:29.146263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class ParameterVectorDataset(Dataset):\n",
    "    def __init__(self, data_paths):\n",
    "        self.data_paths = data_paths  # 数据文件的路径列表\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 加载第 idx 个参数向量\n",
    "        data = torch.load(self.data_paths[idx])\n",
    "        return data"
   ],
   "id": "1f4a8637aadb570a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:16:59.845547Z",
     "start_time": "2024-10-19T04:16:59.828508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 如果保存模型检查点的目录不存在，则创建\n",
    "checkpoint_dir = \"./checkpoints/lora_vae_checkpoints\"\n",
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    \n",
    "# 如果保存模型日志的目录不存在，则创建\n",
    "log_dir = \"./logs/lora_vae_logs\"\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "writer = SummaryWriter(log_dir)"
   ],
   "id": "5c226936e60c162b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:17:00.869343Z",
     "start_time": "2024-10-19T04:17:00.861590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 示例数据加载（请替换为您的数据加载逻辑）\n",
    "# 生成随机数据作为示例\n",
    "batch_size = 2\n",
    "num_samples = 100  # 请根据您的数据量调整\n",
    "in_dim = 135659520  # 请根据您的数据维度调整"
   ],
   "id": "50256b4f5ec9faf3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:17:05.128488Z",
     "start_time": "2024-10-19T04:17:05.119595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = \"./checkpoints/lora_weights_dataset\"\n",
    "\n",
    "rand_val_dataset_path = os.path.join(dataset_path, \"rand_val\")\n",
    "rand_test_dataset_path = os.path.join(dataset_path, \"rand_test\")"
   ],
   "id": "92580677579b78bf",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 19,
   "source": [
    "\n",
    "# 随机生成100个样本，每个样本的维度为 135659520，值的区间为[-1,+1]，并分别保存在 rand_test_normalized_data1.pth, rand_test_normalized_data2.pth, ..., rand_test_normalized_data100.pth 中\n",
    "for i in range(num_samples):\n",
    "    data = torch.rand(in_dim) * 2 - 1\n",
    "    torch.save(data, os.path.join(rand_test_dataset_path, f\"rand_test_normalized_data{i + 1}.pth\"))\n"
   ],
   "id": "f53cb40c2e3b89e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T10:35:17.781677Z",
     "start_time": "2024-10-18T10:30:15.727557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 生成随机的评估数据\n",
    "# 随机生成100个样本，每个样本的维度为 135659520，值的区间为[-1,+1]，并分别保存在 rand_val_normalized_data1.pth, rand_val_normalized_data2.pth, ..., rand_val_normalized_data100.pth 中\n",
    "\n",
    "for i in range(num_samples):\n",
    "    data = torch.rand(in_dim) * 2 - 1\n",
    "    torch.save(data, os.path.join(rand_val_dataset_path, f\"rand_val_normalized_data{i + 1}.pth\"))"
   ],
   "id": "3d3aa44c228f3037",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:17:10.984704Z",
     "start_time": "2024-10-19T04:17:10.972331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 随机测试数据文件列表\n",
    "rand_test_data_paths = glob.glob(os.path.join(rand_test_dataset_path,\"rand_test_normalized_data*.pth\"))\n",
    "# 创建数据集\n",
    "rand_test_data_sets = ParameterVectorDataset(rand_test_data_paths)\n",
    "# 创建数据加载器\n",
    "rand_test_data_loader = DataLoader(rand_test_data_sets, batch_size=batch_size, shuffle=True, num_workers=2)\n"
   ],
   "id": "bafc47d5de16105e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:17:12.519722Z",
     "start_time": "2024-10-19T04:17:12.500733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 随机评估数据文件列表\n",
    "rand_val_data_paths = glob.glob(os.path.join(rand_val_dataset_path, \"rand_val_normalized_data*.pth\"))\n",
    "rand_val_data_paths"
   ],
   "id": "47c06090c3b5a387",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data57.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data79.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data67.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data64.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data50.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data94.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data91.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data80.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data41.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data14.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data52.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data96.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data31.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data70.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data49.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data25.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data36.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data38.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data12.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data24.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data29.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data72.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data93.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data15.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data82.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data1.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data76.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data34.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data10.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data60.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data7.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data56.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data2.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data26.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data13.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data35.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data75.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data77.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data69.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data66.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data30.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data97.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data48.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data28.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data58.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data51.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data9.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data95.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data21.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data65.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data6.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data92.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data44.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data81.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data23.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data3.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data11.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data100.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data85.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data43.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data42.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data78.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data89.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data83.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data84.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data74.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data20.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data87.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data55.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data63.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data40.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data33.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data53.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data90.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data71.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data4.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data59.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data22.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data62.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data39.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data46.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data73.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data61.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data68.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data32.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data45.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data54.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data99.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data19.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data18.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data27.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data16.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data98.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data37.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data8.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data17.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data5.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data88.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data47.pth',\n",
       " './checkpoints/lora_weights_dataset/rand_val/rand_val_normalized_data86.pth']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:17:13.964977Z",
     "start_time": "2024-10-19T04:17:13.956503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 创建数据集\n",
    "rand_val_data_sets = ParameterVectorDataset(rand_val_data_paths)\n",
    "# 创建数据加载器\n",
    "rand_val_data_loader = DataLoader(rand_val_data_sets, batch_size=batch_size, shuffle=True, num_workers=2)\n"
   ],
   "id": "de201c418a7f543a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:17:17.484770Z",
     "start_time": "2024-10-19T04:17:17.473174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 训练数据文件列表\n",
    "train_data_paths = glob.glob(os.path.join(dataset_path, \"normalized_*.pth\"))\n",
    "# 创建数据集\n",
    "train_data_sets = ParameterVectorDataset(train_data_paths)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_data_loader = DataLoader(train_data_sets, batch_size=batch_size, shuffle=True, num_workers=2)"
   ],
   "id": "b5f7d014709db5c1",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:17:28.426433Z",
     "start_time": "2024-10-19T04:17:28.390007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from tqdm import tqdm\n",
    "# from torch.cuda.amp import GradScaler, autocast\n",
    "# from accelerate import Accelerator\n",
    "# import torch\n",
    "# import os\n",
    "# # 试着将重建的lora权重加载到odvae模型中\n",
    "# from odvae import ODVAE, medium, small\n",
    "# \n",
    "# # 设置模型参数\n",
    "# latent_dim = 12\n",
    "# kld_weight = 0.005\n",
    "# in_dim = 2048  # 请确保 in_dim 设置正确\n",
    "# \n",
    "# # 使用 Accelerator 进行多卡训练\n",
    "# accelerator = Accelerator()\n",
    "# \n",
    "# # 初始化模型\n",
    "# ODVAE_model = medium(in_dim=in_dim, latent_dim=latent_dim, kld_weight=kld_weight)\n",
    "# \n",
    "# # 设置优化器\n",
    "# optimizer = torch.optim.Adam(ODVAE_model.parameters(), lr=1e-3, weight_decay=2e-6)\n",
    "# \n",
    "# # 初始化最佳验证损失\n",
    "# best_val_loss = float('inf')\n",
    "# \n",
    "# # 定义学习率调度器\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "#                                                        factor=0.5, patience=10,\n",
    "#                                                        verbose=True, min_lr=1e-6)\n",
    "# \n",
    "# # 定义早停参数\n",
    "# early_stopping_patience = 20\n",
    "# early_stopping_counter = 0\n",
    "# \n",
    "# # 定义训练参数\n",
    "# num_epochs = 30000\n",
    "# batch_size = 4\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "import os\n",
    "# 试着将重建的lora权重加载到odvae模型中\n",
    "from odvae import ODVAE, medium, small\n",
    "\n",
    "# 设置模型参数\n",
    "latent_dim = 256\n",
    "kld_weight = 0.005\n",
    "in_dim = 135659520  # 请确保 in_dim 设置正确\n",
    "\n",
    "# 使用 Accelerator 进行多卡训练\n",
    "accelerator = Accelerator()\n",
    "def free_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# 初始化模型\n",
    "ODVAE_model = medium(in_dim=in_dim, latent_dim=latent_dim, kld_weight=kld_weight)\n",
    "\n",
    "# 设置优化器\n",
    "optimizer = torch.optim.Adam(ODVAE_model.parameters(), lr=1e-3, weight_decay=2e-6)\n",
    "\n",
    "# 初始化最佳验证损失\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# 定义学习率调度器\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                       factor=0.5, patience=10,\n",
    "                                                       verbose=True, min_lr=1e-6)\n",
    "\n",
    "# 定义早停参数\n",
    "early_stopping_patience = 20\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# 定义训练参数\n",
    "num_epochs = 30000\n",
    "# batch_size = 2"
   ],
   "id": "9673bd3e4eb74e17",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T04:18:12.645215Z",
     "start_time": "2024-10-19T04:17:34.579088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     ODVAE_model.train()\n",
    "#     epoch_loss = 0.0\n",
    "#     \n",
    "#     rand_test_loader_tqdm = tqdm(rand_test_data_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Train\", leave=False)\n",
    "# \n",
    "#     \n",
    "#     for batch_idx, batch in enumerate(rand_test_loader_tqdm):\n",
    "#         x = batch  # x 的形状为 [batch_size, 135659520]\n",
    "#         x = x.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         # with autocast():\n",
    "#         # 前向传播\n",
    "#         outputs = ODVAE_model(x)\n",
    "#         loss = outputs['loss']\n",
    "#         # 反向传播和优化\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         \n",
    "#         epoch_loss += loss.item()\n",
    "#         writer.add_scalar('Train/Loss', loss.item(), epoch * len(rand_test_data_loader) + batch_idx)\n",
    "#     avg_test_loss = epoch_loss / len(rand_test_data_loader)\n",
    "#     \n",
    "#     # 验证阶段\n",
    "#     ODVAE_model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     rand_val_loader_tqdm = tqdm(rand_val_data_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\", leave=False)\n",
    "# \n",
    "#     \n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, batch in enumerate(rand_val_loader_tqdm):\n",
    "#             x = batch  # x 的形状为 [batch_size, 135659520]\n",
    "#             x = x.to(device)\n",
    "#             outputs = ODVAE_model(x)\n",
    "#             loss = outputs['loss']\n",
    "#             val_loss += loss.item()\n",
    "#     avg_val_loss = val_loss / len(rand_val_data_loader)\n",
    "#     print(f'Avg Validation Loss: {avg_val_loss:.4f}')\n",
    "#         \n",
    "#     scheduler.step(avg_val_loss)\n",
    "#     writer.add_scalar('AvgLoss/Train', avg_test_loss, epoch)\n",
    "#     writer.add_scalar('AvgLoss/Validation', avg_val_loss, epoch)\n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_test_loss:.4f}')\n",
    "# \n",
    "#     # 记录学习率\n",
    "#     current_lr = optimizer.param_groups[0]['lr']\n",
    "#     writer.add_scalar('Learning_Rate', current_lr, epoch)\n",
    "# \n",
    "#         # 检查验证损失是否降低\n",
    "#     if avg_val_loss < best_val_loss:\n",
    "#         best_val_loss = avg_val_loss\n",
    "#         early_stopping_counter = 0  # 重置早停计数器\n",
    "#         # 保存最佳模型\n",
    "#         checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "#         torch.save(ODVAE_model.state_dict(), checkpoint_path)\n",
    "#         print(f'Best ODVAE_model saved at epoch {epoch+1} with validation loss {best_val_loss:.4f}')\n",
    "#     else:\n",
    "#         early_stopping_counter += 1\n",
    "#         if early_stopping_counter >= early_stopping_patience:\n",
    "#             print(f'Early stopping at epoch {epoch+1}')\n",
    "#             break\n",
    "# \n",
    "#     # 打印当前 epoch 的训练和验证损失\n",
    "#         \n",
    "#     if (epoch + 1) % 1000 == 0:\n",
    "#         checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pth')\n",
    "#         torch.save(ODVAE_model.state_dict(), checkpoint_path)\n",
    "#         print(f'ODVAE_model checkpoint saved at epoch {epoch+1}')\n",
    "#         \n",
    "# writer.close()\n",
    "# 使用 Accelerator 包装模型、优化器和数据加载器\n",
    "ODVAE_model, optimizer, rand_test_data_loader, rand_val_data_loader = accelerator.prepare(\n",
    "    ODVAE_model, optimizer, rand_test_data_loader, rand_val_data_loader\n",
    ")\n",
    "\n",
    "# 开始训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    ODVAE_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    rand_test_loader_tqdm = tqdm(rand_test_data_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Train\", leave=False)\n",
    "\n",
    "    for batch_idx, batch in enumerate(rand_test_loader_tqdm):\n",
    "        x = batch  # x 的形状为 [batch_size, 135659520]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with accelerator.autocast():\n",
    "            # 前向传播\n",
    "            outputs = ODVAE_model(x)\n",
    "            loss = outputs['loss']\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        if accelerator.is_main_process:\n",
    "            writer.add_scalar('Train/Loss', loss.item(), epoch * len(rand_test_data_loader) + batch_idx)\n",
    "\n",
    "        # 释放未使用的显存\n",
    "        free_memory()\n",
    "    avg_test_loss = epoch_loss / len(rand_test_data_loader)\n",
    "\n",
    "    # 验证阶段\n",
    "    ODVAE_model.eval()\n",
    "    val_loss = 0.0\n",
    "    rand_val_loader_tqdm = tqdm(rand_val_data_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(rand_val_loader_tqdm):\n",
    "            x = batch  # x 的形状为 [batch_size, 135659520]\n",
    "            with accelerator.autocast():\n",
    "                outputs = ODVAE_model(x)\n",
    "                loss = outputs['loss']\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(rand_val_data_loader)\n",
    "    if accelerator.is_main_process:\n",
    "        print(f'Avg Validation Loss: {avg_val_loss:.4f}')\n",
    "        scheduler.step(avg_val_loss)\n",
    "        writer.add_scalar('AvgLoss/Train', avg_test_loss, epoch)\n",
    "        writer.add_scalar('AvgLoss/Validation', avg_val_loss, epoch)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "    # 记录学习率\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    if accelerator.is_main_process:\n",
    "        writer.add_scalar('Learning_Rate', current_lr, epoch)\n",
    "\n",
    "    # 检查验证损失是否降低\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        early_stopping_counter = 0  # 重置早停计数器\n",
    "        # 保存最佳模型\n",
    "        if accelerator.is_main_process:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "            accelerator.save(ODVAE_model.state_dict(), checkpoint_path)\n",
    "            print(f'Best ODVAE_model saved at epoch {epoch+1} with validation loss {best_val_loss:.4f}')\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            if accelerator.is_main_process:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "\n",
    "    # 打印当前 epoch 的训练和验证损失\n",
    "    if (epoch + 1) % 1000 == 0 and accelerator.is_main_process:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch+1}.pth')\n",
    "        accelerator.save(ODVAE_model.state_dict(), checkpoint_path)\n",
    "        print(f'ODVAE_model checkpoint saved at epoch {epoch+1}')\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    writer.close()"
   ],
   "id": "7c721fac12e13171",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30000 - Train:   0%|          | 0/50 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/NEW_EDS/JJ_Group/shaoyh/env/aqualora/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/NEW_EDS/JJ_Group/shaoyh/env/aqualora/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/NEW_EDS/JJ_Group/shaoyh/env/aqualora/lib/python3.10/multiprocessing/util.py\", line 133, in _remove_temp_dir\n",
      "    rmtree(tempdir)\n",
      "  File \"/NEW_EDS/JJ_Group/shaoyh/env/aqualora/lib/python3.10/shutil.py\", line 725, in rmtree\n",
      "    _rmtree_safe_fd(fd, path, onerror)\n",
      "  File \"/NEW_EDS/JJ_Group/shaoyh/env/aqualora/lib/python3.10/shutil.py\", line 681, in _rmtree_safe_fd\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"/NEW_EDS/JJ_Group/shaoyh/env/aqualora/lib/python3.10/shutil.py\", line 679, in _rmtree_safe_fd\n",
      "    os.unlink(entry.name, dir_fd=topfd)\n",
      "OSError: [Errno 16] Device or resource busy: '.nfs0000040010e44b3f00000020'\n",
      "                                                             \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x868220 and 12x12)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 89\u001B[0m\n\u001B[1;32m     85\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m accelerator\u001B[38;5;241m.\u001B[39mautocast():\n\u001B[1;32m     88\u001B[0m     \u001B[38;5;66;03m# 前向传播\u001B[39;00m\n\u001B[0;32m---> 89\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mODVAE_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     90\u001B[0m     loss \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     92\u001B[0m \u001B[38;5;66;03m# 反向传播和优化\u001B[39;00m\n",
      "File \u001B[0;32m/NEW_EDS/JJ_Group/shaoyh/env/aqualora/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/NEW_EDS/JJ_Group/shaoyh/dorin/AquaLoRA/odvae.py:97\u001B[0m, in \u001B[0;36mODVAE.forward\u001B[0;34m(self, input, **kwargs)\u001B[0m\n\u001B[1;32m     95\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m     96\u001B[0m x_flatten \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 97\u001B[0m mu \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc_mu\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_flatten\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     98\u001B[0m log_var \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc_var(x_flatten)\n\u001B[1;32m     99\u001B[0m z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreparameterize(mu, log_var)\n",
      "File \u001B[0;32m/NEW_EDS/JJ_Group/shaoyh/env/aqualora/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/NEW_EDS/JJ_Group/shaoyh/env/aqualora/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (2x868220 and 12x12)"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "680c9e200a43c353"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
